<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-data/annotation-guidelines">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">Annotation Guidelines | Ego4D</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ego4d-data.org/docs/data/annotation-guidelines/"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Annotation Guidelines | Ego4D"><meta data-rh="true" name="description" content="This page contains context mostly on the annotation guidelines used in each tasks.  Please also see annotations for the specific formats and benchmark tasks for more detail on the tasks themselves.  And please read the paper here for the most comprehensive introduction."><meta data-rh="true" property="og:description" content="This page contains context mostly on the annotation guidelines used in each tasks.  Please also see annotations for the specific formats and benchmark tasks for more detail on the tasks themselves.  And please read the paper here for the most comprehensive introduction."><link data-rh="true" rel="icon" href="/docs/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://ego4d-data.org/docs/data/annotation-guidelines/"><link data-rh="true" rel="alternate" href="https://ego4d-data.org/docs/data/annotation-guidelines/" hreflang="en"><link data-rh="true" rel="alternate" href="https://ego4d-data.org/docs/data/annotation-guidelines/" hreflang="x-default"><link rel="stylesheet" href="/docs/assets/css/styles.d2213c09.css">
<link rel="preload" href="/docs/assets/js/runtime~main.dc5a4f68.js" as="script">
<link rel="preload" href="/docs/assets/js/main.3e8d1ac0.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><a class="navbar__brand" href="/docs/"><div class="navbar__logo"><img src="/docs/img/ego-4d-logo.png" alt="Ego4d Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/docs/img/ego-4d-logo-dark.png" alt="Ego4d Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Ego4D</b></a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/">Welcome To EGO4D!</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/start-here/">Start Here</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/data/annotation-guidelines/">Data</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/data/annotation-guidelines/">Annotation Guidelines</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data/metadata/">Metadata</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data/videos/">Videos</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data/gaze/">Gaze</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data/imu/">IMU</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data/features/">Features</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data/annotations-schemas/">Annotation Schemas</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data/egotracks/">EgoTracks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data/unprocessed_data/">Unprocessed Data</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/benchmarks/overview/">Benchmark Tasks</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/CLI/">CLI Tool</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/viz/">Visualization Tool</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/privacy/">Privacy and Ethics</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/model-zoo/">Model Zoo</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/challenge/">Ego4D Challenge 2023</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/updates/">updates</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/FAQ/">FAQ</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/contact/">Contact Us</a></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/docs/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Data</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Annotation Guidelines</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Annotation Guidelines</h1></header><p>This page contains context mostly on the annotation guidelines used in each tasks.  Please also see <a href="/docs/data/annotations-schemas/">annotations</a> for the specific formats and <a href="/docs/benchmarks/overview/">benchmark tasks</a> for more detail on the tasks themselves.  And please <a href="https://arxiv.org/abs/2110.07058" target="_blank" rel="noopener noreferrer">read the paper here</a> for the most comprehensive introduction.</p><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>Numbers quoted below are those available at the time of writing this documentation; authoritative information is found in our arxiv paper.</p></div></div><p><strong>Devices:</strong></p><p><img loading="lazy" src="/docs/assets/images/image7-e0a2bd6a7b876b3194e9a7bbf876256c.png" width="2048" height="783" class="img_ev3q"></p><p><strong>Scenario breakdown:</strong></p><p><img loading="lazy" src="/docs/assets/images/image16-9e55f40cbd0b57fc516ff43a2d52324d.png" width="2048" height="703" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="annotations-tldr">Annotations tl;dr<a href="#annotations-tldr" class="hash-link" aria-label="Direct link to Annotations tl;dr" title="Direct link to Annotations tl;dr">​</a></h2><table><thead><tr><th><strong>Task</strong></th><th><strong>Output</strong></th><th><strong>Volume</strong></th></tr></thead><tbody><tr><td><strong>Pre-annotations</strong></td><td></td><td></td></tr><tr><td><a href="#pre-annotations-narrations">Narrations</a></td><td>Dense written sentence narrations in English &amp; a summary of the whole video clip</td><td>Full Dataset</td></tr><tr><td><strong>Episodic Memory (EM)</strong></td><td></td><td></td></tr><tr><td><a href="#natural-language-queries">Natural Language Queries</a></td><td>N free-form natural language queries per video (N=length of video in minutes) selected from a list of query templates + temporal response window from which answers can be deduced</td><td>~<!-- -->240h</td></tr><tr><td><a href="#moments">Moments</a></td><td>Temporal localizations of high level events in a long video clip from a provided taxonomy</td><td>~<!-- -->300h</td></tr><tr><td><a href="#visual-object-queries">Visual Object Queries</a></td><td>For N=3 <strong>query objects</strong> (freely chosen and <strong>named</strong> by the annotator) such that each appears at least twice at separate times in a single video, annotations include: <br> <!-- -->(<!-- -->1<!-- -->)<!-- --> <strong>response track</strong>: bounding boxes over time for one continuous occurrence of the query object; <br> <!-- -->(<!-- -->2<!-- -->)<!-- --> <strong>query frame</strong>: a frame that <em>does</em> <em>not</em> contain the query object, sometime after the response track but before any subsequent occurrence of the object; <br> <!-- -->(<!-- -->3<!-- -->)<!-- --> <strong>visual crop</strong>:  bounding box of a single frame from another occurrence of the same object elsewhere in the video (before or after the originally marked instance)</td><td>~<!-- -->403h</td></tr><tr><td><strong>Forecasting + Hands &amp; Objects (FHO)</strong></td><td></td><td></td></tr><tr><td><a href="#stage-1---critical-frames">1 Critical Frames</a></td><td>Pre-condition (PRE), CONTACT, point of no return (PNR), and post-condition (Post) frames for each narrated action in a video</td><td>~<!-- -->120h</td></tr><tr><td><a href="#stage-2---pre-condition">2 Pre-condition</a></td><td>Bounding boxes and roles for hands (right/left) and objects (objects of change and tools) for each frame from CONTACT to PRE</td><td></td></tr><tr><td><a href="#stage-3---post-condition">3 Post-condition</a></td><td>Bounding boxes and roles for hands and objects for each frame from CONTACT to POST</td><td></td></tr><tr><td><strong>Audio-Visual Diarization &amp; Social (AVS)</strong></td><td></td><td></td></tr><tr><td><a href="#av-step-0-automated-face-head-detection">AV0: Automated Face &amp; Head Detection</a></td><td>Automated overlaid bounding boxes for faces in video clips</td><td>50h</td></tr><tr><td><a href="#av-step-1-face-head-tracks-correction">AV1: Face &amp; Head Tracks Correction</a></td><td>Manually adjusted overlaid bounding boxes for faces in video clips</td><td></td></tr><tr><td><a href="#av-step-2-speaker-labeling-and-av-anchor-extraction">AV2: Speaker Labeling and AV anchor extraction</a></td><td>Anonymous Person IDs for each Face Track in video clip</td><td></td></tr><tr><td><a href="#av-step-3-speech-segmentation-per-speaker">AV3: Speech Segmentation (Per Speaker)</a></td><td>Temporal segments for voice activity for the camera wearer and for each Person ID</td><td></td></tr><tr><td><a href="#av-step-4-transcription">AV4: Transcription</a></td><td>Video clip audio transcriptions</td><td></td></tr><tr><td><a href="#av-step-5-correcting-speech-transcriptions-wip">AV5: Correcting Speech Transcriptions</a></td><td>Corrected Speech Transcription annotations matching voice activity segments and Person IDs from AV2</td><td></td></tr><tr><td><a href="#s-step-1-camera-wearer-attention">S1: Camera-Wearer Attention</a></td><td>Temporal segments in which a person is looking at the camera wearer</td><td></td></tr><tr><td><a href="#s-step-2-speech-target-classification">S2: Speech Target Classification</a></td><td>Temporal segments in which a person is talking to the camera wearer</td><td></td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="narrations">Narrations<a href="#narrations" class="hash-link" aria-label="Direct link to Narrations" title="Direct link to Narrations">​</a></h2><p><strong>Objective:</strong> Annotator provides dense written sentence narrations in
English on a first-person video clip of length 10-30 minutes + a summary
of the whole video.</p><p><strong>Motivation:</strong> Understand what data is available and which data to push
through which annotation phases. Provide a starting point for forming a
taxonomy of labels for actions and objects.</p><p><strong>Tags:</strong></p><p>There are four flags that annotators use in the sentence boxes:</p><ul><li><code>#unsure</code> to denote they are unsure about a specific statement</li><li><code>#summary</code> to denote they are giving the overall video</li><li><code>#C</code> to denote the sentence is an action done by the camera wearer (the person who recorded the video while wearing a camera on their head)</li><li><code>#O</code> to denote that the sentence is an action done by someone other than the camera wearer</li></ul><p>Note that every sentence will have either <code>#C</code> or <code>#O</code>.  Only some sentences (or none) may have <code>#unsure</code>.  Only one sentence for the entire video clip will have <code>#summary</code>.</p><p><strong>Annotation task:</strong></p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th><th><strong>Example</strong></th></tr></thead><tbody><tr><td>1</td><td><em>Narrate the Complete Video with Temporal Sentences</em></td><td>Watch the video from the beginning until something new occurs. <hr>At that time, pause the video, mark the temporal window for which the sentence applies, then &quot;narrate&quot; what you see in the video by typing in a simple sentence into the free-form text input. <hr> Next, resume watching the video. Once you recognize an action to narrate, immediately pause again and repeat.</td><td><em>[set the start time as the point when the person has the knife and the tomato, and the end time as the point when the person has finished chopping, then type]<!-- -->:</em> &quot;C is chopping a tomato&quot; into the text input.<br>(&quot;C&quot; refers to the camera wearer).</td></tr><tr><td>2</td><td><em>Provide a Summary of the Entire Video</em></td><td>As needed, watch the entire video on fast forward to recall the content of the entire video.<br><br>Provide a short summary in text about the contents of the entire video (1-3 sentences).<br><br>This summary should convey the main setting(s) of the video clip (e.g., an apartment, a restaurant, a shop, etc.) as well as an overview of what happened.</td><td>#summary C fixed their breakfast, ate it, then got dressed and left the house.&quot;</td></tr></tbody></table><p><strong>Annotated video examples:</strong></p><p><img loading="lazy" src="/docs/assets/images/image35-91b5eb2d1a6d9ba45ec1f3bba497a145.png" width="1714" height="968" class="img_ev3q"><img loading="lazy" src="/docs/assets/images/image32-8b0a8c1616304b84176927470f098846.png" width="1263" height="908" class="img_ev3q"></p><p><strong>Annotation Stats</strong></p><ul><li><p><strong>Total hours narrated:</strong> 3670</p></li><li><p><strong>Unique scenarios:</strong> 51</p></li></ul><p><img loading="lazy" src="/docs/assets/images/image13-1dbe07a190ef030848b79fe86ab52b9f.png" width="1380" height="560" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="benchmark-annotations">Benchmark Annotations<a href="#benchmark-annotations" class="hash-link" aria-label="Direct link to Benchmark Annotations" title="Direct link to Benchmark Annotations">​</a></h2><p><img loading="lazy" src="/docs/assets/images/image18-fec68ada7e115e2b5ddad1b313a3d283.png" width="1248" height="702" class="img_ev3q"></p><table><thead><tr><th><strong>Target</strong></th><th><strong>#</strong></th><th><strong>Benchmark task</strong></th><th><strong>Research Goal</strong></th></tr></thead><tbody><tr><td><strong>Places</strong></td><td>1</td><td><a href="#episodic-memory">Episodic Memory</a></td><td>Allow an user to ask free-form, natural language questions, with the answer brought back after analyzing past video (<em>When was the last time I changed the batteries in the smoke detector?</em>).</td></tr><tr><td><strong>Objects</strong></td><td>2</td><td><a href="#forecasting--hands--objects-fho">Forecasting</a></td><td>To intelligently deliver notifications to a user, an AR system must understand how an action or piece of information may impact the future state of the world.</td></tr><tr><td></td><td>3</td><td><a href="#forecasting--hands--objects-fho">Hands-Object interaction</a></td><td>AR applications, e.g. providing users instructions in their egocentric real-world view to accomplish tasks (e.g., cooking a recipe).</td></tr><tr><td><strong>People</strong></td><td>4</td><td><a href="#audio-visual-diarization--social-avs">Audio-visual Diarization</a></td><td>To effectively aid people in daily life scenarios, augmented reality must be able to detect and track sounds, responding to users queries or information needs.</td></tr><tr><td></td><td>5</td><td><a href="#audio-visual-diarization--social-avs">Social interactions</a></td><td>Recognize people&#x27;s interactions, their roles, and their attention within collaborative and competitive scenarios within a range of social interactions captured in the Ego4D data.</td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="episodic-memory">Episodic Memory<a href="#episodic-memory" class="hash-link" aria-label="Direct link to Episodic Memory" title="Direct link to Episodic Memory">​</a></h2><p><strong>Motivation</strong>: Augment human memory through personal semantic video
index for an always-on wearable camera</p><p><strong>Objective</strong>: Given long first-person video, <em>localize</em> answers for
queries about objects and events from first-person experience</p><p><em>Who did I sit by at the party? Where are my keys? When did I change the batteries? How often did I read to my child last week? Did I leave the window open?<!-- -->.<!-- -->..</em></p><p><strong>Query types (annotation sub-tasks)</strong>:</p><p>a.  Natural language queries (response = temporal)</p><p>b.  Moments queries (response = temporal)</p><p>c.  Visual/object queries (response = temporal+spatial)</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="natural-language-queries">Natural Language Queries<a href="#natural-language-queries" class="hash-link" aria-label="Direct link to Natural Language Queries" title="Direct link to Natural Language Queries">​</a></h3><h3><img loading="lazy" src="/docs/assets/images/image4-8e3ced36f3852c4e3022c81523a33885.png" width="1100" height="314" class="img_ev3q"></h3><p><strong>Objective:</strong> Create and annotate <strong>N (N=length of video in minutes)</strong>
interesting questions and their corresponding answers for the given
video.</p><p><strong>Annotation Task:</strong></p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th><th><strong>Example</strong></th></tr></thead><tbody><tr><td>0</td><td><em>Annotator watches video</em></td><td></td><td></td></tr><tr><td>1</td><td><em>Asks free-form natural language query at end of video, selecting from list of query templates.</em></td><td>- Select an interesting query template &amp; template category. <br>- Paraphrase question in the past tense.</td><td>- <ins>Template</ins>: &quot;What <span style="color:#cb4b16"><strong>X</strong></span> is <span style="color:#268bd2"><strong>Y</strong></span>?&quot; <br>- <ins>Template Category</ins>: &quot;Objects&quot; <br> - <ins>Paraphrased query</ins>: &quot;What <span style="color:#cb4b16"><strong>color</strong></span> shirt did the <span style="color:#268bd2"><strong>person performing on the road</strong></span> wear?&quot;</td></tr><tr><td></td><td></td><td>Using <strong>&quot;free-form&quot;</strong> text, fill the <strong>query slots</strong> (X, Y, ...) in the template to form a meaningful question equivalent to the paraphrase.</td><td>First free-form query slot: &quot;<span style="color:#cb4b16"><strong>color</strong></span>&quot; <br><br>Second free-form query slot: &quot;<span style="color:#268bd2"><strong>the shirt of the person performing on the road</strong></span>&quot;</td></tr><tr><td></td><td></td><td>Pick the closest verb for each of the slots in the respective drop-down menus</td><td>- <ins>Paraphrased query</ins>: What <span style="color:#cb4b16"><strong>instrument</strong></span> was <span style="color:#268bd2"><strong>the musician playing</strong></span>? <br>- <ins>First verb drop-down selection</ins>: &quot;<!-- -->[VERB NOT APPLICABLE]<!-- -->&quot; <br> Second verb drop-down selection:  &quot;<strong>play</strong>&quot;</td></tr><tr><td>2</td><td><em>Identifies the temporal response window from which answer can be deduced</em></td><td>Seek in the video to the temporal window where the response to the natural language query can be deduced visually. <br><br> Specify query to have only one valid, contiguous temporal window response.</td><td></td></tr><tr><td>3</td><td><em>Repeat this process N=length of video in minutes creating N diverse language queries</em></td><td></td><td></td></tr></tbody></table><p><strong>Annotation Stats:</strong></p><ul><li><p><strong>Total hours annotated:</strong> <!-- -->~<!-- -->240 (x2; one for each vendor)</p></li><li><p><strong>Distribution over question types:</strong></p></li></ul><p><img loading="lazy" src="/docs/assets/images/image2-73d744aacb706a6c6be825d0f333a008.png" width="2048" height="696" class="img_ev3q"></p><ul><li><strong>Scenario breakdown:</strong></li></ul><p><img loading="lazy" src="/docs/assets/images/image3-db38883c4c8f547284a969c5d3197adf.png" width="1162" height="435" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="moments">Moments<a href="#moments" class="hash-link" aria-label="Direct link to Moments" title="Direct link to Moments">​</a></h3><p><img loading="lazy" src="/docs/assets/images/image10-a15f59ac4828168c130fc993a5dba16d.png" width="1017" height="230" class="img_ev3q"></p><p><strong>Objective:</strong> Localize high level events in a long video clip <!-- -->-<!-- -->-
marking any instance of provided activity categories with a temporal
window and the activity&#x27;s name.</p><p><strong>Motivation:</strong> Learn to detect activities or &quot;moments&quot; and their
temporal extent in the video. In the context of episodic memory, the
implicit query from a user would be &quot;When is the last time I did X?&quot;,
and the response from the system would be to show the time window where
activity X was last seen.</p><p><strong>Annotation Task:</strong></p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th><th><strong>Example</strong></th></tr></thead><tbody><tr><td>1</td><td><em>Review the Taxonomy</em></td><td></td><td><img loading="lazy" src="/docs/assets/images/image15-ee6c6c67f35813d37548298c6b540674.png" width="1092" height="940" class="img_ev3q"></td></tr><tr><td>2</td><td><em>Annotate the Video</em></td><td>1. Play the video until you observe an activity, then pause.<br>2. Draw a temporal window around the time span where the activity occurs. <br> 3. Select from the dropdown list the name for that activity. <br> 4. Play the video from the start of the previous activity, repeat steps 1-3.</td><td><img loading="lazy" src="/docs/assets/images/image45-4154b0e8b1843819b876742ac2343582.png" width="2500" height="1338" class="img_ev3q"></td></tr></tbody></table><p><strong>Annotation Stats:</strong></p><ul><li><strong>Total hours annotated:</strong> <!-- -->~<!-- -->328 (x3 annotators)</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="visual-object-queries">Visual Object Queries<a href="#visual-object-queries" class="hash-link" aria-label="Direct link to Visual Object Queries" title="Direct link to Visual Object Queries">​</a></h3><p><img loading="lazy" src="/docs/assets/images/image33-930945bc750e68e528ae8219b42fce82.png" width="874" height="168" class="img_ev3q"></p><p><strong>Objective:</strong> Localize past instances of a given object that appears at
least twice in different parts of the video.</p><p><strong>Motivation:</strong> Support an object search application for video in which
a user asks at time T &quot;where did I last see X?&quot;, and the system scans
back in the video history starting at query frame T, finds the most
recent instance of X, and outlines it in a short track<strong>.</strong></p><p><strong>Annotation Task:</strong></p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th><th><strong>Example</strong></th></tr></thead><tbody><tr><td>1</td><td><em>Identify <strong>query objects</strong></em></td><td>Preview the entire video.<br>Identify a set of N=3 interesting objects to label as queries (= objects that appear at least twice at distinct non-contiguous parts of the video clip)</td><td></td></tr><tr><td>2</td><td><em>Select a <strong>response track</strong></em></td><td>- Select one occurrence of the query object.<br> - Mark the query object with a bounding box over time, from the frame the object enters the field of view until it leaves the field of view, for that object occurrence.</td><td><img loading="lazy" src="/docs/assets/images/image30-d06bb98836a451e9e6da02c38ce456b0.gif" width="480" height="270" class="img_ev3q"></td></tr><tr><td>3</td><td><em>Select a <strong>query frame</strong></em></td><td>- Select a frame that does not contain the query object, sometime far after that object occurrence, but before any subsequent occurrence of the object.<br> - Mark the time point with a large bounding box.</td><td><img loading="lazy" src="/docs/assets/images/image9-037f03bb67828cdc6e1e450fef4891c5.png" width="393" height="259" class="img_ev3q"></td></tr><tr><td>4</td><td><em>Select a <strong>visual crop</strong></em></td><td>- Find another occurrence of the same object elsewhere in the video (before or after the originally marked instance from Step 2). <br> - Draw a bounding box in one frame around that object.</td><td><img loading="lazy" src="/docs/assets/images/image11-95c1f2627bc7363d7ed098704364aecf.png" width="423" height="280" class="img_ev3q"></td></tr><tr><td>5</td><td><em><strong>Name the object</strong> using the <strong>free text</strong> box</em></td><td></td><td></td></tr><tr><td>6</td><td><em>Repeat Steps 1-5 three times for the same video clip and different objects</em></td><td></td><td></td></tr></tbody></table><p><strong>Annotation Stats:</strong></p><ul><li><strong>Total hours annotated:</strong> <!-- -->~<!-- -->432</li></ul><blockquote><p><img loading="lazy" src="/docs/assets/images/image1-cb95f7a6ce8d8cc57a8a376a4870bd4b.png" width="285" height="284" class="img_ev3q"></p></blockquote><ul><li><strong>Scenario breakdown:</strong></li></ul><p><img loading="lazy" src="/docs/assets/images/image21-997834909d39c1d2ea8729613e8cf735.png" width="1117" height="280" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="forecasting--hands--objects-fho">Forecasting + Hands &amp; Objects (FHO)<a href="#forecasting--hands--objects-fho" class="hash-link" aria-label="Direct link to Forecasting + Hands &amp; Objects (FHO)" title="Direct link to Forecasting + Hands &amp; Objects (FHO)">​</a></h2><p><strong>Objective:</strong> Recognize object state changes temporally and spatially
(HO); predict these interactions spatially and temporally before they
happen (F).</p><p><strong>Motivation:</strong> Understanding and anticipating human-object
interactions.</p><table><thead><tr><th><strong>Annotation Stats</strong></th><th><strong>Scenario Distribution</strong></th></tr></thead><tbody><tr><td>Labeled videos: 1,074 <br><br> Labeled clips: 1,672 <br><br> Labeled hours: 116.274 <br><br> Number of scenarios: 53 <br><br> Number of universities: 7 <br><br> Number of participants: 397 <br><br> Num interactions: 91,002 <br><br> Num rejected: 18,839 <br><br> Num with state change: 70,718 <img loading="lazy" width="500px" class="img_ev3q"></td><td><img loading="lazy" src="/docs/assets/images/image19-00e19a2a62a562005bd744ae7965764a.png" width="2000" height="800" class="img_ev3q"></td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="stage-1---critical-frames">Stage 1 - Critical Frames<a href="#stage-1---critical-frames" class="hash-link" aria-label="Direct link to Stage 1 - Critical Frames" title="Direct link to Stage 1 - Critical Frames">​</a></h3><p><img loading="lazy" src="/docs/assets/images/image5-1c194e80ab2a55776812a699b248b5ee.png" width="899" height="235" class="img_ev3q"></p><p><strong>Objective:</strong> Annotator watches an egocentric video and marks
pre-condition (PRE), contact, point of no return (PNR), and
post-condition (Post) frames.</p><p><strong>Annotation Task:</strong></p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th><th><strong>Example</strong></th></tr></thead><tbody><tr><td>1</td><td><em>Read the narrated action to be labeled</em></td><td>1. Reject videos that do not contain hand-object interactions<br> 2. Reject videos that not contain the narrated action</td><td><em>Example:</em> &quot;C glides hand planer along the wood&quot;<br><img loading="lazy" src="/docs/assets/images/image46-dc4bb0c499a85443479df0b6efa01372.png" width="2500" height="1344" class="img_ev3q"></td></tr><tr><td>2</td><td><em>Select the verb corresponding to the narration</em></td><td>- If an appropriate verb is not available, select OTHER from the dropdown and type in the verb in the text box.</td><td><img loading="lazy" src="/docs/assets/images/image28-3c520df2678da7c9cc0c1061b3aef497.png" width="2048" height="1083" class="img_ev3q"></td></tr><tr><td>3</td><td>*Select the <strong>state change type</strong> present in the video</td><td>- Select from one of 8 options from the dropdown</td><td><img loading="lazy" src="/docs/assets/images/image22-22d30630133c337e26b3fee2612bbe32.png" width="2048" height="1103" class="img_ev3q"></td></tr><tr><td>4</td><td>*Mark the <strong>CONTACT</strong> (only if present), <strong>PRE</strong> and <strong>POST</strong> frames.</td><td>- Find the CONTACT frame <br> - Pause the video <br> - Select the &quot;Contact Frame&quot; from the dropdown <br> - Repeat the same protocol for PRE and POST frames.</td><td><img loading="lazy" src="/docs/assets/images/image47-dd612c0b66e10b5c869fc6cef6539089.png" width="2500" height="1342" class="img_ev3q"></td></tr></tbody></table><p><strong>PRE, CONTACT, PNR, POST examples:</strong></p><p>a.  Example: &quot;light blowtorch&quot;</p><blockquote><p><img loading="lazy" src="/docs/assets/images/image38-80d977484213083a627b3a0d642d91b4.jpg" width="2500" height="525" class="img_ev3q"></p></blockquote><p>b.  Example: &quot;put down wood&quot; (object already in hands, no CONTACT frame)</p><blockquote><p><img loading="lazy" src="/docs/assets/images/image39-097e7ec2f7bd174679c70429ec7d2aee.jpg" width="2500" height="701" class="img_ev3q"></p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="stage-2---pre-condition">Stage 2 - Pre-condition<a href="#stage-2---pre-condition" class="hash-link" aria-label="Direct link to Stage 2 - Pre-condition" title="Direct link to Stage 2 - Pre-condition">​</a></h3><p><strong>Objective:</strong> Label bounding boxes and roles for hands (right/left) and
objects (objects of change and tools).</p><p><strong>Annotation Task:</strong></p><ins>Note</ins>: clips annotated from previous stage play in reverse from CONTACT to PRE frame:<table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th><th><strong>Example</strong></th></tr></thead><tbody><tr><td>1</td><td><em>Read the narrated action to be labeled</em></td><td></td><td><em>Example</em>: &quot;C straightens the cloth&quot; <img loading="lazy" src="/docs/assets/images/image25-77c8e6deeeb932a4fb79d1a6dfcd7a11.png" width="1904" height="986" class="img_ev3q"></td></tr><tr><td>2</td><td><em>Label the contact frame (first frame shown)</em></td><td>Label right and left hands (if visible), by correcting the existing bounding box or adding a new one.</td><td><img loading="lazy" src="/docs/assets/images/image29-0ae02bb25421fa38252fc8eeaed8aab3.png" width="1904" height="976" class="img_ev3q"></td></tr><tr><td></td><td></td><td>Label the <strong>object(s) of change</strong>:<br><br> - Draw the <ins>bounding box</ins> <br> - Mark the object as Object of change <br> - Select the <ins>name of the object</ins> from list provided <br> - Select <ins>instance ID</ins> (for multiple objects of the same type) <br> - Repeat for each object of change</td><td><img loading="lazy" src="/docs/assets/images/image36-7889dc79cf3aeae56e00ae659a9656b6.png" width="1900" height="968" class="img_ev3q"></td></tr><tr><td></td><td></td><td>Label the <strong>tool</strong> (if present): Draw the <ins>bounding box</ins> <br> - Mark the object as Tool <br> - Select the <ins>name of the tool</ins> from list provided <br> - Select <ins>instance ID</ins> (for multiple objects of the same type)</td><td><img loading="lazy" src="/docs/assets/images/image27-989a9cfc53d58fd644b80e92d393b3fe.png" width="1906" height="974" class="img_ev3q"></td></tr><tr><td>3</td><td><em>Label the remaining frames</em></td><td>Go to the next frame <br> - Adjust the hand boxes <br> - Adjust the object of change box <br> - Adjust the tool box (if present) <br> - Repeat for the remaining frames</td><td><img loading="lazy" src="/docs/assets/images/image41-37cfe3266c456c9accac870986c15d68.png" width="1898" height="982" class="img_ev3q"></td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="stage-3---post-condition">Stage 3 - Post-condition<a href="#stage-3---post-condition" class="hash-link" aria-label="Direct link to Stage 3 - Post-condition" title="Direct link to Stage 3 - Post-condition">​</a></h3><p><strong>Objective:</strong> Label bounding boxes and roles for hands and objects
(from Contact to Post frame).</p><p><strong>Annotation Task:</strong>
<!-- -->[Note]<!-- -->: clips annotated from Stage 1 play from CONTACT to POST
frame:</p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th><th><strong>Example</strong></th></tr></thead><tbody><tr><td>1</td><td><em>Read the narrated action to be labeled</em></td><td></td><td><em>Example:</em> &quot;C straightens the cloth&quot;<br> <img loading="lazy" src="/docs/assets/images/image25-77c8e6deeeb932a4fb79d1a6dfcd7a11.png" width="1904" height="986" class="img_ev3q"></td></tr><tr><td>2</td><td><em>Check the contact frame (first frame shown)</em></td><td>Contact frame will already be labeled with: <br> - Left hand (if visible) <br> - Right hand (if visible) <br> - Active object <br> - Tool (if applicable)</td><td></td></tr><tr><td>3</td><td><em>Label the remaining frames</em></td><td>- Go to the next frame <br> - Adjust (or add) the hand boxes <br> - Adjust the object of change box <br> - Adjust the tool box (if present) <br> - Repeat for the remaining frames</td><td><img loading="lazy" src="/docs/assets/images/image37-a8be03c374b2562ab33199b2810d3666.png" width="1906" height="982" class="img_ev3q"></td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="audio-visual-diarization--social-avs">Audio-Visual Diarization &amp; Social (AVS)<a href="#audio-visual-diarization--social-avs" class="hash-link" aria-label="Direct link to Audio-Visual Diarization &amp; Social (AVS)" title="Direct link to Audio-Visual Diarization &amp; Social (AVS)">​</a></h2><p><strong>Objective:</strong></p><ul><li><p><strong>AV</strong>: Locate each speaker spatially and temporally, segment and transcribe the speech content (in a given video), assign each speaker an anonymous label.</p></li><li><p><strong>S:</strong> predict the following social cues:</p><ul><li><p>Who is talking to the camera wearer at each time segment</p></li><li><p>Who is looking at the camera wearer at each time segment</p></li></ul></li></ul><p><strong>Motivation:</strong> Understand conversational behavior from the naturalistic
egocentric perspective; capture low level detection, segmentation and
tracking attributes of people\&#x27;s interactions in a scene, and more high
level (intent/emotions driven) attributes that drive social and group
conversations in the real world.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="av-step-0-automated-face--head-detection">AV Step 0: Automated Face &amp; Head Detection<a href="#av-step-0-automated-face--head-detection" class="hash-link" aria-label="Direct link to AV Step 0: Automated Face &amp; Head Detection" title="Direct link to AV Step 0: Automated Face &amp; Head Detection">​</a></h3><p>A face detection algorithm is run on the given input video to detect all
the faces. The resulting bounding boxes are going to be populated and
overlaid on the input video.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="av-step-1-face--head-tracks-correction">AV Step 1: Face &amp; Head Tracks Correction<a href="#av-step-1-face--head-tracks-correction" class="hash-link" aria-label="Direct link to AV Step 1: Face &amp; Head Tracks Correction" title="Direct link to AV Step 1: Face &amp; Head Tracks Correction">​</a></h3><p><strong>Objective:</strong> Have a correct face bounding box around all the faces
visible in the video</p><p><strong>Annotation Task:</strong></p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th></tr></thead><tbody><tr><td>1</td><td><em>For each frame in the video, identify all subjects in the frame and check to see if they have bounding boxes.</em></td><td>1. Subject has a bounding box (bbox): <br> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- -->a. Bbox is PASSING  →  Move onto the next subject in the frame. <br> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- -->b. Bbox is FAILING → Adjust/Re-draw the bbox (making sure the right face track is selected) <br> 2. Subject doesn&#x27;t have a bbox → Create a new bounding box and either assign it a new track or merge an existing face track. <br> 3. Bbox does not capture a face → Delete bbox.</td></tr></tbody></table><p><strong>Examples:</strong></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><strong>Passing</strong> Bbox</td><td><img loading="lazy" src="/docs/assets/images/image6-f8efcbef2b533452d325ccc01b140cbb.png" width="1276" height="715" class="img_ev3q"></td></tr><tr><td><strong>Failing</strong> Bbox</td><td><img loading="lazy" src="/docs/assets/images/image12-738add5129b302a3b521db6fdfec78d5.png" width="1276" height="711" class="img_ev3q"></td></tr><tr><td><strong>Missing</strong> Bbox</td><td><img loading="lazy" src="/docs/assets/images/image23-66ce8bdd734d6ef2b7bce5867fd1577d.png" width="1276" height="715" class="img_ev3q"></td></tr><tr><td>Bbox to be <strong>deleted</strong></td><td><img loading="lazy" src="/docs/assets/images/image20-681ac05983a459c200bdef80495a9985.png" width="1276" height="718" class="img_ev3q"></td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="av-step-2-speaker-labeling-and-av-anchor-extraction">AV Step 2: Speaker Labeling and AV anchor extraction<a href="#av-step-2-speaker-labeling-and-av-anchor-extraction" class="hash-link" aria-label="Direct link to AV Step 2: Speaker Labeling and AV anchor extraction" title="Direct link to AV Step 2: Speaker Labeling and AV anchor extraction">​</a></h3><p><strong>Objective:</strong> Assign each Face Track<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup> (from Step 1) a &#x27;Person ID&#x27;
(for each new subject which has an interaction with the camera-wearer or
is present in the camera for 500+ frames).</p><p><strong>Annotation Task:</strong></p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th></tr></thead><tbody><tr><td>1</td><td><em>Identify the &#x27;Next Track&#x27; and go to the first frame of this track.</em></td><td>1. Toggle On the &#x27;Out-of-Frame&#x27; Track List <br> 2. Select the next Track from the list <br> 3. Click &#x27;First Key Frame&#x27;</td></tr><tr><td>2</td><td><em>Assign this Track a unique ‘Person ID’ (e.g. Person 1, Person 2, ect)</em></td><td>1. Use the drop down menu to select a Person ID <br> 2. Each time this person appears in the video, assign their Track # to their designated Person ID</td></tr><tr><td>3</td><td><em>Repeat steps 1-4 until all tracks have Person ID’s assigned.</em></td><td></td></tr></tbody></table><p>|--------------------------|</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="av-step-3-speech-segmentation-per-speaker">AV Step 3: Speech Segmentation (Per Speaker)<a href="#av-step-3-speech-segmentation-per-speaker" class="hash-link" aria-label="Direct link to AV Step 3: Speech Segmentation (Per Speaker)" title="Direct link to AV Step 3: Speech Segmentation (Per Speaker)">​</a></h3><p><strong>Objective:</strong> Label voice activity for all subjects in the video.</p><p><strong>Annotation:</strong></p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th></tr></thead><tbody><tr><td>1</td><td><em>Label voice activity for the <strong>camera wearer</strong> first and then for each Person ID.</em></td><td>1. Annotate the video using the time segment tool. <br> 2. Start an annotation when a person makes a sound (speech, coughing, sigh, any utterance). <br> 3. Stop an annotation when a person stops making sounds. <br> 4. Do not stop an annotation if a person starts making sound again within 1 second after they stopped. <br> 5. Label the segment according to the Person ID displayed in the bounding box around their head. <br> 6. Repeat the process for all sounds made by the people in the video.</td></tr></tbody></table><p>|--------------------------|</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="av-step-4-transcription">AV Step 4: Transcription<a href="#av-step-4-transcription" class="hash-link" aria-label="Direct link to AV Step 4: Transcription" title="Direct link to AV Step 4: Transcription">​</a></h3><p><strong>Objective:</strong> Transcribe voice activity for all subjects in the video.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="av-step-5-correcting-speech-transcriptions-wip">AV Step 5: Correcting Speech Transcriptions <!-- -->[<!-- -->WIP<!-- -->]<a href="#av-step-5-correcting-speech-transcriptions-wip" class="hash-link" aria-label="Direct link to av-step-5-correcting-speech-transcriptions-wip" title="Direct link to av-step-5-correcting-speech-transcriptions-wip">​</a></h3><p><strong>Objective:</strong> Correcting Speech Transcription annotation from Step 4.</p><p><strong>Annotation Task:</strong></p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th></tr></thead><tbody><tr><td>0</td><td><em>Pre-load the annotation tool.</em></td><td>The task begins with the pre-load of the following things: <br> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- -->- Output of AV Step 3 (Speech Segmentation per Person ID) <br> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- -->- Output of AV Step 4 (Human transcriptions) <br> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- -->- Automatic transcriptions from ASR algorithms.</td></tr><tr><td>1</td><td><em>For each human transcription chunk, identify the corresponding person IDs with voice activity on.</em></td><td>For each person with the active voice activity: <br> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> - Listen to the video<br> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> <!-- --> - If the person’s speech is = to the content in the transcription chunk, then copy this speech content from transcript into a new dialog box/tag that corresponds to the person.</td></tr><tr><td>2</td><td><em>Repeat Step 1 for the machine generated transcription chunks</em></td><td></td></tr></tbody></table><p><strong>Examples:</strong></p><table><thead><tr><th>&lt; To Be Uploaded &gt;</th></tr></thead></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="social-step-1-camera-wearer-attention">Social Step 1: Camera-Wearer Attention<a href="#social-step-1-camera-wearer-attention" class="hash-link" aria-label="Direct link to Social Step 1: Camera-Wearer Attention" title="Direct link to Social Step 1: Camera-Wearer Attention">​</a></h3><p><strong>Objective</strong>: Annotate temporal segments in which a person is looking
at the camera wearer.</p><p><strong>Annotation Task:</strong></p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th></tr></thead><tbody><tr><td>1</td><td><em>Watch the video and find the time when someone is looking at the camera wearer</em></td><td></td></tr><tr><td>2</td><td><em>Annotate the time segment using the time segment tool:</em></td><td>1. Start an annotation when a person start to look at the camera wearer. <br> 2. Stop an annotation when a person stops looking at the camera wearer. <br> 3. Label the segment according to the Person ID displayed in the bounding box around their head. <br> 4. Repeat the process for all cases in the video.</td></tr></tbody></table><p>|--------------------------|</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="social-step-2-speech-target-classification">Social Step 2: Speech Target Classification<a href="#social-step-2-speech-target-classification" class="hash-link" aria-label="Direct link to Social Step 2: Speech Target Classification" title="Direct link to Social Step 2: Speech Target Classification">​</a></h3><p><strong>Objective</strong>: Given already annotated AV Voice Activity segmentation,
the annotator is going to annotate the particular speech segments in
which the person is talking to the camera wearer.</p><p><strong>Annotation Task:</strong></p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th></tr></thead><tbody><tr><td>1</td><td><em>Watch the video with AV voice segmentation results (start-end time, person ID)</em></td><td></td></tr><tr><td>2</td><td><em>Annotate segments where someone is talking to the camera wearer. Repeat the process for all cases in the video.</em></td><td>1. Identify a segment in which someone is talking to the camera wearer. <br> 2. Click the time segment, then you can see the Voice activity annotation information on the left side bar. <br> 3. Click the drop down box below the &quot;Target of Speech.&quot;<br><img loading="lazy" src="/docs/assets/images/image8-eb96212c0c12409b9412cb2e466c393b.png" width="506" height="162" class="img_ev3q"> <br> 4. In the dropdown menu, select &quot;Camera-Wearer&quot; if the speech is only toward the camera wearer. <br> 5. Choose &quot;Camera-Wearer and others&quot; if the speech segment is toward multiple people including the camera wearer (e.g., talking to multiple audience members). <br> 6. Repeat the process for all relevant segments.</td></tr></tbody></table></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/start-here/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Start Here</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/data/metadata/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Metadata</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#annotations-tldr" class="table-of-contents__link toc-highlight">Annotations tl;dr</a></li><li><a href="#narrations" class="table-of-contents__link toc-highlight">Narrations</a></li><li><a href="#benchmark-annotations" class="table-of-contents__link toc-highlight">Benchmark Annotations</a></li><li><a href="#episodic-memory" class="table-of-contents__link toc-highlight">Episodic Memory</a><ul><li><a href="#natural-language-queries" class="table-of-contents__link toc-highlight">Natural Language Queries</a></li><li><a href="#moments" class="table-of-contents__link toc-highlight">Moments</a></li><li><a href="#visual-object-queries" class="table-of-contents__link toc-highlight">Visual Object Queries</a></li></ul></li><li><a href="#forecasting--hands--objects-fho" class="table-of-contents__link toc-highlight">Forecasting + Hands &amp; Objects (FHO)</a><ul><li><a href="#stage-1---critical-frames" class="table-of-contents__link toc-highlight">Stage 1 - Critical Frames</a></li><li><a href="#stage-2---pre-condition" class="table-of-contents__link toc-highlight">Stage 2 - Pre-condition</a></li><li><a href="#stage-3---post-condition" class="table-of-contents__link toc-highlight">Stage 3 - Post-condition</a></li></ul></li><li><a href="#audio-visual-diarization--social-avs" class="table-of-contents__link toc-highlight">Audio-Visual Diarization &amp; Social (AVS)</a><ul><li><a href="#av-step-0-automated-face--head-detection" class="table-of-contents__link toc-highlight">AV Step 0: Automated Face &amp; Head Detection</a></li><li><a href="#av-step-1-face--head-tracks-correction" class="table-of-contents__link toc-highlight">AV Step 1: Face &amp; Head Tracks Correction</a></li><li><a href="#av-step-2-speaker-labeling-and-av-anchor-extraction" class="table-of-contents__link toc-highlight">AV Step 2: Speaker Labeling and AV anchor extraction</a></li><li><a href="#av-step-3-speech-segmentation-per-speaker" class="table-of-contents__link toc-highlight">AV Step 3: Speech Segmentation (Per Speaker)</a></li><li><a href="#av-step-4-transcription" class="table-of-contents__link toc-highlight">AV Step 4: Transcription</a></li><li><a href="#av-step-5-correcting-speech-transcriptions-wip" class="table-of-contents__link toc-highlight">AV Step 5: Correcting Speech Transcriptions [WIP]</a></li><li><a href="#social-step-1-camera-wearer-attention" class="table-of-contents__link toc-highlight">Social Step 1: Camera-Wearer Attention</a></li><li><a href="#social-step-2-speech-target-classification" class="table-of-contents__link toc-highlight">Social Step 2: Speech Target Classification</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/">Intro</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/data/annotation-guidelines/">Annotation Guidelines</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/challenge/">Ego4D Challenge</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/contact/">Contact Us</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Ego4d" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://ego4d-data.org/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Ego4D Main Site<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Ego4d</div></div></div></footer></div>
<script src="/docs/assets/js/runtime~main.dc5a4f68.js"></script>
<script src="/docs/assets/js/main.3e8d1ac0.js"></script>
</body>
</html>