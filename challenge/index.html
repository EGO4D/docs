<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<title data-react-helmet="true">Ego4D Challenge 2022 | Ego4D</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://ego4d-data.org//docs/challenge/"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Ego4D Challenge 2022 | Ego4D"><meta data-react-helmet="true" name="description" content="Please note, the challenge end date has been changed to September 18th to allow for more time for report submission and evaluation."><meta data-react-helmet="true" property="og:description" content="Please note, the challenge end date has been changed to September 18th to allow for more time for report submission and evaluation."><link data-react-helmet="true" rel="shortcut icon" href="/docs/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://ego4d-data.org//docs/challenge/"><link data-react-helmet="true" rel="alternate" href="https://ego4d-data.org//docs/challenge/" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://ego4d-data.org//docs/challenge/" hreflang="x-default"><link rel="stylesheet" href="/docs/assets/css/styles.7818fb39.css">
<link rel="preload" href="/docs/assets/js/runtime~main.f4f73420.js" as="script">
<link rel="preload" href="/docs/assets/js/main.491071e2.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/docs/"><div class="navbar__logo"><img src="/docs/img/ego-4d-logo.png" alt="Ego4d Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/docs/img/ego-4d-logo-dark.png" alt="Ego4d Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">Ego4D</b></a></div><div class="navbar__items navbar__items--right"><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">üåú</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">üåû</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_lDyR"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_i9tI" type="button"></button><aside class="docSidebarContainer_0YBq"><div class="sidebar_a3j0"><nav class="menu thin-scrollbar menu_cyFh"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/">Welcome To EGO4D!</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/start-here/">Start Here</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">Data</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">Benchmark Tasks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/model-zoo/">Model Zoo</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/viz/">Visualization Tool</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/CLI/">CLI Tool</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/privacy/">Privacy and Ethics</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/docs/challenge/">Ego4D Challenge 2022</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/FAQ/">FAQ</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/contact/">Contact Us</a></li></ul></nav></div></aside><main class="docMainContainer_r8cw"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_zHA2"><div class="docItemContainer_oiyr"><article><div class="tocCollapsible_aw-L theme-doc-toc-mobile tocMobile_Tx6Y"><button type="button" class="clean-btn tocCollapsibleButton_zr6a">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Ego4D Challenge 2022</h1></header><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>Challenge Ends 9/18/22</h5></div><div class="admonition-content"><p>Please note, the challenge end date has been changed to September 18th to allow for more time for report submission and evaluation.</p><p>Also note for VQ specifically, you&#x27;ll need to follow instructions <a href="https://eval.ai/web/challenges/challenge-page/1843/overview" target="_blank" rel="noopener noreferrer">here</a> to download updated annotations for the challenge.</p></div></div><h2 class="anchor anchorWithStickyNavbar_y2LR" id="overview">Overview<a aria-hidden="true" class="hash-link" href="#overview" title="Direct link to heading">‚Äã</a></h2><p>In 2022, we will host 16 challenges, representing each of Ego4D‚Äôs five benchmarks. These are: </p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="episodic-memory"><a href="/docs/benchmarks/episodic-memory/">Episodic memory</a>:<a aria-hidden="true" class="hash-link" href="#episodic-memory" title="Direct link to heading">‚Äã</a></h3><ul><li><a href="https://eval.ai/web/challenges/challenge-page/1843/overview" target="_blank" rel="noopener noreferrer">Visual queries with 2D localization</a> and <a href="https://eval.ai/web/challenges/challenge-page/1646/overview" target="_blank" rel="noopener noreferrer">VQ 3D localization</a>: Given an egocentric video clip and an image crop depicting the query object, return the last time the object was seen in the input video, in terms of the tracked bounding box (2D + temporal localization) or the 3D displacement vector from the camera to the object in the environment. </li><li><a href="https://eval.ai/web/challenges/challenge-page/1629/overview" target="_blank" rel="noopener noreferrer">Natural language queries</a>: Given a video clip and a query expressed in natural language, localize the temporal window within all the video history where the answer to the question is evident.   </li><li><a href="https://eval.ai/web/challenges/challenge-page/1626/overview" target="_blank" rel="noopener noreferrer">Moments queries</a>: Given an egocentric video and an activity name (e.g., a ‚Äúmoment‚Äù), localize all instances of that activity in the past video </li></ul><h3 class="anchor anchorWithStickyNavbar_y2LR" id="hands-and-objects"><a href="/docs/benchmarks/hands-and-objects/">Hands and Objects</a>:<a aria-hidden="true" class="hash-link" href="#hands-and-objects" title="Direct link to heading">‚Äã</a></h3><ul><li><a href="https://eval.ai/web/challenges/challenge-page/1622/overview" target="_blank" rel="noopener noreferrer">Temporal localization</a>: Given an egocentric video clip, localize temporally the key frames that indicate an object state change.</li><li><a href="https://eval.ai/web/challenges/challenge-page/1627/overview" target="_blank" rel="noopener noreferrer">Object state change classification</a>: Given an egocentric video clip, indicate the presence or absence of an object state change.</li><li><a href="https://eval.ai/web/challenges/challenge-page/1632/overview" target="_blank" rel="noopener noreferrer">State change object detection</a>: Given an egocentric video clip, identify the objects whose states are changing and outline them with bounding boxes. </li></ul><h3 class="anchor anchorWithStickyNavbar_y2LR" id="audio-visual-diarization--social"><a href="/docs/benchmarks/av-diarization/">Audio-Visual Diarization &amp; Social</a>:<a aria-hidden="true" class="hash-link" href="#audio-visual-diarization--social" title="Direct link to heading">‚Äã</a></h3><ul><li><a href="https://eval.ai/web/challenges/challenge-page/1633/overview" target="_blank" rel="noopener noreferrer">Audio-visual localization</a>: Given an egocentric video clip, localize the speakers in the visual field of view. </li><li><a href="https://eval.ai/web/challenges/challenge-page/1640/overview" target="_blank" rel="noopener noreferrer">Audio-visual speaker diarization</a>: Given an egocentric video clip, identify which person spoke and when they spoke.</li><li><a href="https://eval.ai/web/challenges/challenge-page/1641/overview" target="_blank" rel="noopener noreferrer">Audio-only Diarization Challenge</a>: Given an egocentric video clip, identify which person spoke and when they spoke based on audio alone.</li><li><a href="https://eval.ai/web/challenges/challenge-page/1637/overview" target="_blank" rel="noopener noreferrer">Speech transcription</a>: Given an egocentric video clip, transcribe the speech of each person.</li><li><a href="https://eval.ai/web/challenges/challenge-page/1625/overview" target="_blank" rel="noopener noreferrer">Talking to me</a>: Given an egocentric video clip, identify whether someone in the scene is talking to the camera wearer.</li><li><a href="https://eval.ai/web/challenges/challenge-page/1624/overview" target="_blank" rel="noopener noreferrer">Looking at me</a>: Given an egocentric video clip, identify whether someone in the scene is looking at the camera wearer.</li></ul><h3 class="anchor anchorWithStickyNavbar_y2LR" id="forecasting"><a href="/docs/benchmarks/forecasting/">Forecasting</a>:<a aria-hidden="true" class="hash-link" href="#forecasting" title="Direct link to heading">‚Äã</a></h3><ul><li><a href="https://eval.ai/web/challenges/challenge-page/1630/overview" target="_blank" rel="noopener noreferrer">Hand forecasting</a>: Given a short preceding video clip, predict where the hand will be visible in the future, in terms of a bounding box center in keyframes.  </li><li><a href="https://eval.ai/web/challenges/challenge-page/1623/overview" target="_blank" rel="noopener noreferrer">Short-term hand object prediction</a>: Given a video clip, predict the next active objects, the next action, and the time to contact. </li><li><a href="https://eval.ai/web/challenges/challenge-page/1598/overview" target="_blank" rel="noopener noreferrer">Long-term activity prediction</a>: Given a video clip, the goal is to predict what sequence of activities will happen in the future? For example, after kneading dough, what will the baker do next?  </li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="dataset">Dataset<a aria-hidden="true" class="hash-link" href="#dataset" title="Direct link to heading">‚Äã</a></h2><p>Ego4D challenge participants will use Ego4D‚Äôs annotated data set of more than 3,670 hours of video data, capturing the daily-life scenarios of more than 900 unique individuals from nine different countries around the world. Unique train, validation and unannotated test sets are available to download per challenge at <a href="https://ego4d-data.org/docs/" target="_blank" rel="noopener noreferrer">https://ego4d-data.org/docs/</a>. </p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="participation-guidelines">Participation Guidelines<a aria-hidden="true" class="hash-link" href="#participation-guidelines" title="Direct link to heading">‚Äã</a></h2><p>Participate in the contest by registering on the <a href="https://eval.ai/" target="_blank" rel="noopener noreferrer">EvalAI challenge page</a> and create a team. All participants must register as a part of a ‚Äúparticipating team‚Äù on EvalAI to ensure the submission limits are honored. Participants will upload their predictions in the format specified for the specific challenge, and will be evaluated on AWS instance by comparing to ground truth predictions. Instructions for training, local evaluation, and online submission are provided at EvalAI. Please refer to the individual EvalAI pages for each challenge for submission guidelines, task specifications, and evaluation criteria.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="dates">Dates<a aria-hidden="true" class="hash-link" href="#dates" title="Direct link to heading">‚Äã</a></h2><p>Top performing teams will be invited to speak at our accepted CVPR and ECCV workshops in June and October.  For the current ECCV challenge set, submissions will be due Sept 18th, 2022. </p><p><strong>October 1, 2022 (<a href="https://ego4d-data.org/Workshop/ECCV22/" target="_blank" rel="noopener noreferrer">ECCV Workshop</a>, phase 2)</strong></p><ul><li>Visual Queries</li><li>Natural Language Queries</li><li>PNR Temporal Localization</li><li>Object State Change Classification</li><li>Short-term Object Interaction Anticipation</li><li>Long-term Action Anticipation</li><li>Moments Queries </li><li>3D Localization </li><li>State Change Object Detection </li><li>Localization and Tracking </li><li>Diarization (Audio)</li><li>Diarization (Audio+Video)</li><li>Transcription </li><li>Looking at Me</li><li>Talking to Me</li><li>Future Hand Prediction</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="competition-rules-and-prize-information">Competition Rules and Prize Information<a aria-hidden="true" class="hash-link" href="#competition-rules-and-prize-information" title="Direct link to heading">‚Äã</a></h2><p>Competition rules can be found <a href="https://ego4d-interactive-fig1.s3.eu-west-2.amazonaws.com/tc.pdf" target="_blank" rel="noopener noreferrer">here</a>. Additionally, we are thrilled that FAIR is able to offer the following prize thresholds per challenges: </p><ul><li>First place: $3,000 </li><li>Second place: $2,000</li><li>Third place: $1,000</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="challenge-reports">Challenge Reports<a aria-hidden="true" class="hash-link" href="#challenge-reports" title="Direct link to heading">‚Äã</a></h2><p>In addition to the submission on EvalAI, participants must submit a report describing their method to the workshop CMT (link TBD). In addition to your method and results, please remember to include examples of positive and negative results (limitations) of your model.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="acknowledgements">Acknowledgements<a aria-hidden="true" class="hash-link" href="#acknowledgements" title="Direct link to heading">‚Äã</a></h2><p>The Ego4D challenge would not have been possible without the infrastructure and support of the <a href="https://eval.ai/team" target="_blank" rel="noopener noreferrer">EvalAI team</a>. Thank you! </p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="organizers">Organizers<a aria-hidden="true" class="hash-link" href="#organizers" title="Direct link to heading">‚Äã</a></h3><p><strong>Rohit Girdhar</strong></p><p>Santhosh Kumar Ramakrishnan	</p><p>Chen Zhao</p><p>Merey Ramazanova</p><p>Satwik Kottur	</p><p>Mengmeng Xu</p><p>Vincent Cartillier	</p><p>Yifei Huang	</p><p>Qichen Fu	</p><p>Siddhant Bansal	</p><p>Hao Jiang	</p><p>Vamsi Ithapu</p><p>Jachym Kolar</p><p>Christian Fuegen,</p><p>Leda Sari</p><p>Eric Zhongcong Xu	 </p><p>Yunyi Zhu  </p><p>Murong Ma </p><p>Zachary Chavis	</p><p>Wenqi Jia</p><p>Miao Liu</p><p>Antonino Furnari	</p><p>Tushar Nagarajan</p><p>Karttikeya Mangalam </p><p>Dima Damen</p><p>Giovanni Maria Farinella</p><p>Michael Wray</p><p>Gene Byrne</p><p>Andrew Westbury</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/privacy/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">¬´ <!-- -->Privacy and Ethics</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/FAQ/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">FAQ<!-- --> ¬ª</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_vrFS thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#overview" class="table-of-contents__link toc-highlight">Overview</a><ul><li><a href="#episodic-memory" class="table-of-contents__link toc-highlight">Episodic memory:</a></li><li><a href="#hands-and-objects" class="table-of-contents__link toc-highlight">Hands and Objects:</a></li><li><a href="#audio-visual-diarization--social" class="table-of-contents__link toc-highlight">Audio-Visual Diarization & Social:</a></li><li><a href="#forecasting" class="table-of-contents__link toc-highlight">Forecasting:</a></li></ul></li><li><a href="#dataset" class="table-of-contents__link toc-highlight">Dataset</a></li><li><a href="#participation-guidelines" class="table-of-contents__link toc-highlight">Participation Guidelines</a></li><li><a href="#dates" class="table-of-contents__link toc-highlight">Dates</a></li><li><a href="#competition-rules-and-prize-information" class="table-of-contents__link toc-highlight">Competition Rules and Prize Information</a></li><li><a href="#challenge-reports" class="table-of-contents__link toc-highlight">Challenge Reports</a></li><li><a href="#acknowledgements" class="table-of-contents__link toc-highlight">Acknowledgements</a><ul><li><a href="#organizers" class="table-of-contents__link toc-highlight">Organizers</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/">Intro</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/data/annotation-guidelines/">Annotation Guidelines</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/challenge/">Ego4D Challenge</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/contact/">Contact Us</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/Ego4d" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://ego4d-data.org/" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Ego4D Main Site<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ¬© 2022 Ego4d</div></div></div></footer></div>
<script src="/docs/assets/js/runtime~main.f4f73420.js"></script>
<script src="/docs/assets/js/main.491071e2.js"></script>
</body>
</html>