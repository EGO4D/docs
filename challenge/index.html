<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-challenge">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">Ego4D Challenge 2023 | Ego4D</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ego4d-data.org/docs/challenge/"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Ego4D Challenge 2023 | Ego4D"><meta data-rh="true" name="description" content="Overview"><meta data-rh="true" property="og:description" content="Overview"><link data-rh="true" rel="icon" href="/docs/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://ego4d-data.org/docs/challenge/"><link data-rh="true" rel="alternate" href="https://ego4d-data.org/docs/challenge/" hreflang="en"><link data-rh="true" rel="alternate" href="https://ego4d-data.org/docs/challenge/" hreflang="x-default"><link rel="stylesheet" href="/docs/assets/css/styles.d2213c09.css">
<link rel="preload" href="/docs/assets/js/runtime~main.deea6f01.js" as="script">
<link rel="preload" href="/docs/assets/js/main.3e8d1ac0.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><a class="navbar__brand" href="/docs/"><div class="navbar__logo"><img src="/docs/img/ego-4d-logo.png" alt="Ego4d Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/docs/img/ego-4d-logo-dark.png" alt="Ego4d Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Ego4D</b></a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/">Welcome To EGO4D!</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/start-here/">Start Here</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/data/annotation-guidelines/">Data</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/benchmarks/overview/">Benchmark Tasks</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/CLI/">CLI Tool</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/viz/">Visualization Tool</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/privacy/">Privacy and Ethics</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/model-zoo/">Model Zoo</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/docs/challenge/">Ego4D Challenge 2023</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/updates/">Updates</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/FAQ/">FAQ</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/contact/">Contact Us</a></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/docs/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Ego4D Challenge 2023</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Ego4D Challenge 2023</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2><p>In CVPR 2023, we will host <strong>14</strong> challenges including 2 new challenges (EgoTracks &amp; PACO Zero-Shot), representing each of Ego4D’s five benchmarks. These are:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="episodic-memory"><a href="/docs/benchmarks/episodic-memory/">Episodic memory</a>:<a href="#episodic-memory" class="hash-link" aria-label="Direct link to episodic-memory" title="Direct link to episodic-memory">​</a></h3><ul><li><a href="https://eval.ai/web/challenges/challenge-page/1843/overview" target="_blank" rel="noopener noreferrer">Visual queries with 2D localization (VQ2D)</a> and <a href="https://eval.ai/web/challenges/challenge-page/1646/overview" target="_blank" rel="noopener noreferrer">Visual Queries 3D localization (VQ3D)</a>: Given an egocentric video clip and an image crop depicting the query object, return the most recent occurrence of the object in the input video, in terms of contiguous bounding boxes (2D + temporal localization) or the 3D displacement vector from the camera to the object in the environment. <ul><li>Quickstart: <a href="https://colab.research.google.com/drive/1vtVOQzLarBCspQjH5RtHZ8qzH0VZxrmZ?usp=sharing" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" class="img_ev3q"></a></li></ul></li><li><a href="https://eval.ai/web/challenges/challenge-page/1629/overview" target="_blank" rel="noopener noreferrer">Natural language queries (NLQ)</a>: Given a video clip and a query expressed in natural language, localize the temporal window within all the video history where the answer to the question is evident.<ul><li>Quickstart: <a href="https://colab.research.google.com/drive/1S1LTplak-Fno3lMumCLoIfzYsx_TfNes?usp=sharing" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" class="img_ev3q"></a></li></ul></li><li><a href="https://eval.ai/web/challenges/challenge-page/1626/overview" target="_blank" rel="noopener noreferrer">Moments queries (MQ)</a>: Given an egocentric video and an activity name (e.g., a “moment”), localize all instances of that activity in the past video</li><li><a href="https://eval.ai/web/challenges/challenge-page/1969/overview" target="_blank" rel="noopener noreferrer">EgoTracks</a>: Given an egocentric video and a visual template of an object, localize the bounding box containing the object in each frame of the video along with a confidence score representing the presence of the object. <strong>[NEW for 2023]</strong></li><li><a href="https://eval.ai/web/challenges/challenge-page/1970/overview" target="_blank" rel="noopener noreferrer">PACO Zero-Shot:</a> Retrieve the bounding box of a specific object instance from a dataset, based on a textual query describing the instance. Query is composed using object and part attributes describing the object of interest. <strong>[NEW for 2023]</strong></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="hands-and-objects"><a href="/docs/benchmarks/hands-and-objects/">Hands and Objects</a>:<a href="#hands-and-objects" class="hash-link" aria-label="Direct link to hands-and-objects" title="Direct link to hands-and-objects">​</a></h3><ul><li><a href="https://eval.ai/web/challenges/challenge-page/1622/overview" target="_blank" rel="noopener noreferrer">Temporal localization</a>: Given an egocentric video clip, localize temporally the key frames that indicate an object state change.</li><li><a href="https://eval.ai/web/challenges/challenge-page/1627/overview" target="_blank" rel="noopener noreferrer">Object state change classification</a>: Given an egocentric video clip, indicate the presence or absence of an object state change.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="audio-visual-diarization"><a href="/docs/benchmarks/av-diarization/">Audio-Visual Diarization</a>:<a href="#audio-visual-diarization" class="hash-link" aria-label="Direct link to audio-visual-diarization" title="Direct link to audio-visual-diarization">​</a></h3><ul><li><a href="https://eval.ai/web/challenges/challenge-page/1640/overview" target="_blank" rel="noopener noreferrer">Audio-visual speaker diarization</a>: Given an egocentric video clip, identify which person spoke and when they spoke.</li><li><a href="https://eval.ai/web/challenges/challenge-page/1637/overview" target="_blank" rel="noopener noreferrer">Speech transcription</a>: Given an egocentric video clip, transcribe the speech of each person.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="social-understanding"><a href="/docs/benchmarks/social/">Social Understanding</a>:<a href="#social-understanding" class="hash-link" aria-label="Direct link to social-understanding" title="Direct link to social-understanding">​</a></h3><ul><li><a href="https://eval.ai/web/challenges/challenge-page/1625/overview" target="_blank" rel="noopener noreferrer">Talking to me</a>: Given an egocentric video clip, identify whether someone in the scene is talking to the camera wearer.</li><li><a href="https://eval.ai/web/challenges/challenge-page/1624/overview" target="_blank" rel="noopener noreferrer">Looking at me</a>: Given an egocentric video clip, identify whether someone in the scene is looking at the camera wearer.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="forecasting"><a href="/docs/benchmarks/forecasting/">Forecasting</a>:<a href="#forecasting" class="hash-link" aria-label="Direct link to forecasting" title="Direct link to forecasting">​</a></h3><ul><li><a href="https://eval.ai/web/challenges/challenge-page/1623/overview" target="_blank" rel="noopener noreferrer">Short-term hand object prediction</a>: Given a video clip, predict the next active objects, and, for each of them, predict the next action, and the time to contact.<ul><li>Quickstart: <a href="https://colab.research.google.com/drive/1Ok_6F1O6K8kX1S4sEnU62HoOBw_CPngR?usp=sharing" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" class="img_ev3q"></a></li></ul></li><li><a href="https://eval.ai/web/challenges/challenge-page/1598/overview" target="_blank" rel="noopener noreferrer">Long-term activity prediction</a>: Given a video clip, the goal is to predict what sequence of activities will happen in the future. For example, after kneading dough, list the actions that the baker will do next. </li></ul><p>Other Ego4D challenges which are not part of CVPR 2023 workshop remain open on EvalAI website for submissions but are not eligible for prizes.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="dataset">Dataset<a href="#dataset" class="hash-link" aria-label="Direct link to Dataset" title="Direct link to Dataset">​</a></h2><p>Ego4D challenge participants will use Ego4D’s annotated data set of more than 3,670 hours of video data, capturing the daily-life scenarios of more than 900 unique individuals from nine different countries around the world. Unique train, validation and unannotated test sets are available to download per challenge at <a href="https://ego4d-data.org/docs/" target="_blank" rel="noopener noreferrer">https://ego4d-data.org/docs/</a>.</p><p>This year&#x27;s challenge will use Ego4D v2.0 which contains ~2X train and val annotations for Forecasting, Hands &amp; Objects and NLQ, a number of corrections and usability enhancements, and two new related dataset enhancements (PACO &amp; EgoTracks). The test set remains the same as previous versions of the challenge. More details can be found <a href="https://ego4d-data.org/docs/updates/" target="_blank" rel="noopener noreferrer">here</a>. We have also updated the baselines for NLQ, MQ, VQ2D and forecasting tasks leveraging more training data available in Ego4D v2.0 release. </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="participation-guidelines">Participation Guidelines<a href="#participation-guidelines" class="hash-link" aria-label="Direct link to Participation Guidelines" title="Direct link to Participation Guidelines">​</a></h2><p>Participate in the contest by registering on the <a href="https://eval.ai/" target="_blank" rel="noopener noreferrer">EvalAI challenge page</a> and create a team. All participants must register as a part of a “participating team” on EvalAI to ensure the submission limits are honored. Participants will upload their predictions in the format specified for the specific challenge, and will be evaluated on AWS instances by comparing to ground truth predictions. Instructions for training, local evaluation, and online submission are provided at EvalAI. Please refer to the individual EvalAI pages for each challenge for submission guidelines, task specifications, and evaluation criteria.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="dates">Dates<a href="#dates" class="hash-link" aria-label="Direct link to Dates" title="Direct link to Dates">​</a></h2><p>The challenge will launch on March 1, 2023 with the leaderboard closing on May 19, 2023. Winners will be announced at the <a href="https://sites.google.com/view/ego4d-epic-cvpr2023-workshop/" target="_blank" rel="noopener noreferrer">Joint International 3rd Ego4D and 11th EPIC Workshop</a> at CVPR 2023. Top performing teams may be invited to speak at the workshop.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="competition-rules-and-prize-information">Competition Rules and Prize Information<a href="#competition-rules-and-prize-information" class="hash-link" aria-label="Direct link to Competition Rules and Prize Information" title="Direct link to Competition Rules and Prize Information">​</a></h2><p>Competition rules can be found <a href="/docs/tc.pdf" target="_blank" rel="noopener noreferrer">here</a>. Additionally, we are thrilled that FAIR is able to offer the following prize thresholds per challenges:</p><ul><li>First place: $1500</li><li>Second place: $1000</li><li>Third place: $500</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="challenge-reports">Challenge Reports<a href="#challenge-reports" class="hash-link" aria-label="Direct link to Challenge Reports" title="Direct link to Challenge Reports">​</a></h2><p>In addition to the submission on EvalAI, participants must submit a report describing their method to the workshop CMT (link TBD). In addition to your method and results, please remember to include examples of positive and negative results (limitations) of your model. These validation reports will be evaluated by challenge hosts from the Ego4D consortium before winner determination can be made. Similarly, challenge validation reports, research code from winning entries, and names of participants from the winning teams for all successful submissions must be shared publicly with the research community.   </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="acknowledgements">Acknowledgements<a href="#acknowledgements" class="hash-link" aria-label="Direct link to Acknowledgements" title="Direct link to Acknowledgements">​</a></h2><p>The Ego4D challenge would not have been possible without the infrastructure and support of the <a href="https://eval.ai/team" target="_blank" rel="noopener noreferrer">EvalAI team</a>. Thank you!</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="organizers">Organizers<a href="#organizers" class="hash-link" aria-label="Direct link to Organizers" title="Direct link to Organizers">​</a></h3><ul><li><strong>Suyog Jain</strong></li><li><strong>Rohit Girdhar</strong></li><li><strong>Andrew Westbury</strong></li><li>Santhosh Kumar Ramakrishnan</li><li>Chen Zhao</li><li>Merey Ramazanova</li><li>Satwik Kottur</li><li>Mengmeng Xu</li><li>Vincent Cartillier </li><li>Yifei Huang</li><li>Qichen Fu</li><li>Siddhant Bansal</li><li>Hao Jiang</li><li>Vamsi Ithapu</li><li>Jachym Kolar</li><li>Christian Fuegen</li><li>Leda Sari</li><li>Eric Zhongcong Xu </li><li>Zachary Chavis </li><li>Wenqi Jia</li><li>Miao Liu</li><li>Antonino Furnari</li><li>Francesco Ragusa </li><li>Tushar Nagarajan</li><li>Dima Damen</li><li>Giovanni Maria Farinella</li><li>Michael Wray</li><li>Hao Tang</li><li>Kevin Liang</li><li>Weiyao Wang</li><li>Vladan Petrovic</li><li>Anmol Kalia</li><li>Vignesh Ramanathan</li><li>Dhruv Mahajan</li><li>Gene Byrne</li><li>Matt Feiszli</li><li>Kristen Grauman</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="past-challenges--winners">Past Challenges / Winners<a href="#past-challenges--winners" class="hash-link" aria-label="Direct link to Past Challenges / Winners" title="Direct link to Past Challenges / Winners">​</a></h2><p><strong><a href="https://ego4d-data.org/workshops/eccv22/" target="_blank" rel="noopener noreferrer">ECCV Workshop 2022</a></strong> (Oct 24, 2022)</p><p><strong><a href="https://ego4d-data.org/workshops/cvpr22/" target="_blank" rel="noopener noreferrer">CVPR Workshop 2022</a></strong> (June 19, 2022)</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/model-zoo/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Model Zoo</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/updates/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Updates</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#overview" class="table-of-contents__link toc-highlight">Overview</a><ul><li><a href="#episodic-memory" class="table-of-contents__link toc-highlight">Episodic memory:</a></li><li><a href="#hands-and-objects" class="table-of-contents__link toc-highlight">Hands and Objects:</a></li><li><a href="#audio-visual-diarization" class="table-of-contents__link toc-highlight">Audio-Visual Diarization:</a></li><li><a href="#social-understanding" class="table-of-contents__link toc-highlight">Social Understanding:</a></li><li><a href="#forecasting" class="table-of-contents__link toc-highlight">Forecasting:</a></li></ul></li><li><a href="#dataset" class="table-of-contents__link toc-highlight">Dataset</a></li><li><a href="#participation-guidelines" class="table-of-contents__link toc-highlight">Participation Guidelines</a></li><li><a href="#dates" class="table-of-contents__link toc-highlight">Dates</a></li><li><a href="#competition-rules-and-prize-information" class="table-of-contents__link toc-highlight">Competition Rules and Prize Information</a></li><li><a href="#challenge-reports" class="table-of-contents__link toc-highlight">Challenge Reports</a></li><li><a href="#acknowledgements" class="table-of-contents__link toc-highlight">Acknowledgements</a><ul><li><a href="#organizers" class="table-of-contents__link toc-highlight">Organizers</a></li></ul></li><li><a href="#past-challenges--winners" class="table-of-contents__link toc-highlight">Past Challenges / Winners</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/">Intro</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/data/annotation-guidelines/">Annotation Guidelines</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/challenge/">Ego4D Challenge</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/contact/">Contact Us</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Ego4d" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://ego4d-data.org/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Ego4D Main Site<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Ego4d</div></div></div></footer></div>
<script src="/docs/assets/js/runtime~main.deea6f01.js"></script>
<script src="/docs/assets/js/main.3e8d1ac0.js"></script>
</body>
</html>