<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-updates">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">Updates | Ego4D</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://ego4d-data.org/docs/updates/"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Updates | Ego4D"><meta data-rh="true" name="description" content="Note that the FHO hands &amp; SCOD (state change object detection) v2 train &amp; val jsons are currently not yet available and will be released shortly."><meta data-rh="true" property="og:description" content="Note that the FHO hands &amp; SCOD (state change object detection) v2 train &amp; val jsons are currently not yet available and will be released shortly."><link data-rh="true" rel="icon" href="/docs/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://ego4d-data.org/docs/updates/"><link data-rh="true" rel="alternate" href="https://ego4d-data.org/docs/updates/" hreflang="en"><link data-rh="true" rel="alternate" href="https://ego4d-data.org/docs/updates/" hreflang="x-default"><link rel="stylesheet" href="/docs/assets/css/styles.d2213c09.css">
<link rel="preload" href="/docs/assets/js/runtime~main.deea6f01.js" as="script">
<link rel="preload" href="/docs/assets/js/main.3e8d1ac0.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><a class="navbar__brand" href="/docs/"><div class="navbar__logo"><img src="/docs/img/ego-4d-logo.png" alt="Ego4d Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/docs/img/ego-4d-logo-dark.png" alt="Ego4d Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Ego4D</b></a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/">Welcome To EGO4D!</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/start-here/">Start Here</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/data/annotation-guidelines/">Data</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/benchmarks/overview/">Benchmark Tasks</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/CLI/">CLI Tool</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/viz/">Visualization Tool</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/privacy/">Privacy and Ethics</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/model-zoo/">Model Zoo</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/challenge/">Ego4D Challenge 2023</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/docs/updates/">Updates</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/FAQ/">FAQ</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/contact/">Contact Us</a></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/docs/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Updates</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Updates</h1><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>FHO hands &amp; SCOD train/val pending</div><div class="admonitionContent_S0QG"><p>Note that the FHO hands &amp; SCOD (state change object detection) v2 train &amp; val jsons are currently not yet available and will be released shortly.</p></div></div><p>Ego4D was recently updated to version v2.0 (Feb &#x27;23) - focused on additional Forecasting, Hands &amp; Objects (243 hrs vs 120 hrs) and Natural Language Queries (27k vs 17.3k queries) annotations, a number of corrections and usability enhancements, and two new related dataset enhancements (PACO object and EgoTracks tracking path annotations).  Details below.</p><p>CVPR challenges for this year will be based on the updated dataset for FHO &amp; NLQ benchmarks.</p><p>As ever, we would love any questions or feedback on current or future changes, observed issues with the dataset and requests for future updates/utilities/dataloaders/examples/etc.  Please join us on the Ego4D Forum: <a href="https://discuss.ego4d-data.org/" target="_blank" rel="noopener noreferrer">https://discuss.ego4d-data.org/</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="v20-update">v2.0 Update<a href="#v20-update" class="hash-link" aria-label="Direct link to v2.0 Update" title="Direct link to v2.0 Update">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="fho-annotation-updates">FHO Annotation Updates<a href="#fho-annotation-updates" class="hash-link" aria-label="Direct link to FHO Annotation Updates" title="Direct link to FHO Annotation Updates">​</a></h3><p>Forecasting, Hands &amp; Objects has roughly double the annotations compared to v1 (243 hours of annotated clips vs 120 hrs in v1), split in the same 75/25 proportion in train/val.  Benchmark task jsons have been updated accordingly for train/val, though the unannotated test json&#x27;s remain the same (and can be submitted to the same eval.ai challenges as with v1). </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="nlq-annotation-updates">NLQ Annotation Updates<a href="#nlq-annotation-updates" class="hash-link" aria-label="Direct link to NLQ Annotation Updates" title="Direct link to NLQ Annotation Updates">​</a></h3><p>Natural Language Queries has been updated to include additional query and video coverage. For train and val, there are now 27k validated queries vs 17.3k in v1.</p><p>In addition to a a number of queries were 0s (i.e. point in time with start=end) and those intervals have been annotated to correct that. (See: <a href="https://github.com/EGO4D/episodic-memory/issues/14" target="_blank" rel="noopener noreferrer">https://github.com/EGO4D/episodic-memory/issues/14</a> &amp; special thanks to @<a href="https://github.com/md-mohaiminul" target="_blank" rel="noopener noreferrer">md-mohaiminul</a> for the flag there.)   </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="erroneous-videos-removed-from-dataset--benchmarks">Erroneous Videos Removed From Dataset &amp; Benchmarks<a href="#erroneous-videos-removed-from-dataset--benchmarks" class="hash-link" aria-label="Direct link to Erroneous Videos Removed From Dataset &amp; Benchmarks" title="Direct link to Erroneous Videos Removed From Dataset &amp; Benchmarks">​</a></h3><p>A small number of videos were removed outright from the dataset - 1 video with frozen frames, 1 with varying resolution, and several videos too short to be practically useful.  A small number (&lt; 5) of stereo videos were in benchmark splits (including test in some cases) and were removed (but remain in the dataset appropriately flagged in metadata).  </p><p>Specific uids removed are provided in the changelog and the ego4d.json metadata has been updated appropriately.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="additional-annotations-formats-fho_mainjson">Additional annotations formats (fho_main.json)<a href="#additional-annotations-formats-fho_mainjson" class="hash-link" aria-label="Direct link to Additional annotations formats (fho_main.json)" title="Direct link to Additional annotations formats (fho_main.json)">​</a></h3><p>There have been several requests for additional annotation formats, particularly for a unified json across the FHO tasks, which is now available. Please see the schemas page for additional details: <a href="/docs/data/annotations-schemas/">annotation schemas</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="vq-annotation-updates">VQ Annotation Updates<a href="#vq-annotation-updates" class="hash-link" aria-label="Direct link to VQ Annotation Updates" title="Direct link to VQ Annotation Updates">​</a></h3><p>Some bounding boxes were incorrectly rotated for VQ2D annotations and have been corrected.  This is the only set of corrections which is expected to have a significant impact on results and we will share updated baseline numbers with the release.  Note that this was corrected in the v1.0.5 version of the dataset for the prior &#x27;22 VQ challenge and was the existing annotations for that challenge.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="paco-parts-and-attributes-of-common-objects-dataset">PACO (Parts and Attributes of Common Objects) Dataset<a href="#paco-parts-and-attributes-of-common-objects-dataset" class="hash-link" aria-label="Direct link to PACO (Parts and Attributes of Common Objects) Dataset" title="Direct link to PACO (Parts and Attributes of Common Objects) Dataset">​</a></h3><p><a href="https://arxiv.org/abs/2301.01795" target="_blank" rel="noopener noreferrer">PACO (Parts and Attributes of Common Objects) Dataset</a></p><p>Object models are gradually progressing from predicting just category labels to providing detailed descriptions of object instances. This motivates the need for large datasets which go beyond traditional object masks and provide richer annotations such as part masks and attributes. Hence, we introduce PACO: Parts and Attributes of Common Objects. It spans 75 object categories, 456 object-part categories and 55 attributes across image (LVIS) and video (Ego4D) datasets. We provide 641K part masks annotated across 260K object boxes, with roughly half of them exhaustively annotated with attributes as well. We design evaluation metrics and provide benchmark results for three tasks on the dataset: part mask segmentation, object and part attribute prediction and zero-shot instance detection. Dataset, models, and code are open-sourced: <a href="https://github.com/facebookresearch/paco" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/paco</a></p><p><img loading="lazy" alt="PACO" src="/docs/assets/images/paco_challenge_sample-cdb71e1478f5bebbbb8b892620ade5d1.png" width="3121" height="2048" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="egotracks">EgoTracks<a href="#egotracks" class="hash-link" aria-label="Direct link to EgoTracks" title="Direct link to EgoTracks">​</a></h3><p><a href="/docs/data/egotracks/">EgoTracks Documentation</a></p><p><a href="https://arxiv.org/abs/2301.03213" target="_blank" rel="noopener noreferrer">EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset</a></p><p>Visual object tracking is a key component to many egocentric vision problems. However, the full spectrum of challenges of egocentric tracking faced by an embodied AI is underrepresented in many existing datasets; these tend to focus on relatively short, third-person videos. Egocentric video has several distinguishing characteristics from those commonly found in past datasets: frequent large camera motions and hand interactions with objects commonly lead to occlusions or objects exiting the frame, and object appearance can change rapidly due to widely different points of view, scale, or object states. Embodied tracking is also naturally long-term, and being able to consistently (re-)associate objects to their appearances and disappearances over as long as a lifetime is critical. Previous datasets under-emphasize this re-detection problem, and their &quot;framed&quot; nature has led to adoption of various spatiotemporal priors that we find do not necessarily generalize to egocentric video. We thus introduce EgoTracks, a new dataset for long-term egocentric visual object tracking. Sourced from the Ego4D dataset, this new dataset presents a significant challenge to recent state-of-the-art single-object tracking models, which we find score poorly on traditional tracking metrics for our new dataset, compared to popular benchmarks. We further show improvements that can be made to a STARK tracker to significantly increase its performance on egocentric data, resulting in a baseline model we call EgoSTARK. We publicly release our annotations and benchmark, hoping our dataset leads to further advancements in tracking.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="prior-incremental-updates">Prior Incremental Updates<a href="#prior-incremental-updates" class="hash-link" aria-label="Direct link to Prior Incremental Updates" title="Direct link to Prior Incremental Updates">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="downsampled-videos">Downsampled Videos<a href="#downsampled-videos" class="hash-link" aria-label="Direct link to Downsampled Videos" title="Direct link to Downsampled Videos">​</a></h3><p>Downsampled videos have been added - download as dataset <code>video_540ss</code> via the <a href="https://github.com/facebookresearch/Ego4d/blob/main/ego4d/cli/README.md" target="_blank" rel="noopener noreferrer">CLI</a>.  Videos have been scaled to 540px on the short side - more details available via the <a href="/docs/data/videos/">docs</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="feature-updates">Feature Updates<a href="#feature-updates" class="hash-link" aria-label="Direct link to Feature Updates" title="Direct link to Feature Updates">​</a></h3><p>An error in feature generation that added some noise was corrected.  Full detail will be provided shortly, and the features have already been updated.</p><p>Omnivore FP16 features were added: <a href="https://ego4d-data.org/docs/data/features/" target="_blank" rel="noopener noreferrer">Features</a></p><p>Additional features will be released shortly as well.  (EgoVLP &amp; Narration Embeddings)</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="further-details">Further Details<a href="#further-details" class="hash-link" aria-label="Direct link to Further Details" title="Direct link to Further Details">​</a></h2><p>Upon release, the <a href="https://github.com/facebookresearch/Ego4d/blob/main/CHANGELOG" target="_blank" rel="noopener noreferrer">CHANGELOG</a> has more detailed specifics on the update as well.  And we&#x27;ll post on the <a href="https://discuss.ego4d-data.org/" target="_blank" rel="noopener noreferrer">Ego4D forum</a> with details and follow up discussion there.</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/challenge/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Ego4D Challenge 2023</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/FAQ/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">FAQ</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#v20-update" class="table-of-contents__link toc-highlight">v2.0 Update</a><ul><li><a href="#fho-annotation-updates" class="table-of-contents__link toc-highlight">FHO Annotation Updates</a></li><li><a href="#nlq-annotation-updates" class="table-of-contents__link toc-highlight">NLQ Annotation Updates</a></li><li><a href="#erroneous-videos-removed-from-dataset--benchmarks" class="table-of-contents__link toc-highlight">Erroneous Videos Removed From Dataset &amp; Benchmarks</a></li><li><a href="#additional-annotations-formats-fho_mainjson" class="table-of-contents__link toc-highlight">Additional annotations formats (fho_main.json)</a></li><li><a href="#vq-annotation-updates" class="table-of-contents__link toc-highlight">VQ Annotation Updates</a></li><li><a href="#paco-parts-and-attributes-of-common-objects-dataset" class="table-of-contents__link toc-highlight">PACO (Parts and Attributes of Common Objects) Dataset</a></li><li><a href="#egotracks" class="table-of-contents__link toc-highlight">EgoTracks</a></li></ul></li><li><a href="#prior-incremental-updates" class="table-of-contents__link toc-highlight">Prior Incremental Updates</a><ul><li><a href="#downsampled-videos" class="table-of-contents__link toc-highlight">Downsampled Videos</a></li><li><a href="#feature-updates" class="table-of-contents__link toc-highlight">Feature Updates</a></li></ul></li><li><a href="#further-details" class="table-of-contents__link toc-highlight">Further Details</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/">Intro</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/data/annotation-guidelines/">Annotation Guidelines</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/challenge/">Ego4D Challenge</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/contact/">Contact Us</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Ego4d" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://ego4d-data.org/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Ego4D Main Site<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Ego4d</div></div></div></footer></div>
<script src="/docs/assets/js/runtime~main.deea6f01.js"></script>
<script src="/docs/assets/js/main.3e8d1ac0.js"></script>
</body>
</html>