<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<title data-react-helmet="true">Ego4D Data Overview | Ego4D</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://ego4d-data.org//docs/data-overview/"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Ego4D Data Overview | Ego4D"><meta data-react-helmet="true" name="description" content="WIP: Includes internal links, broken markdown, etc to be resolved"><meta data-react-helmet="true" property="og:description" content="WIP: Includes internal links, broken markdown, etc to be resolved"><link data-react-helmet="true" rel="shortcut icon" href="/docs/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://ego4d-data.org//docs/data-overview/"><link data-react-helmet="true" rel="alternate" href="https://ego4d-data.org//docs/data-overview/" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://ego4d-data.org//docs/data-overview/" hreflang="x-default"><link rel="stylesheet" href="/docs/assets/css/styles.f4e51aac.css">
<link rel="preload" href="/docs/assets/js/runtime~main.09a49ffe.js" as="script">
<link rel="preload" href="/docs/assets/js/main.9f1b178e.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/docs/"><div class="navbar__logo"><img src="/docs/img/ego-4d-logo.png" alt="Ego4d Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/docs/img/ego-4d-logo-dark.png" alt="Ego4d Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">Ego4D</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/intro/">Start Here</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/EGO4D/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ðŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ðŸŒž</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_lDyR"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_i9tI" type="button"></button><aside class="docSidebarContainer_0YBq"><div class="sidebar_a3j0"><nav class="menu thin-scrollbar menu_cyFh"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro/">Welcome To EGO4D!</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/start-here/">Start Here</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">Benchmark Tasks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/docs/data-overview/">Ego4D Data Overview</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/annotations/">Annotations</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/challenge/">Ego4D Challenge</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/contact/">Contact Us</a></li></ul></nav></div></aside><main class="docMainContainer_r8cw"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_zHA2"><div class="docItemContainer_oiyr"><article><div class="tocCollapsible_aw-L theme-doc-toc-mobile tocMobile_Tx6Y"><button type="button" class="clean-btn tocCollapsibleButton_zr6a">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Ego4D Data Overview</h1></header><p><strong>WIP: Includes internal links, broken markdown, etc to be resolved</strong></p><p><strong><a href="#background">Background</a></strong></p><blockquote><p><a href="#key-information">Key Information</a> </p><p><a href="#annotations-tldr">Annotations tl;dr</a> </p></blockquote><p><strong><a href="#pre-annotations-narrations">Pre-annotations: Narrations</a></strong></p><p><strong><a href="#annotations">Annotations</a></strong></p><blockquote><p><a href="#episodic-memory">Episodic Memory</a></p><p><a href="#natural-language-queries">Natural Language Queries</a></p><p><a href="#moments">Moments</a></p><p><a href="#visual-object-queries">Visual Object Queries</a></p><p><a href="#forecasting-hands-objects-fho">Forecasting + Hands &amp; Objects (FHO)</a></p><p><a href="#stage-1---critical-frames">Stage 1 - Critical Frames</a> </p><p><a href="#stage-2---pre-condition">Stage 2 - Pre-condition</a> </p><p><a href="#stage-3---post-condition">Stage 3 - Post-condition</a> </p><p><a href="#audio-visual-diarization-social-avs">Audio-Visual Diarization &amp; Social (AVS)</a> </p><p><a href="#av-step-0-automated-face-head-detection">AV Step 0: Automated Face &amp; Head Detection</a> </p><p><a href="#av-step-1-face-head-tracks-correction">AV Step 1: Face &amp; Head Tracks Correction</a> </p><p><a href="#av-step-2-speaker-labeling-and-av-anchor-extraction">AV Step 2: Speaker Labeling and AV anchor extraction</a></p><p><a href="#av-step-3-speech-segmentation-per-speaker">AV Step 3: Speech Segmentation (Per Speaker)</a></p><p><a href="#av-step-4-transcription">AV Step 4: Transcription</a></p><p><a href="#av-step-5-correcting-speech-transcriptions-wip">AV Step 5: Correcting Speech Transcriptions</a></p><p><a href="#social-step-1-camera-wearer-attention">Social Step 1: Camera-Wearer Attention</a></p><p><a href="#social-step-2-speech-target-classification">Social Step 2: Speech Target Classification</a></p></blockquote><h2 class="anchor anchorWithStickyNavbar_y2LR" id="background">Background<a aria-hidden="true" class="hash-link" href="#background" title="Direct link to heading">â€‹</a></h2><p><strong>One-liner:</strong> Building a densely-annotated dataset of <!-- -->~<!-- -->10,000 hours of
ego-centric video for public release.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="key-information">Key Information<a aria-hidden="true" class="hash-link" href="#key-information" title="Direct link to heading">â€‹</a></h2><ul><li><p>~<!-- -->3,400 hours of unscripted, in-the-wild video data across:</p><blockquote><p>9 countries from 13 different partner groups (+ 400 hours from Meta&#x27;s reality labs).</p></blockquote><blockquote><p>926 unique camera-wears recording 120 different scenarios, with hundreds of different actions and objects</p></blockquote><blockquote><p>2.5M dense textual &quot;narrations&quot; (= individual text sentences describing <!-- -->~<!-- -->2,600 hours of video data)\&quot;</p></blockquote></li></ul><p><strong>Devices:</strong></p><p><img src="/docs/assets/images/image7-e0a2bd6a7b876b3194e9a7bbf876256c.png"></p><p><strong>Scenario breakdown:</strong></p><p><img src="/docs/assets/images/image16-9e55f40cbd0b57fc516ff43a2d52324d.png"></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="annotations-tldr">Annotations tl;dr<a aria-hidden="true" class="hash-link" href="#annotations-tldr" title="Direct link to heading">â€‹</a></h2><table><thead><tr><th><strong>Task</strong></th><th><strong>Output</strong></th><th><strong>Volume</strong></th></tr></thead><tbody><tr><td><strong>Pre-annotations</strong></td><td></td><td></td></tr><tr><td><a href="#pre-annotations-narrations">Narrations</a></td><td>Dense written sentence narrations in English &amp; a summary of the whole video clip</td><td>Full Dataset</td></tr><tr><td><strong>Episodic Memory (EM)</strong></td><td></td><td></td></tr><tr><td><a href="#natural-language-queries">Natural Language Queries</a></td><td>N free-form natural anguage queries per video (N=length of video in minutes) selected from a list of query templates + temporal response window from which answers can be deduced</td><td>~<!-- -->240h</td></tr><tr><td><a href="#moments">Moments</a></td><td>Temporal localizations of high level events in a long video clip from a provided taxonomy</td><td>~<!-- -->300h</td></tr><tr><td><a href="#visual-object-queries">Visual Object Queries</a></td><td>For N=3 <strong>query objects</strong> (freely chosen and <strong>named</strong> by the annotator) such that each appears at least twice at separate times in a single video, annotations include: <br> <!-- -->(<!-- -->1<!-- -->)<!-- --> <strong>response track</strong>: bounding boxes over time for one continuous occurrence of the query object; <br> <!-- -->(<!-- -->2<!-- -->)<!-- --> <strong>query frame</strong>: a frame that <em>does</em> <em>not</em> contain the query object, sometime after the response track but before any subsequent occurrence of the object; <br> <!-- -->(<!-- -->3<!-- -->)<!-- --> <strong>visual crop</strong>:  bounding box of a single frame from another occurrence of the same object elsewhere in the video (before or after the originally marked instance)</td><td>~<!-- -->403h</td></tr><tr><td><strong>Forecasting + Hands &amp; Objects (FHO)</strong></td><td></td><td></td></tr><tr><td><a href="#stage-1---critical-frames">1 Critical Frames</a></td><td>Pre-condition (PRE), CONTACT, point of no return (PNR), and post-condition (Post) frames for each narrated action in a video</td><td>~<!-- -->120h</td></tr><tr><td><a href="#stage-2---pre-condition">2 Pre-condition</a></td><td>Bounding boxes and roles for hands (right/left) and objects (objects of change and tools) for each frame from CONTACT to PRE</td><td></td></tr><tr><td><a href="#stage-3---post-condition">3 Post-condition</a></td><td>Bounding boxes and roles for hands and objects for each frame from CONTACT to POST</td><td></td></tr><tr><td><strong>Audio-Visual Diarization &amp; Social (AVS)</strong></td><td></td><td></td></tr><tr><td><a href="#av-step-0-automated-face-head-detection">AV0: Automated Face &amp; Head Detection</a></td><td>Automated overlaid bounding boxes for faces in video clips</td><td>50h</td></tr><tr><td><a href="#av-step-1-face-head-tracks-correction">AV1: Face &amp; Head Tracks Correction</a></td><td>Manually adjusted overlaid bounding boxes for faces in video clips</td><td></td></tr><tr><td><a href="#av-step-2-speaker-labeling-and-av-anchor-extraction">AV2: Speaker Labeling and AV anchor extraction</a></td><td>Anonymous Person IDs for each Face Track in video clip</td><td></td></tr><tr><td><a href="#av-step-3-speech-segmentation-per-speaker">AV3: Speech Segmentation (Per Speaker)</a></td><td>Temporal segments for voice activity for the camera wearer and for each Person ID</td><td></td></tr><tr><td><a href="#av-step-4-transcription">AV4: Transcription</a></td><td>Video clip audio transcriptions</td><td></td></tr><tr><td><a href="#av-step-5-correcting-speech-transcriptions-wip">AV5: Correcting Speech Transcriptions</a></td><td>Corrected Speech Transcription annotations matching voice activity segments and Person IDs from AV2</td><td></td></tr><tr><td><a href="#s-step-1-camera-wearer-attention">S1: Camera-Wearer Attention</a></td><td>Temporal segments in which a person is looking at the camera wearer</td><td></td></tr><tr><td><a href="#s-step-2-speech-target-classification">S2: Speech Target Classification</a></td><td>Temporal segments in which a person is talking to the camera wearer</td><td></td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_y2LR" id="narrations">Narrations<a aria-hidden="true" class="hash-link" href="#narrations" title="Direct link to heading">â€‹</a></h2><p><strong>Objective:</strong> Annotator provides dense written sentence narrations in
English on a first-person video clip of length 10-30 minutes + a summary
of the whole video.</p><p><strong>Motivation:</strong> Understand what data is available and which data to push
through which annotation phases. Provide a starting point for forming a
taxonomy of labels for actions and objects.</p><p><strong>Annotation task:</strong></p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th><th><strong>Example</strong></th></tr></thead><tbody><tr><td>1</td><td><em>Narrate the Complete Video with Temporal Sentences</em></td><td>Watch the video from the beginning until something new occurs.</td><td>set the start time as the point when the person has the knife and the tomato, and the end time as the point when the person has finished chopping, then type: &quot;C is chopping a tomato&quot; into the text input. (&quot;C&quot; refers to the camera wearer).</td></tr></tbody></table><p>|--------|------------------|------------------|------------------|
|        |                  | At that time,    |                  |
|        |                  | pause the video, |                  |
|        |                  | mark the         |                  |
|        |                  | <em>temporal        |                  |
|        |                  | window</em> for      |                  |
|        |                  | which the        |                  |
|        |                  | sentence         |                  |
|        |                  | applies, then    |                  |
|        |                  | &quot;narrate&quot; what   |                  |
|        |                  | you see in the   |                  |
|        |                  | video by typing  |                  |
|        |                  | in a simple      |                  |
|        |                  | sentence into    |                  |
|        |                  | the free-form    |                  |
|        |                  | text input.      |                  |
|--------|------------------|------------------|------------------|
|        |                  | Next, resume     |                  |
|        |                  | watching the     |                  |
|        |                  | video. Once you  |                  |
|        |                  | recognize an     |                  |
|        |                  | action to        |                  |
|        |                  | narrate,         |                  |
|        |                  | immediately      |                  |
|        |                  | pause again and  |                  |
|        |                  | repeat.          |                  |
|--------|------------------|------------------|------------------|
| 2      | <em>Provide a       | As needed, watch | <!-- -->#<!-- -->summary C      |
|        | Summary of the   | the entire video | fixed their      |
|        | Entire Video</em>    | on fast forward  | breakfast, ate   |
|        |                  | to recall the    | it, then got     |
|        |                  | content of the   | dressed and left |
|        |                  | entire video.    | the house.&quot;      |
|        |                  |                  |                  |
|        |                  | Provide a short  |                  |
|        |                  | summary in text  |                  |
|        |                  | about the        |                  |
|        |                  | contents of the  |                  |
|        |                  | entire video     |                  |
|        |                  | (1-3 sentences). |                  |
|        |                  |                  |                  |
|        |                  | This summary     |                  |
|        |                  | should convey    |                  |
|        |                  | the main         |                  |
|        |                  | setting(s) of    |                  |
|        |                  | the video clip   |                  |
|        |                  | (e.g., an        |                  |
|        |                  | apartment, a     |                  |
|        |                  | restaurant, a    |                  |
|        |                  | shop, etc.) as   |                  |
|        |                  | well as an       |                  |
|        |                  | overview of what |                  |
|        |                  | happened.        |                  |
|--------|------------------|------------------|------------------|</p><p><strong>UI Examples</strong></p><ul><li><strong>Narration:</strong></li></ul><p><img src="/docs/assets/images/image42-9e5ac4e4b20ec34bb336528981701d9a.png"></p><ul><li><strong>Summary:</strong></li></ul><blockquote><p><img src="/docs/assets/images/image44-35c45438632a232cfa14668d3b427e51.png"></p></blockquote><p><strong>Annotated videos examples:</strong></p><p>[<a href="https://drive.google.com/file/d/14NrVdpYT2RyJU_rKG99AkIToIwNO6JEY/view?usp=sharing" target="_blank" rel="noopener noreferrer">Example
reel</a></p><p><img src="/docs/assets/images/image35-91b5eb2d1a6d9ba45ec1f3bba497a145.png"><img src="/docs/assets/images/image32-8b0a8c1616304b84176927470f098846.png"></p><p><strong>Annotation Stats</strong></p><ul><li><p><strong>Total hours narrated:</strong> 3400</p></li><li><p><strong>Unique scenarios:</strong> 51</p></li></ul><p><img src="/docs/assets/images/image13-1dbe07a190ef030848b79fe86ab52b9f.png"></p><p><strong>Links:</strong></p><ul><li><p>Narration</p><blockquote><p><a href="https://docs.google.com/document/d/1kHHgJFQM2wbm2M81GjyicoZnGfgix7vnIHhEfkcB0_8/edit#heading=h.zb77nl2kkug0" target="_blank" rel="noopener noreferrer">notes</a>
and
<a href="https://l.workplace.com/l.php?u=https%3A%2F%2Fdocs.google.com%2Fdocument%2Fd%2F1avjdALDI3x1jrnY3DiCv72GGZJzXSuRRMjbmcQURKS0%2Fedit%3Fusp%3Dsharing&amp;h=AT0SmFl7unFKCRMTaAd_2TRlp8Wc7pA0eZEBsRyDqTA5z_vaAxftnRJsGAtJa1PBX60OS0M98dEFj7bBuOuy797sFTls8HXCPzIrjfegkk1gxJOO3elVYcWiVdl2NOF3W0zO1TepzJVPHXx3HL_K&amp;__tn__=-UK-y-R&amp;c%5B0%5D=AT2NBxuTggKaQcYTPVeWQRgFrhv33HhiCNjdu1zHc2EqWu_nDJd2thvGRuiSyviJKTcpIlGgCiweuV-3X_1fkLBC10oNlFIBGlLAj6sX1A_twCFkuq2dCpP__mpZm_HrKDiRn-BIDyrsuNnymY9Kq2LGA3382FSPPwXvZdK2TX8ksWeu8KPPsuxOdCoRRQwJMmy6" target="_blank" rel="noopener noreferrer">instructions</a></p></blockquote></li><li><p><a href="https://docs.google.com/presentation/d/1es_hniyef5bGhtMyeSZkBfbdeXKwElPBjT0YXxr7BvA/edit?usp=sharing" target="_blank" rel="noopener noreferrer">Narration analysis</a></p></li><li><p><a href="https://l.workplace.com/l.php?u=https%3A%2F%2Fdocs.google.com%2Fspreadsheets%2Fd%2F1w1dW1-IyqvufD-X_3SFSBBdLAhOy7JZbNSbhZjDel6I%2Fedit%23gid%3D0&amp;h=AT0JEXfOJJdon1Fc0QP5lnJKL4hMqBvLTnHJPXJleRbSCBAuFMzwiFGE07hyz_YKspJB68Do3y5pyan4963Npz9a4oIBpDhbT1515yy1pJ1uAV0gxJhvCp5EtO-ENJI1yCVueixUaClGp8SIizl6&amp;__tn__=-UK-y-R&amp;c%5B0%5D=AT2NBxuTggKaQcYTPVeWQRgFrhv33HhiCNjdu1zHc2EqWu_nDJd2thvGRuiSyviJKTcpIlGgCiweuV-3X_1fkLBC10oNlFIBGlLAj6sX1A_twCFkuq2dCpP__mpZm_HrKDiRn-BIDyrsuNnymY9Kq2LGA3382FSPPwXvZdK2TX8ksWeu8KPPsuxOdCoRRQwJMmy6" target="_blank" rel="noopener noreferrer">Scenario breakdown</a></p></li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="benchmark-annotations">Benchmark Annotations<a aria-hidden="true" class="hash-link" href="#benchmark-annotations" title="Direct link to heading">â€‹</a></h2><p><img src="/docs/assets/images/image18-fec68ada7e115e2b5ddad1b313a3d283.png"></p><table><thead><tr><th><strong>Target</strong></th><th><strong>#</strong></th><th><strong>Benchmark task</strong></th><th><strong>Research Goal</strong></th></tr></thead><tbody><tr><td><strong>Places</strong></td><td>1</td><td><a href="#episodic-memory">Episodic Memory</a></td><td>Allow an Assistant user to ask free-form, natural language questions, with the answer brought back after analyzing past video (<em>When was the last time I changed the batteries in the smoke detector?</em>).</td></tr><tr><td><strong>Objects</strong></td><td>2</td><td><a href="#forecasting--hands--objects-fho">Forecasting</a></td><td>To intelligently deliver notifications to a user, an AR system must understand how an action or piece of information may impact the future state of the world.</td></tr><tr><td></td><td>3</td><td><a href="#forecasting--hands--objects-fho">Hands-Object interaction</a></td><td>AR applications, e.g. providing users instructions in their egocentric real-world view to accomplish tasks (e.g., cooking a recipe).</td></tr><tr><td><strong>People</strong></td><td>4</td><td><a href="#audio-visual-diarization--social-avs">Audio-visual Diarization</a></td><td>To effectively aid people in daily life scenarios, augmented reality must be able to detect and track sounds, responding to users queries or information needs.</td></tr><tr><td></td><td>5</td><td><a href="#audio-visual-diarization--social-avs">Social interactions</a></td><td>Recognize people&#x27;s interactions, their roles, and their attention within collaborative and competitive scenarios within a range of social interactions captured in the Ego4D data.</td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_y2LR" id="episodic-memory">Episodic Memory<a aria-hidden="true" class="hash-link" href="#episodic-memory" title="Direct link to heading">â€‹</a></h2><p><strong>Motivation</strong>: Augment human memory through personal semantic video
index for an always-on wearable camera</p><p><strong>Objective</strong>: Given long first-person video, <em>localize</em> answers for
queries about objects and events from first-person experience</p><p><em>Who did I sit by at the party? Where are my keys? When did I change the batteries? How often did I read to my child last week? Did I leave the window open?<!-- -->.<!-- -->..</em></p><p><strong>Query types (annotation sub-tasks)</strong>:</p><p>a.  Natural language queries (response = temporal)</p><p>b.  Moments queries (response = temporal)</p><p>c.  Visual/object queries (response = temporal+spatial)</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="natural-language-queries">Natural Language Queries<a aria-hidden="true" class="hash-link" href="#natural-language-queries" title="Direct link to heading">â€‹</a></h3><h3><img src="/docs/assets/images/image4-8e3ced36f3852c4e3022c81523a33885.png"></h3><p><strong>Objective:</strong> Create and annotate <strong>N (N=length of video in minutes)</strong>
interesting questions and their corresponding answers for the given
video.</p><p><strong>Annotation Task</strong> (see <a href="https://docs.google.com/document/d/1RGbm4BjKt8bZ6a95gYno8DJemuCZBhM2d6WWt_W3vLY/edit?usp=sharing" target="_blank" rel="noopener noreferrer">Annotation instructions</a>)<strong>:</strong></p><p>+--------+------------------+------------------+------------------+
| <strong>#</strong> | <strong>Step</strong>         | <strong>Sub-step</strong>     | <strong>Example</strong>      |
+========+==================+==================+==================+
| 0      | <em>Annotator       |                  |                  |
|        | watches video</em>   |                  |                  |
+--------+------------------+------------------+------------------+
| 1      | <em>Asks free-form  | -   Select an    | -                |
|        | natural language |     interesting  | <!-- -->[Template]<!-- -->: |
|        | query at end of  |     query        |     &quot;What <strong>X</strong>  |
|        | video, selecting |     template &amp;   |     is <strong>Y</strong>?&quot;   |
|        | from list of     |     template     |                  |
|        | query            |     category     | -   [Template    |
|        | templates.</em>      |                  |                  |
|        |                  | -   Paraphrase   |  <!-- -->[Category]<!-- -->: |
|        |                  |     question in  |     &quot;Objects&quot;    |
|        |                  |     the past     |                  |
|        |                  |     tense.       | -   <!-- -->[Paraphrased |
|        |                  |                  |     query:]<!-- --> |
|        |                  |                  |     &quot;What        |
|        |                  |                  |     <strong>color</strong>    |
|        |                  |                  |     shirt did    |
|        |                  |                  |     <strong>the person |
|        |                  |                  |     performing   |
|        |                  |                  |     on the       |
|        |                  |                  |     road</strong>       |
|        |                  |                  |     wear?&quot;       |
+--------+------------------+------------------+------------------+
|        |                  | Using            | First free-form  |
|        |                  | <strong>&quot;free-form&quot;</strong>  | query slot:      |
|        |                  | text, fill the   | &quot;<strong>color</strong>&quot;      |
|        |                  | <strong>query slots</strong>  |                  |
|        |                  | (X, Y, <!-- -->.<!-- -->..) in  | Second free-form |
|        |                  | the template to  | query slot:      |
|        |                  | form a           | &quot;<strong>the shirt of  |
|        |                  | meaningful       | the person       |
|        |                  | question         | performing on    |
|        |                  | equivalent to    | the road</strong>&quot;      |
|        |                  | the paraphrase.  |                  |
+--------+------------------+------------------+------------------+
|        |                  | Pick the closest | -   <!-- -->[Paraphrased |
|        |                  | verb for each of |     query]<!-- -->: |
|        |                  | the slots in the |     What         |
|        |                  | respective       |                  |
|        |                  | drop-down menus  |   <strong>instrument</strong> |
|        |                  |                  |     was <strong>the    |
|        |                  |                  |     musician     |
|        |                  |                  |     playing</strong>?   |
|        |                  |                  |                  |
|        |                  |                  | -   <!-- -->[First verb  |
|        |                  |                  |     drop-down selection]<!-- -->: |
|        |                  |                  |     &quot;<!-- -->[<!-- -->VERB NOT APPLICABLE<!-- -->]<!-- -->&quot; |
|        |                  |                  |                  |
|        |                  |                  | -   Second verb  |
|        |                  |                  |     drop-down    |
|        |                  |                  |     selection:   |
|        |                  |                  |     &quot;<strong>play</strong>&quot;   |
+--------+------------------+------------------+------------------+
| 2      | <em>Identifies the  | Seek in the      |                  |
|        | temporal         | video to the     |                  |
|        | response window  | temporal window  |                  |
|        | from which       | where the        |                  |
|        | answer can be    | response to the  |                  |
|        | deduced</em>         | natural language |                  |
|        |                  | query can be     |                  |
|        |                  | deduced visually |                  |
|        |                  |                  |                  |
|        |                  | Specify query to |                  |
|        |                  | have only one    |                  |
|        |                  | valid,           |                  |
|        |                  | contiguous       |                  |
|        |                  | temporal window  |                  |
|        |                  | response         |                  |
+--------+------------------+------------------+------------------+
| 3      | <em>Repeat this     |                  |                  |
|        | process N=length |                  |                  |
|        | of video in      |                  |                  |
|        | minutes creating |                  |                  |
|        | N diverse        |                  |                  |
|        | language         |                  |                  |
|        | queries</em>         |                  |                  |
+--------+------------------+------------------+------------------+</p><p><strong>UI Examples:</strong></p><p><img src="/docs/assets/images/image43-7929512eb1545fa9915c9f6261447238.png"></p><p><strong>Annotated UI Example:</strong></p><table><thead><tr><th><strong>#</strong></th><th><strong>Name</strong></th><th><strong>Example</strong></th></tr></thead><tbody><tr><td>1</td><td>Query Set Label</td><td><em>Query Set 1</em></td></tr><tr><td>2</td><td>Template Query Category</td><td><em>Object</em></td></tr><tr><td>3</td><td>Template</td><td><em>Where is object X before/after event Y?</em></td></tr><tr><td>4</td><td>Paraphrased template in natural language</td><td><em>Where were the blue pliers before I picked them up?</em></td></tr><tr><td>5</td><td>First slot (X)</td><td><em>Blue pliers</em></td></tr><tr><td>6</td><td>Dominant Verb Taxonomy for the first slot (X)</td><td>Verb: <em>[<!-- -->VERB NOT APPLICABLE<!-- -->]</em></td></tr><tr><td>7</td><td>Second slot (Y)</td><td><em>I picked them up</em></td></tr><tr><td>8</td><td>Dominant Verb Taxonomy for the second slot (Y)</td><td><em>pick</em></td></tr></tbody></table><p><img src="/docs/assets/images/image31-219567f3e6b0e31e8b839676ab8fafbb.png"></p><p><strong>Annotated videos examples:</strong></p><p><a href="https://drive.google.com/file/d/14NrVdpYT2RyJU_rKG99AkIToIwNO6JEY/view?usp=sharing" target="_blank" rel="noopener noreferrer">Example (cooking, bike mechanic)</a></p><p><img src="/docs/assets/images/image14-168aa13b5d867f63500a67faa6de9f14.png"></p><p><strong>Annotation Stats (TODO: Jul 21):</strong></p><ul><li><p><strong>Total hours annotated:</strong> <!-- -->~<!-- -->240 (x2; one for each vendor)</p></li><li><p><strong>Distribution over question types:</strong></p></li></ul><p><img src="/docs/assets/images/image2-73d744aacb706a6c6be825d0f333a008.png"></p><ul><li><strong>Scenario breakdown:</strong></li></ul><p><img src="/docs/assets/images/image3-db38883c4c8f547284a969c5d3197adf.png"></p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="moments">Moments<a aria-hidden="true" class="hash-link" href="#moments" title="Direct link to heading">â€‹</a></h3><p><img src="/docs/assets/images/image10-a15f59ac4828168c130fc993a5dba16d.png"></p><p><strong>Objective:</strong> Localize high level events in a long video clip <!-- -->-<!-- -->-
marking any instance of provided activity categories with a temporal
window and the activity&#x27;s name.</p><p><strong>Motivation:</strong> Learn to detect activities or &quot;moments&quot; and their
temporal extent in the video. In the context of episodic memory, the
implicit query from a user would be &quot;When is the last time I did X?&quot;,
and the response from the system would be to show the time window where
activity X was last seen.</p><p><strong>Annotation Task</strong> (see <a href="https://docs.google.com/document/d/1lGtcGjxYOOQsf9SalVehEocqjsh26j0LbmhGucojkOw/edit" target="_blank" rel="noopener noreferrer">Annotation instructions</a>)<strong>:</strong></p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th><th><strong>Example</strong></th></tr></thead><tbody><tr><td>1</td><td>*Review the</td><td></td><td>![]</td></tr><tr><td></td><td>Taxonomy*</td><td></td><td>(media/image15.p</td></tr><tr><td></td><td></td><td></td><td>ng)</td></tr></tbody></table><p>+--------+------------------+------------------+------------------+
| 2      | <em>Annotate the    | 1.  Play the     | </em>See [<!-- -->[UI        |
|        | Video*           |     video until  | Examples]<!-- -->  |
|        |                  |     you observe  | (#llbjhw5qjwgg)* |
|        |                  |     an activity, |                  |
|        |                  |     then pause.  |                  |
|        |                  |                  |                  |
|        |                  | 2.  Draw a       |                  |
|        |                  |     temporal     |                  |
|        |                  |     window       |                  |
|        |                  |     around the   |                  |
|        |                  |     time span    |                  |
|        |                  |     where the    |                  |
|        |                  |     activity     |                  |
|        |                  |     occurs       |                  |
|        |                  |                  |                  |
|        |                  | 3.  Select from  |                  |
|        |                  |     the dropdown |                  |
|        |                  |     list the     |                  |
|        |                  |     name for     |                  |
|        |                  |     that         |                  |
|        |                  |     activity     |                  |
|        |                  |                  |                  |
|        |                  | 4.  Play the     |                  |
|        |                  |     video from   |                  |
|        |                  |     the start of |                  |
|        |                  |     the previous |                  |
|        |                  |     activity,    |                  |
|        |                  |     repeat steps |                  |
|        |                  |     1-3          |                  |
+--------+------------------+------------------+------------------+</p><p><strong>UI Examples:</strong></p><p><img src="/docs/assets/images/image45-4154b0e8b1843819b876742ac2343582.png"></p><p><strong>Annotation Stats (Jul 21):</strong></p><ul><li><strong>Total hours annotated:</strong> <!-- -->~<!-- -->300 (x3 raters)</li></ul><h3 class="anchor anchorWithStickyNavbar_y2LR" id="visual-object-queries">Visual Object Queries<a aria-hidden="true" class="hash-link" href="#visual-object-queries" title="Direct link to heading">â€‹</a></h3><p><img src="/docs/assets/images/image33-930945bc750e68e528ae8219b42fce82.png"></p><p><strong>Objective:</strong> Localize past instances of a given object that appears at
least twice in different parts of the video.</p><p><strong>Motivation:</strong> Support an object search application for video in which
a user asks at time T &quot;where did I last see X?&quot;, and the system scans
back in the video history starting at query frame T, finds the most
recent instance of X, and outlines it in a short track<strong>.</strong></p><p><strong>Annotation Task:</strong> (see <a href="https://docs.google.com/document/d/1Ks9qVQjTE16tJXsC3fh-64Rlkoc_qbQafxyu1c6uNYw/edit?usp=sharing" target="_blank" rel="noopener noreferrer">Annotation instructions</a>)</p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th><th><strong>Example</strong></th></tr></thead><tbody><tr><td>1</td><td>*Identify</td><td>Preview the</td><td></td></tr><tr><td></td><td>**query objects</td><td>entire video</td><td></td></tr><tr><td></td><td>***</td><td></td><td></td></tr><tr><td></td><td></td><td>Identify a set</td><td></td></tr><tr><td></td><td></td><td>of N=3</td><td></td></tr><tr><td></td><td></td><td>interesting</td><td></td></tr><tr><td></td><td></td><td>objects to label</td><td></td></tr><tr><td></td><td></td><td>as queries (=</td><td></td></tr><tr><td></td><td></td><td>objects that</td><td></td></tr><tr><td></td><td></td><td>appear at least</td><td></td></tr><tr><td></td><td></td><td>twice at</td><td></td></tr><tr><td></td><td></td><td>distinct</td><td></td></tr><tr><td></td><td></td><td>non-contiguous</td><td></td></tr><tr><td></td><td></td><td>parts of the</td><td></td></tr><tr><td></td><td></td><td>video clip)</td><td></td></tr></tbody></table><p>+--------+------------------+------------------+------------------+
| 2      | *Select a        | -   Select one   | ![]              |
|        | <strong>response       |     occurrence   | (media/image30.g |
|        | track*</strong>         |     of the query | if){width=&quot;1.895 |
|        |                  |     object.      | 8333333333333in&quot; |
|        |                  |                  | height=&quot;1.0694   |
|        |                  | -   Mark the     | 444444444444in&quot;} |
|        |                  |     query object |                  |
|        |                  |     with a       |                  |
|        |                  |     bounding box |                  |
|        |                  |     over time,   |                  |
|        |                  |     from the     |                  |
|        |                  |     frame the    |                  |
|        |                  |     object       |                  |
|        |                  |     enters the   |                  |
|        |                  |     field of     |                  |
|        |                  |     view until   |                  |
|        |                  |     it leaves    |                  |
|        |                  |     the field of |                  |
|        |                  |     view, for    |                  |
|        |                  |     that object  |                  |
|        |                  |     occurrence.  |                  |
+--------+------------------+------------------+------------------+
| 3      | *Select a        | -   Select a     | <!-- -->![               |
|        | **query frame*** |     frame that   | ]<!-- -->(media/image9.p |
|        |                  |     does <em>not</em>   | ng){width=&quot;1.833 |
|        |                  |     contain the  | 3333333333333in&quot; |
|        |                  |     query        | height=&quot;1.25in&quot;} |
|        |                  |     object,      |                  |
|        |                  |     sometime far |                  |
|        |                  |     <em>after</em> that |                  |
|        |                  |     object       |                  |
|        |                  |     occurrence,  |                  |
|        |                  |     but <em>before</em> |                  |
|        |                  |     any          |                  |
|        |                  |     subsequent   |                  |
|        |                  |     occurrence   |                  |
|        |                  |     of the       |                  |
|        |                  |     object.      |                  |
|        |                  |                  |                  |
|        |                  | -   Mark the     |                  |
|        |                  |     time point   |                  |
|        |                  |     with a large |                  |
|        |                  |     bounding     |                  |
|        |                  |     box.         |                  |
+--------+------------------+------------------+------------------+
| 4      | *Select a        | -   Find another | ![]              |
|        | <strong>visual crop*</strong> |     occurrence   | (media/image11.p |
|        |                  |     of the same  | ng){width=&quot;1.895 |
|        |                  |     object       | 8333333333333in&quot; |
|        |                  |     elsewhere in | height=&quot;1.25in&quot;} |
|        |                  |     the video    |                  |
|        |                  |     (before or   |                  |
|        |                  |     after the    |                  |
|        |                  |     originally   |                  |
|        |                  |     marked       |                  |
|        |                  |     instance     |                  |
|        |                  |     from Step 2) |                  |
|        |                  |                  |                  |
|        |                  | -   Draw a       |                  |
|        |                  |     bounding box |                  |
|        |                  |     in           |                  |
|        |                  |     <!-- -->[one]<!-- -->{.ul}   |                  |
|        |                  |     frame around |                  |
|        |                  |     that object. |                  |
+--------+------------------+------------------+------------------+
| 5      | <strong>*Name</strong>        |                  |                  |
|        | <strong>the</strong>          |                  |                  |
|        | <strong>object</strong> using |                  |                  |
|        | the <strong>free       |                  |                  |
|        | text</strong> box<em>      |                  |                  |
+--------+------------------+------------------+------------------+
| 6      | </em>Repeat Steps    |                  |                  |
|        | 1-5 three times  |                  |                  |
|        | for the same     |                  |                  |
|        | video clip and   |                  |                  |
|        | different        |                  |                  |
|        | objects*         |                  |                  |
+--------+------------------+------------------+------------------+</p><p><strong>Annotation Stats (Jul 21):</strong></p><ul><li><strong>Total hours annotated:</strong> <!-- -->~<!-- -->403</li></ul><blockquote><p><img src="/docs/assets/images/image1-cb95f7a6ce8d8cc57a8a376a4870bd4b.png"></p></blockquote><ul><li><strong>Scenario breakdown:</strong></li></ul><p><img src="/docs/assets/images/image21-997834909d39c1d2ea8729613e8cf735.png"></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="forecasting--hands--objects-fho">Forecasting + Hands &amp; Objects (FHO)<a aria-hidden="true" class="hash-link" href="#forecasting--hands--objects-fho" title="Direct link to heading">â€‹</a></h2><p><strong>Objective:</strong> Recognize object state changes temporally and spatially
(HO); predict these interactions spatially and temporally before they
happen (F).</p><p><strong>Motivation:</strong> Understanding and anticipating human-object
interactions.</p><p><strong>Scenario Distribution:</strong></p><p><img src="/docs/assets/images/image19-00e19a2a62a562005bd744ae7965764a.png"></p><p>|----------------------------------|----------------------------------|
| <strong>Annotation</strong>                   | <strong>Scenario Distribution:</strong>       |
| (Aug 21):**                      |                                  |
|                                  |                                  |
| Labeled videos: 1,074            |                                  |
|                                  |                                  |
| Labeled clips: 1,672             |                                  |
|                                  |                                  |
| Labeled hours: 116.274           |                                  |
|                                  |                                  |
| Number of scenarios: 53          |                                  |
|                                  |                                  |
| Number of universities: 7        |                                  |
|                                  |                                  |
| Number of participants: 397      |                                  |
|                                  |                                  |
| Num interactions: 91,002         |                                  |
|                                  |                                  |
| Num rejected: 18,839             |                                  |
|                                  |                                  |
| Num with state change: 70,718    |                                  |
|                                  |                                  |
+----------------------------------+----------------------------------+</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="stage-1---critical-frames">Stage 1 - Critical Frames<a aria-hidden="true" class="hash-link" href="#stage-1---critical-frames" title="Direct link to heading">â€‹</a></h3><p><img src="/docs/assets/images/image5-1c194e80ab2a55776812a699b248b5ee.png"></p><p><strong>Objective:</strong> Annotator watches an egocentric video and marks
pre-condition (PRE), contact, point of no return (PNR), and
post-condition (Post) frames.</p><p><strong>Annotation Task</strong> (See <a href="https://docs.google.com/document/d/13BmI98M_4gzd31vYAtQ8wRSLHggpnrts0gOVDTrWnDM/edit?usp=sharing" target="_blank" rel="noopener noreferrer">Annotation instructions</a>)<strong>:</strong></p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th><th><strong>Example</strong></th></tr></thead><tbody><tr><td>1</td><td>*Read the</td><td>1<!-- -->.<!-- --> Reject</td><td><em>Example:</em> &quot;C</td></tr><tr><td></td><td>narrated action</td><td>videos that do</td><td>glides hand</td></tr><tr><td></td><td>to be labeled*</td><td>not contain</td><td>planer along the</td></tr><tr><td></td><td></td><td>hand-object</td><td>wood&quot;![]</td></tr><tr><td></td><td></td><td>interactions</td><td>(media/image46.p</td></tr><tr><td></td><td></td><td></td><td>ng){width=&quot;1.895</td></tr><tr><td></td><td></td><td>2<!-- -->.<!-- --> Reject</td><td>8333333333333in&quot;</td></tr><tr><td></td><td></td><td>videos that not</td><td>height=&quot;1.0277</td></tr><tr><td></td><td></td><td>contain the</td><td>777777777777in&quot;}</td></tr><tr><td></td><td></td><td>narrated action</td><td></td></tr></tbody></table><p>+--------+------------------+------------------+------------------+
| 2      | <em>Select the verb | -   If an        | ![]              |
|        | corresponding to |     appropriate  | (media/image28.p |
|        | the narration</em>   |     verb is not  | ng){width=&quot;1.895 |
|        |                  |     available,   | 8333333333333in&quot; |
|        |                  |     select OTHER | height=&quot;1.0in&quot;}  |
|        |                  |     from the     |                  |
|        |                  |     dropdown and |                  |
|        |                  |     type in the  |                  |
|        |                  |     verb in the  |                  |
|        |                  |     text box.    |                  |
+--------+------------------+------------------+------------------+
| 3      | <em>Select the      | -   Select one   | ![]              |
|        | <strong>state change   |     &gt; of 8       | (media/image22.p |
|        | type</strong> present   |     &gt; options    | ng){width=&quot;1.895 |
|        | in the video</em>    |     &gt; from the   | 8333333333333in&quot; |
|        |                  |     &gt; dropdown   | height=&quot;1.0277   |
|        |                  |                  | 777777777777in&quot;} |
+--------+------------------+------------------+------------------+
| 4      | <em>Mark the        | -   Find the     | ![]              |
|        | <strong>CONTACT</strong>      |     &gt; CONTACT    | (media/image47.p |
|        | (only if         |     &gt; frame      | ng){width=&quot;1.895 |
|        | present),</em>       |                  | 8333333333333in&quot; |
|        | <strong>PRE</strong> and      | -   Pause the    | height=&quot;1.0138   |
|        | <strong>POST</strong> frames. |     &gt; video      | 888888888888in&quot;} |
|        |                  |                  |                  |
|        |                  | -   Select the   |                  |
|        |                  |     &gt; &quot;Contact   |                  |
|        |                  |     &gt; Frame&quot;     |                  |
|        |                  |     &gt; from the   |                  |
|        |                  |     &gt; dropdown   |                  |
|        |                  |                  |                  |
|        |                  | -   Repeat the   |                  |
|        |                  |     &gt; same       |                  |
|        |                  |     &gt; protocol   |                  |
|        |                  |     &gt; for PRE    |                  |
|        |                  |     &gt; and POST   |                  |
|        |                  |     &gt; frames.    |                  |
+--------+------------------+------------------+------------------+</p><p><strong>PRE, CONTACT, PNR, POST examples:</strong></p><p>a.  Example: &quot;light blowtorch&quot;</p><blockquote><p><img src="/docs/assets/images/image38-80d977484213083a627b3a0d642d91b4.jpg"></p></blockquote><p>b.  Example: &quot;put down wood&quot; (object already in hands, no CONTACT</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#393A34"><span class="token plain">&gt; frame)![](media/image39.jpg)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>c.  <a href="https://drive.google.com/file/d/1Fvg6ddceiVAbOru69XXB3PExuZAPd7ad/view?usp=sharing" target="_blank" rel="noopener noreferrer">VIDEO EXAMPLES</a></p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="stage-2---pre-condition">Stage 2 - Pre-condition<a aria-hidden="true" class="hash-link" href="#stage-2---pre-condition" title="Direct link to heading">â€‹</a></h3><p><strong>Objective:</strong> Label bounding boxes and roles for hands (right/left) and
objects (objects of change and tools).</p><p><strong>Annotation Task</strong> (see <a href="https://docs.google.com/document/d/1bjbjJVFEUnl_GnTFmjfZry49_7c7DdR_RBotyjLoGgM/edit?usp=sharing" target="_blank" rel="noopener noreferrer">Annotation instructions</a>
and <a href="https://drive.google.com/file/d/14gXr6yMb815L79jp0QN_n2X9e_OXpqIa/view" target="_blank" rel="noopener noreferrer">video tutorial</a>)
<strong>Note</strong>: clips annotated from previous stage play in reverse from
CONTACT to PRE frame:</p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th><th><strong>Example</strong></th></tr></thead><tbody><tr><td>1</td><td>*Read the</td><td></td><td><em>Example:</em> &quot;C</td></tr><tr><td></td><td>narrated action</td><td></td><td>straightens the</td></tr><tr><td></td><td>to be labeled*</td><td></td><td>cloth&quot;![]</td></tr><tr><td></td><td></td><td></td><td>(media/image25.p</td></tr><tr><td></td><td></td><td></td><td>ng){width=&quot;1.895</td></tr><tr><td></td><td></td><td></td><td>8333333333333in&quot;</td></tr><tr><td></td><td></td><td></td><td>height=&quot;0.9861</td></tr><tr><td></td><td></td><td></td><td>111111111112in&quot;}</td></tr></tbody></table><p>+--------+------------------+------------------+------------------+
| 2      | <em>Label the       | Label <strong>right    | ![]              |
|        | contact frame    | and left hands</strong> | (media/image29.p |
|        | (first frame     | (if visible), by | ng){width=&quot;1.895 |
|        | shown)</em>          | correcting the   | 8333333333333in&quot; |
|        |                  | existing         | height=&quot;0.9722   |
|        |                  | bounding box or  | 222222222222in&quot;} |
|        |                  | adding a new     |                  |
|        |                  | one.             |                  |
+--------+------------------+------------------+------------------+
|        |                  | â€‹â€‹Label the      | ![]              |
|        |                  | <strong>object(s) of   | (media/image36.p |
|        |                  | change</strong>:        | ng){width=&quot;1.895 |
|        |                  |                  | 8333333333333in&quot; |
|        |                  | -   Draw the     | height=&quot;0.9722   |
|        |                  |     <!-- -->[bounding    | 222222222222in&quot;} |
|        |                  |     box]<!-- -->{.ul}    |                  |
|        |                  |                  |                  |
|        |                  | -   Mark the     |                  |
|        |                  |     object as    |                  |
|        |                  |     Object of    |                  |
|        |                  |     change       |                  |
|        |                  |                  |                  |
|        |                  | -   Select the   |                  |
|        |                  |     <!-- -->[name of the |                  |
|        |                  |     object]<!-- -->{.ul} |                  |
|        |                  |     from list    |                  |
|        |                  |     provided     |                  |
|        |                  |                  |                  |
|        |                  | -   Select       |                  |
|        |                  |     <!-- -->[instance    |                  |
|        |                  |     ID]<!-- -->{.ul}     |                  |
|        |                  |     (for         |                  |
|        |                  |     multiple     |                  |
|        |                  |     objects of   |                  |
|        |                  |     the same     |                  |
|        |                  |     type)        |                  |
|        |                  |                  |                  |
|        |                  | -   Repeat for   |                  |
|        |                  |     each object  |                  |
|        |                  |     of change    |                  |
+--------+------------------+------------------+------------------+
|        |                  | &gt; Label the      | ![]              |
|        |                  | &gt; <strong>tool</strong> (if   | (media/image27.p |
|        |                  | &gt; present):      | ng){width=&quot;1.895 |
|        |                  |                  | 8333333333333in&quot; |
|        |                  | -   Draw the     | height=&quot;0.9722   |
|        |                  |     &gt; <!-- -->[bounding  | 222222222222in&quot;} |
|        |                  |     &gt; box]<!-- -->{.ul}  |                  |
|        |                  |                  |                  |
|        |                  | -   Mark the     |                  |
|        |                  |     &gt; object as  |                  |
|        |                  |     &gt; Tool       |                  |
|        |                  |                  |                  |
|        |                  | -   Select the   |                  |
|        |                  |     &gt; <!-- -->[name of   |                  |
|        |                  |     &gt; the        |                  |
|        |                  |     &gt; tool]<!-- -->{.ul} |                  |
|        |                  |     &gt; from list  |                  |
|        |                  |     &gt; provided   |                  |
|        |                  |                  |                  |
|        |                  | -   Select       |                  |
|        |                  |     &gt; <!-- -->[instance  |                  |
|        |                  |     &gt; ID]<!-- -->{.ul}   |                  |
|        |                  |     &gt; (for       |                  |
|        |                  |     &gt; multiple   |                  |
|        |                  |     &gt; objects of |                  |
|        |                  |     &gt; the same   |                  |
|        |                  |     &gt; type)      |                  |
+--------+------------------+------------------+------------------+
| 3      | <em>Label the       | -   Go to the    | ![]              |
|        | remaining        |     &gt; next frame | (media/image41.p |
|        | frames</em>          |                  | ng){width=&quot;1.895 |
|        |                  | -   Adjust the   | 8333333333333in&quot; |
|        |                  |     &gt; hand boxes | height=&quot;0.9861   |
|        |                  |                  | 111111111112in&quot;} |
|        |                  | -   Adjust the   |                  |
|        |                  |     &gt; object of  |                  |
|        |                  |     &gt; change box |                  |
|        |                  |                  |                  |
|        |                  | -   Adjust the   |                  |
|        |                  |     &gt; tool box   |                  |
|        |                  |     &gt; (if        |                  |
|        |                  |     &gt; present)   |                  |
|        |                  |                  |                  |
|        |                  | -   Repeat for   |                  |
|        |                  |     &gt; the        |                  |
|        |                  |     &gt; remaining  |                  |
|        |                  |     &gt; frames     |                  |
+--------+------------------+------------------+------------------+</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="stage-3---post-condition">Stage 3 - Post-condition<a aria-hidden="true" class="hash-link" href="#stage-3---post-condition" title="Direct link to heading">â€‹</a></h3><p><strong>Objective:</strong> Label bounding boxes and roles for hands and objects
(from Contact to Post frame).</p><p><strong>Annotation Task</strong> (see <a href="https://docs.google.com/document/d/18kSRpBNhYirvlFDF6MplpkiRLstGh5BMko4DKHwq_9o/edit?usp=sharing" target="_blank" rel="noopener noreferrer">Annotation instructions</a>)
<!-- -->[Note]<!-- -->{.ul}: clips annotated from Stage 1 play from CONTACT to POST
frame:</p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th><th><strong>Example</strong></th></tr></thead><tbody><tr><td>1</td><td>*Read the</td><td></td><td><em>Example:</em> &quot;C</td></tr><tr><td></td><td>narrated action</td><td></td><td>straightens the</td></tr><tr><td></td><td>to be labeled*</td><td></td><td>cloth&quot;![]</td></tr><tr><td></td><td></td><td></td><td>(media/image25.p</td></tr><tr><td></td><td></td><td></td><td>ng){width=&quot;1.895</td></tr><tr><td></td><td></td><td></td><td>8333333333333in&quot;</td></tr><tr><td></td><td></td><td></td><td>height=&quot;0.9861</td></tr><tr><td></td><td></td><td></td><td>111111111112in&quot;}</td></tr></tbody></table><p>+--------+------------------+------------------+------------------+
| 2      | <em>Check the       | Contact frame    |                  |
|        | contact frame    | will already be  |                  |
|        | (first frame     | labeled with:    |                  |
|        | shown)</em>          |                  |                  |
|        |                  | -   Left hand    |                  |
|        |                  |     &gt; (if        |                  |
|        |                  |     &gt; visible)   |                  |
|        |                  |                  |                  |
|        |                  | -   Right hand   |                  |
|        |                  |     &gt; (if        |                  |
|        |                  |     &gt; visible)   |                  |
|        |                  |                  |                  |
|        |                  | -   Active       |                  |
|        |                  |     &gt; object     |                  |
|        |                  |                  |                  |
|        |                  | -   Tool (if     |                  |
|        |                  |                  |                  |
|        |                  |    &gt; applicable) |                  |
+--------+------------------+------------------+------------------+
|        |                  |                  |                  |
+--------+------------------+------------------+------------------+
| 3      | <em>Label the       | -   Go to the    | ![]              |
|        | remaining        |     &gt; next frame | (media/image37.p |
|        | frames</em>          |                  | ng){width=&quot;1.895 |
|        |                  | -   Adjust (or   | 8333333333333in&quot; |
|        |                  |     &gt; add) the   | height=&quot;0.9722   |
|        |                  |     &gt; hand boxes | 222222222222in&quot;} |
|        |                  |                  |                  |
|        |                  | -   Adjust the   |                  |
|        |                  |     &gt; object of  |                  |
|        |                  |     &gt; change box |                  |
|        |                  |                  |                  |
|        |                  | -   Adjust the   |                  |
|        |                  |     &gt; tool box   |                  |
|        |                  |     &gt; (if        |                  |
|        |                  |     &gt; present)   |                  |
|        |                  |                  |                  |
|        |                  | -   Repeat for   |                  |
|        |                  |     &gt; the        |                  |
|        |                  |     &gt; remaining  |                  |
|        |                  |     &gt; frames     |                  |
+--------+------------------+------------------+------------------+</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="audio-visual-diarization--social-avs">Audio-Visual Diarization &amp; Social (AVS)<a aria-hidden="true" class="hash-link" href="#audio-visual-diarization--social-avs" title="Direct link to heading">â€‹</a></h2><p><strong>Objective:</strong></p><ul><li><p><strong>AV</strong>: Locate each speaker spatially and temporally, segment and</p><blockquote><p>transcribe the speech content (in a given video), assign each
speaker an anonymous label. [<a href="https://docs.google.com/document/d/188OjXu_UvwB2vLX5SestusNuhdao7QcZ11BbuKd5-8U/edit?usp=sharing" target="_blank" rel="noopener noreferrer">Audio Visual Detection &amp; Tracking
Annotations Summary
<!-- -->[<!-- -->Updated<!-- -->]</a></p></blockquote></li><li><p><strong>S:</strong> predict the following social cues:</p><ul><li><p>Who is talking to the camera wearer at each time segment</p></li><li><p>Who is looking at the camera wearer at each time segment</p></li></ul></li></ul><p><strong>Motivation:</strong> Understand conversational behavior from the naturalistic
egocentric perspective; capture low level detection, segmentation and
tracking attributes of people\&#x27;s interactions in a scene, and more high
level (intent/emotions driven) attributes that drive social and group
conversations in the real world.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="av-step-0-automated-face--head-detection">AV Step 0: Automated Face &amp; Head Detection<a aria-hidden="true" class="hash-link" href="#av-step-0-automated-face--head-detection" title="Direct link to heading">â€‹</a></h3><p>A face detection algorithm is run on the given input video to detect all
the faces. The resulting bounding boxes are going to be populated and
overlaid on the input video.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="av-step-1-face--head-tracks-correction">AV Step 1: Face &amp; Head Tracks Correction<a aria-hidden="true" class="hash-link" href="#av-step-1-face--head-tracks-correction" title="Direct link to heading">â€‹</a></h3><p><strong>Objective:</strong> Have a correct face bounding box around all the faces
visible in the video</p><p><strong>Annotation Task</strong> (see <a href="https://docs.google.com/document/d/1mgPTHJWJt1HWmOiM-UQOp-rc8S7zvnkw/edit?usp=sharing&amp;ouid=109871152660798629950&amp;rtpof=true&amp;sd=true" target="_blank" rel="noopener noreferrer">Annotation instructions</a>):</p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th></tr></thead><tbody><tr><td>1</td><td>*For each frame in</td><td>1.  *Subject <strong>has</strong></td></tr><tr><td></td><td>the video, identify</td><td>&gt; a bounding box</td></tr><tr><td></td><td>all subjects in the</td><td>&gt; (bbox):*</td></tr><tr><td></td><td>frame and check to</td><td></td></tr><tr><td></td><td>see if they have</td><td>a.  *Bbox is</td></tr><tr><td></td><td>bounding boxes.*</td><td></td></tr><tr><td></td><td></td><td>&gt; <strong>PASSING</strong></td></tr><tr><td></td><td></td><td>&gt; â†’ Move</td></tr><tr><td></td><td></td><td>&gt; onto the</td></tr><tr><td></td><td></td><td>&gt; next</td></tr><tr><td></td><td></td><td>&gt; subject in</td></tr><tr><td></td><td></td><td>&gt; the frame*</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td>b.  *Bbox is</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td>&gt; <strong>FAILING</strong></td></tr><tr><td></td><td></td><td>&gt; â†’</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td>&gt; Adjust/Re-draw</td></tr><tr><td></td><td></td><td>&gt; the bbox</td></tr><tr><td></td><td></td><td>&gt; (making</td></tr><tr><td></td><td></td><td>&gt; sure the</td></tr><tr><td></td><td></td><td>&gt; right face</td></tr><tr><td></td><td></td><td>&gt; track is</td></tr><tr><td></td><td></td><td>&gt; selected)*</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td>2.  *Subject</td></tr><tr><td></td><td></td><td>&gt; **doesn&#x27;t</td></tr><tr><td></td><td></td><td>&gt; have** a bbox</td></tr><tr><td></td><td></td><td>&gt; â†’ Create a new</td></tr><tr><td></td><td></td><td>&gt; bounding box</td></tr><tr><td></td><td></td><td>&gt; and either</td></tr><tr><td></td><td></td><td>&gt; assign it a</td></tr><tr><td></td><td></td><td>&gt; new track o</td></tr><tr><td></td><td></td><td>&gt; merge an</td></tr><tr><td></td><td></td><td>&gt; existing face</td></tr><tr><td></td><td></td><td>&gt; track.*</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td>3.  *Bbox does not</td></tr><tr><td></td><td></td><td>&gt; capture a face</td></tr><tr><td></td><td></td><td>&gt; â†’ Delete</td></tr><tr><td></td><td></td><td>&gt; bbox.*</td></tr></tbody></table><p>+----------------------+----------------------+----------------------+
| <strong>Examples:</strong>        |                      |                      |
+----------------------+----------------------+----------------------+
| <strong>Passing</strong> Bbox     | <!-- -->![image.png]<!-- -->(media   |                      |
|                      | /image6.png){width=&quot; |                      |
|                      | 4.104166666666667in&quot; |                      |
|                      | height=&quot;2.           |                      |
|                      | 2916666666666665in&quot;} |                      |
+----------------------+----------------------+----------------------+
| <strong>Failing</strong> bbox     | <!-- -->![image.png]<!-- -->(media/  |                      |
|                      | image12.png){width=&quot; |                      |
|                      | 4.104166666666667in&quot; |                      |
|                      | height=&quot;2.           |                      |
|                      | 2916666666666665in&quot;} |                      |
+----------------------+----------------------+----------------------+
| <strong>Missing</strong> bbox     | <!-- -->![image.png]<!-- -->(media/  |                      |
|                      | image23.png){width=&quot; |                      |
|                      | 4.104166666666667in&quot; |                      |
|                      | height=&quot;2.           |                      |
|                      | 2916666666666665in&quot;} |                      |
+----------------------+----------------------+----------------------+
| Bbox to be           | <!-- -->![image.png]<!-- -->(media/  |                      |
| <strong>deleted</strong>          | image20.png){width=&quot; |                      |
|                      | 4.104166666666667in&quot; |                      |
|                      | height=&quot;2.           |                      |
|                      | 3055555555555554in&quot;} |                      |
+----------------------+----------------------+----------------------+</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="av-step-2-speaker-labeling-and-av-anchor-extraction">AV Step 2: Speaker Labeling and AV anchor extraction<a aria-hidden="true" class="hash-link" href="#av-step-2-speaker-labeling-and-av-anchor-extraction" title="Direct link to heading">â€‹</a></h3><p><strong>Objective:</strong> Assign each Face Track<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup> (from Step 1) a &#x27;Person ID&#x27;
(for each new subject which has an interaction with the camera-wearer or
is present in the camera for 500+ frames).</p><p><strong>Annotation Task</strong> (see <a href="https://fb-my.sharepoint.com/:w:/p/sallyyoo/EXSVyiXDcypOjOdvgE24Rq0BmSU2iEDVHDneItZblllefQ?e=1nU14r" target="_blank" rel="noopener noreferrer">Annotation instructions</a>):</p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th></tr></thead><tbody><tr><td>1</td><td>*Identify the &#x27;Next</td><td>1.  *Toggle On the</td></tr><tr><td></td><td>Track&#x27; and go to the</td><td>&#x27;Out-of-Frame&#x27;</td></tr><tr><td></td><td>first frame of this</td><td>Track List*</td></tr><tr><td></td><td>track.*</td><td></td></tr><tr><td></td><td></td><td>2.  *Select the next</td></tr><tr><td></td><td></td><td>Track from the</td></tr><tr><td></td><td></td><td>list*</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td>3.  *Click &#x27;First</td></tr><tr><td></td><td></td><td>Key Frame&#x27;*</td></tr></tbody></table><p>+----------------------+----------------------+----------------------+
| 2                    | <em>Assign this Track a | 1.  </em>Use the drop    |
|                      | unique &#x27;Person ID&#x27;   |     down menu to     |
|                      | (e.g. Person 1,      |     select a Person  |
|                      | Person 2, ect)<em>      |     ID</em>              |
|                      |                      |                      |
|                      |                      | 2.  <em>Each time this  |
|                      |                      |     person appears   |
|                      |                      |     in the video,    |
|                      |                      |     assign their     |
|                      |                      |     Track <!-- -->#<!-- --> to      |
|                      |                      |     their designated |
|                      |                      |     Person ID</em>       |
+----------------------+----------------------+----------------------+
| 3                    | <em>Repeat steps 1-4    |                      |
|                      | until all tracks     |                      |
|                      | have Person ID&#x27;s     |                      |
|                      | assigned.</em>           |                      |
+----------------------+----------------------+----------------------+
| <strong>Examples:</strong>        |                      |                      |
+----------------------+----------------------+----------------------+
| [<!-- -->[AV - Step 2 -      |                      |                      |
| Person ID Example    |                      |                      |
| Annot                |                      |                      |
| ation.mp4]<!-- -->(htt |                      |                      |
| ps://drive.google.co |                      |                      |
| m/file/d/1R1R1BXXMTx |                      |                      |
| FsZKl98tM73gIWRrMCZj |                      |                      |
| O2/view?usp=sharing) |                      |                      |
|                      |                      |                      |
| ![](media/           |                      |                      |
| image26.png){width=&quot; |                      |                      |
| 6.203125546806649in&quot; |                      |                      |
| height=&quot;2            |                      |                      |
| .077746062992126in&quot;} |                      |                      |
+----------------------+----------------------+----------------------+</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="av-step-3-speech-segmentation-per-speaker">AV Step 3: Speech Segmentation (Per Speaker)<a aria-hidden="true" class="hash-link" href="#av-step-3-speech-segmentation-per-speaker" title="Direct link to heading">â€‹</a></h3><p><strong>Objective:</strong> Label voice activity for all subjects in the video.</p><p><strong>Annotation Task</strong> (see <a href="https://fb-my.sharepoint.com/:w:/p/sallyyoo/EXSVyiXDcypOjOdvgE24Rq0BmSU2iEDVHDneItZblllefQ?e=1nU14r" target="_blank" rel="noopener noreferrer">Annotation instructions</a>):</p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th></tr></thead><tbody><tr><td>1</td><td>*Label voice</td><td>1.  Annotate the</td></tr><tr><td></td><td>activity for the</td><td>video using the</td></tr><tr><td></td><td><strong>camera wearer</strong></td><td>time segment</td></tr><tr><td></td><td>first and then for</td><td>toolâ€¯</td></tr><tr><td></td><td>each Person ID.*</td><td></td></tr><tr><td></td><td></td><td>2.  Start an</td></tr><tr><td></td><td></td><td>annotation when</td></tr><tr><td></td><td></td><td>a person makes a</td></tr><tr><td></td><td></td><td>sound (speech,</td></tr><tr><td></td><td></td><td>coughing, sigh,</td></tr><tr><td></td><td></td><td>**any</td></tr><tr><td></td><td></td><td>utterance**).</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td>3.  Stop an</td></tr><tr><td></td><td></td><td>annotation when</td></tr><tr><td></td><td></td><td>a person stops</td></tr><tr><td></td><td></td><td>making sounds.</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td>4.  Do not stop an</td></tr><tr><td></td><td></td><td>annotation if a</td></tr><tr><td></td><td></td><td>person starts</td></tr><tr><td></td><td></td><td>making sound</td></tr><tr><td></td><td></td><td>again within 1</td></tr><tr><td></td><td></td><td>second after</td></tr><tr><td></td><td></td><td>they stopped</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td>5.  Label the</td></tr><tr><td></td><td></td><td>segment</td></tr><tr><td></td><td></td><td>according to the</td></tr><tr><td></td><td></td><td>Person ID</td></tr><tr><td></td><td></td><td>displayed in the</td></tr><tr><td></td><td></td><td>bounding box</td></tr><tr><td></td><td></td><td>around their</td></tr><tr><td></td><td></td><td>head</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td>6.  Repeat the</td></tr><tr><td></td><td></td><td>process for all</td></tr><tr><td></td><td></td><td>sounds made by</td></tr><tr><td></td><td></td><td>the people in</td></tr><tr><td></td><td></td><td>the video.</td></tr></tbody></table><p>+----------------------+----------------------+----------------------+
| <strong>Examples:</strong>        |                      |                      |
+----------------------+----------------------+----------------------+
| [<!-- -->[AV - Step 3 -      |                      |                      |
| Voice Activity       |                      |                      |
| Annotation           |                      |                      |
| Ex                   |                      |                      |
| ample.mp4]<!-- -->(htt |                      |                      |
| ps://drive.google.co |                      |                      |
| m/file/d/19zHRx6nC-l |                      |                      |
| i7wC0ivImC5b-KCjLMES |                      |                      |
| Pt/view?usp=sharing) |                      |                      |
|                      |                      |                      |
| ![](media/           |                      |                      |
| image34.png){width=&quot; |                      |                      |
| 6.239583333333333in&quot; |                      |                      |
| height=&quot;2            |                      |                      |
| .236111111111111in&quot;} |                      |                      |
+----------------------+----------------------+----------------------+</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="av-step-4-transcription">AV Step 4: Transcription<a aria-hidden="true" class="hash-link" href="#av-step-4-transcription" title="Direct link to heading">â€‹</a></h3><p><strong>Objective:</strong> Transcribe voice activity for all subjects in the video.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="av-step-5-correcting-speech-transcriptions-wip">AV Step 5: Correcting Speech Transcriptions <!-- -->[<!-- -->WIP<!-- -->]<a aria-hidden="true" class="hash-link" href="#av-step-5-correcting-speech-transcriptions-wip" title="Direct link to heading">â€‹</a></h3><p><strong>Objective:</strong> Correcting Speech Transcription annotation from Step 4.</p><p><strong>Annotation Task</strong> (see <a href="https://docs.google.com/document/d/1Wi-dRM9sKPtRdjdLIxGYxJYjfU72on3css-8IjrKAOc/edit?usp=sharing" target="_blank" rel="noopener noreferrer">Annotation instructions</a>
<!-- -->[<!-- -->WIP<!-- -->]<!-- -->):</p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th></tr></thead><tbody><tr><td>0</td><td>*Pre-load the</td><td>The task begins with</td></tr><tr><td></td><td>annotation tool.*</td><td>the pre-load of the</td></tr><tr><td></td><td></td><td>following things:</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td>-   Output of AV Step 3</td></tr><tr><td></td><td></td><td>&gt; (Speech</td></tr><tr><td></td><td></td><td>&gt; Segmentation per</td></tr><tr><td></td><td></td><td>&gt; Person ID)</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td>-   Output of AV Step 4</td></tr><tr><td></td><td></td><td>&gt; (Human</td></tr><tr><td></td><td></td><td>&gt; transcriptions)</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td>-   Automatic</td></tr><tr><td></td><td></td><td>&gt; transcriptions</td></tr><tr><td></td><td></td><td>&gt; from ASR</td></tr><tr><td></td><td></td><td>&gt; algorithms.</td></tr></tbody></table><p>+---------------+-------------------------+-------------------------+
| 1             | <em>For each human         | For each person with    |
|               | transcription chunk,    | the active voice        |
|               | identify the            | activity:               |
|               | corresponding person    |                         |
|               | IDs with voice activity | -   Listen to the video |
|               | on</em>                     |                         |
|               |                         | -   If the person&#x27;s     |
|               |                         |     &gt; speech is = to    |
|               |                         |     &gt; the content in    |
|               |                         |     &gt; the transcription |
|               |                         |     &gt; chunk, then copy  |
|               |                         |     &gt; this speech       |
|               |                         |     &gt; content from      |
|               |                         |     &gt; transcript into a |
|               |                         |     &gt; new dialog        |
|               |                         |     &gt; box/tag that      |
|               |                         |     &gt; corresponds to    |
|               |                         |     &gt; the person.       |
+---------------+-------------------------+-------------------------+
| 2             | <em>Repeat Step 1 for the  |                         |
|               | machine generated       |                         |
|               | transcription chunks.</em>  |                         |
+---------------+-------------------------+-------------------------+
| <strong>Examples:</strong> |                         |                         |
+---------------+-------------------------+-------------------------+
| TBU           |                         |                         |
+---------------+-------------------------+-------------------------+</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="social-step-1-camera-wearer-attention">Social Step 1: Camera-Wearer Attention<a aria-hidden="true" class="hash-link" href="#social-step-1-camera-wearer-attention" title="Direct link to heading">â€‹</a></h3><p><strong>Objective</strong>: Annotate temporal segments in which a person is looking
at the camera wearer.</p><p><strong>Annotation Task</strong> (see [<a href="https://docs.google.com/document/d/1CqgM73xrYuva5eKSfTmM4Tby7tso1jXX/edit?usp=sharing&amp;ouid=109871152660798629950&amp;rtpof=true&amp;sd=true" target="_blank" rel="noopener noreferrer">Annotation instructions</a>):</p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th></tr></thead><tbody><tr><td>1</td><td>*Watch the video and</td><td></td></tr><tr><td></td><td>find the time when</td><td></td></tr><tr><td></td><td>someone is looking</td><td></td></tr><tr><td></td><td>at the camera</td><td></td></tr><tr><td></td><td>wearer*</td><td></td></tr></tbody></table><p>+----------------------+----------------------+----------------------+
| 2                    | <em>Annotate the time   | 1.  Start an         |
|                      | segment using the    |     annotation when  |
|                      | time segment tool:â€¯</em> |     a person start   |
|                      |                      |     to look at the   |
|                      |                      |     camera wearer    |
|                      |                      |                      |
|                      |                      | 2.  Stop an          |
|                      |                      |     annotation when  |
|                      |                      |     a person stops   |
|                      |                      |     looking at the   |
|                      |                      |     camera wearer    |
|                      |                      |                      |
|                      |                      | 3.  Label the        |
|                      |                      |     segment          |
|                      |                      |     according to the |
|                      |                      |     Person ID        |
|                      |                      |     displayed in the |
|                      |                      |     bounding box     |
|                      |                      |     around their     |
|                      |                      |     head             |
|                      |                      |                      |
|                      |                      | 4.  Repeat the       |
|                      |                      |     process for all  |
|                      |                      |     cases in the     |
|                      |                      |     video.           |
+----------------------+----------------------+----------------------+
| <strong>Examples:</strong>        |                      |                      |
+----------------------+----------------------+----------------------+
| [<!-- -->[social_annotation  |                      |                      |
| _demo.mp4]<!-- -->(htt |                      |                      |
| ps://drive.google.co |                      |                      |
| m/file/d/10Z0Ge0bIXJ |                      |                      |
| NbhUZ61iT0bckCj1Vno- |                      |                      |
| G7/view?usp=sharing) |                      |                      |
|                      |                      |                      |
| <img src="/docs/assets/images/image17-1871d1fc1dbc6ff4b0bbcc34095e9b42.png"> | | |
+----------------------+----------------------+----------------------+</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="social-step-2-speech-target-classification">Social Step 2: Speech Target Classification<a aria-hidden="true" class="hash-link" href="#social-step-2-speech-target-classification" title="Direct link to heading">â€‹</a></h3><p><strong>Objective</strong>: Given already annotated AV Voice Activity segmentation,
the annotator is going to annotate the particular speech segments in
which the person is talking to the camera wearer.</p><p><strong>Annotation Task</strong> (see <a href="https://docs.google.com/document/d/1wnJqZESJpQrwaCkdFZWm8Makmb5bF4r5/edit?usp=sharing&amp;ouid=109871152660798629950&amp;rtpof=true&amp;sd=true" target="_blank" rel="noopener noreferrer">Annotation instructions</a>):</p><table><thead><tr><th><strong>#</strong></th><th><strong>Step</strong></th><th><strong>Sub-step</strong></th></tr></thead><tbody><tr><td>1</td><td>*Watch the video</td><td></td></tr><tr><td></td><td>with AV voice</td><td></td></tr><tr><td></td><td>segmentation results</td><td></td></tr><tr><td></td><td>(start-end time,</td><td></td></tr><tr><td></td><td>person ID)*</td><td></td></tr></tbody></table><p>+----------------------+----------------------+----------------------+
| 2                    | <em>Annotate segments   | 1.  </em>Identify a      |
|                      | where someone is     |     &gt; segment in     |
|                      | talking to the       |     &gt; which someone  |
|                      | camera wearer<em>       |     &gt; is talking to  |
|                      |                      |     &gt; the camera     |
|                      | </em>Repeat the process  |     &gt; wearer<em>        |
|                      | for all cases in the |                      |
|                      | video.</em>              | 2.  <em>Click the time  |
|                      |                      |     &gt; segment, then  |
|                      |                      |     &gt; you can see    |
|                      |                      |     &gt; the Voice      |
|                      |                      |     &gt; activity       |
|                      |                      |     &gt; annotation     |
|                      |                      |     &gt; information on |
|                      |                      |     &gt; the left side  |
|                      |                      |     &gt; bar.</em>          |
|                      |                      |                      |
|                      |                      | 3.  <em>Click the drop  |
|                      |                      |     &gt; down box below |
|                      |                      |     &gt; the &quot;Target of |
|                      |                      |                      |
|                      |                      |  &gt; Speech&quot;</em>![](media |
|                      |                      | /image8.png){width=&quot; |
|                      |                      | 3.198122265966754in&quot; |
|                      |                      |     &gt; height=&quot;1      |
|                      |                      | .030867235345582in&quot;} |
|                      |                      |                      |
|                      |                      | 4.  <em>In the dropdown |
|                      |                      |     &gt; menu, select   |
|                      |                      |                      |
|                      |                      |    &gt; &quot;Camera-Wearer&quot; |
|                      |                      |     &gt; if the speech  |
|                      |                      |     &gt; is only toward |
|                      |                      |     &gt; the camera     |
|                      |                      |     &gt; wearer.</em>       |
|                      |                      |                      |
|                      |                      | 5.  <em>Choose          |
|                      |                      |     &gt; &quot;Camera-Wearer |
|                      |                      |     &gt; and others&quot; if |
|                      |                      |     &gt; the speech     |
|                      |                      |     &gt; segment is     |
|                      |                      |     &gt; toward         |
|                      |                      |     &gt; multiple       |
|                      |                      |     &gt; people         |
|                      |                      |     &gt; including the  |
|                      |                      |     &gt; camera wearer  |
|                      |                      |     &gt; (e.g., talking |
|                      |                      |     &gt; to multiple    |
|                      |                      |     &gt; audience       |
|                      |                      |     &gt; members).</em>     |
|                      |                      |                      |
|                      |                      | 6.  <em>Repeat the      |
|                      |                      |     &gt; process for    |
|                      |                      |     &gt; all relevant   |
|                      |                      |     &gt; segments.</em>     |
+----------------------+----------------------+----------------------+
| <strong>Examples:</strong>        |                      |                      |
+----------------------+----------------------+----------------------+
| [<!-- -->[social_step2_ex    |                      |                      |
| ample.mp4]<!-- -->(htt |                      |                      |
| ps://drive.google.co |                      |                      |
| m/file/d/1KUuaEr86sa |                      |                      |
| nTGI0-oNAYlgrvxru1TN |                      |                      |
| fw/view?usp=sharing) |                      |                      |
|                      |                      |                      |
| ![](media/           |                      |                      |
| image40.png){width=&quot; |                      |                      |
| 6.239583333333333in&quot; |                      |                      |
| height=&quot;3.           |                      |                      |
| 6666666666666665in&quot;} |                      |                      |
+----------------------+----------------------+----------------------+</p><table><tr><td></td><td></td></tr></table></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://https://ego4d-data.org/docs/data-overview.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_mS5F" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_mt2f"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/benchmarks/Social/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« <!-- -->Social Interactions</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/annotations/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Annotations<!-- --> Â»</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_vrFS thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#background" class="table-of-contents__link toc-highlight">Background</a></li><li><a href="#key-information" class="table-of-contents__link toc-highlight">Key Information</a></li><li><a href="#annotations-tldr" class="table-of-contents__link toc-highlight">Annotations tl;dr</a></li><li><a href="#narrations" class="table-of-contents__link toc-highlight">Narrations</a></li><li><a href="#benchmark-annotations" class="table-of-contents__link toc-highlight">Benchmark Annotations</a></li><li><a href="#episodic-memory" class="table-of-contents__link toc-highlight">Episodic Memory</a><ul><li><a href="#natural-language-queries" class="table-of-contents__link toc-highlight">Natural Language Queries</a></li><li><a href="#moments" class="table-of-contents__link toc-highlight">Moments</a></li><li><a href="#visual-object-queries" class="table-of-contents__link toc-highlight">Visual Object Queries</a></li></ul></li><li><a href="#forecasting--hands--objects-fho" class="table-of-contents__link toc-highlight">Forecasting + Hands &amp; Objects (FHO)</a><ul><li><a href="#stage-1---critical-frames" class="table-of-contents__link toc-highlight">Stage 1 - Critical Frames</a></li><li><a href="#stage-2---pre-condition" class="table-of-contents__link toc-highlight">Stage 2 - Pre-condition</a></li><li><a href="#stage-3---post-condition" class="table-of-contents__link toc-highlight">Stage 3 - Post-condition</a></li></ul></li><li><a href="#audio-visual-diarization--social-avs" class="table-of-contents__link toc-highlight">Audio-Visual Diarization &amp; Social (AVS)</a><ul><li><a href="#av-step-0-automated-face--head-detection" class="table-of-contents__link toc-highlight">AV Step 0: Automated Face &amp; Head Detection</a></li><li><a href="#av-step-1-face--head-tracks-correction" class="table-of-contents__link toc-highlight">AV Step 1: Face &amp; Head Tracks Correction</a></li><li><a href="#av-step-2-speaker-labeling-and-av-anchor-extraction" class="table-of-contents__link toc-highlight">AV Step 2: Speaker Labeling and AV anchor extraction</a></li><li><a href="#av-step-3-speech-segmentation-per-speaker" class="table-of-contents__link toc-highlight">AV Step 3: Speech Segmentation (Per Speaker)</a></li><li><a href="#av-step-4-transcription" class="table-of-contents__link toc-highlight">AV Step 4: Transcription</a></li><li><a href="#av-step-5-correcting-speech-transcriptions-wip" class="table-of-contents__link toc-highlight">AV Step 5: Correcting Speech Transcriptions [WIP]</a></li><li><a href="#social-step-1-camera-wearer-attention" class="table-of-contents__link toc-highlight">Social Step 1: Camera-Wearer Attention</a></li><li><a href="#social-step-2-speech-target-classification" class="table-of-contents__link toc-highlight">Social Step 2: Speech Target Classification</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/intro/">Intro</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/ego4d" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/Ego4d" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2021 Ego4d</div></div></div></footer></div>
<script src="/docs/assets/js/runtime~main.09a49ffe.js"></script>
<script src="/docs/assets/js/main.9f1b178e.js"></script>
</body>
</html>