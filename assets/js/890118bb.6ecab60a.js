"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[359],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>u});var n=a(7294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var s=n.createContext({}),c=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=c(e.components);return n.createElement(s.Provider,{value:t},e.children)},h="mdxType",g={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),h=c(a),d=i,u=h["".concat(s,".").concat(d)]||h[d]||g[d]||r;return a?n.createElement(u,o(o({ref:t},p),{},{components:a})):n.createElement(u,o({ref:t},p))}));function u(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=a.length,o=new Array(r);o[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[h]="string"==typeof e?e:i,o[1]=l;for(var c=2;c<r;c++)o[c]=a[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},9138:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var n=a(7462),i=(a(7294),a(3905));const r={sidebar_position:10},o="Ego4D and EgoExo4D Challenge 2025",l={unversionedId:"challenge",id:"challenge",title:"Ego4D and EgoExo4D Challenge 2025",description:"Overview",source:"@site/docs/challenge.md",sourceDirName:".",slug:"/challenge",permalink:"/docs/challenge",draft:!1,tags:[],version:"current",sidebarPosition:10,frontMatter:{sidebar_position:10},sidebar:"tutorialSidebar",previous:{title:"Model Zoo",permalink:"/docs/model-zoo"},next:{title:"Updates",permalink:"/docs/updates"}},s={},c=[{value:"<strong>Overview</strong>",id:"overview",level:2},{value:"<strong>Ego4D challenges</strong>",id:"ego4d-challenges",level:2},{value:"Episodic memory:",id:"episodic-memory",level:3},{value:"Social Understanding:",id:"social-understanding",level:3},{value:"Forecasting:",id:"forecasting",level:3},{value:"<strong>EgoExo4D challenges</strong>",id:"egoexo4d-challenges",level:2},{value:"EgoPose Benchmark",id:"egopose-benchmark",level:3},{value:"Relations Benchmark",id:"relations-benchmark",level:3},{value:"Keystep Benchmark",id:"keystep-benchmark",level:3},{value:"Proficiency Benchmark",id:"proficiency-benchmark",level:3},{value:"Dataset",id:"dataset",level:2},{value:"Participation Guidelines",id:"participation-guidelines",level:2},{value:"Dates",id:"dates",level:2},{value:"Competition Rules and Prize Information",id:"competition-rules-and-prize-information",level:2},{value:"Challenge Reports",id:"challenge-reports",level:2},{value:"Acknowledgements",id:"acknowledgements",level:2},{value:"Organizers",id:"organizers",level:3},{value:"Past Challenges / Winners",id:"past-challenges--winners",level:2}],p={toc:c};function h(e){let{components:t,...a}=e;return(0,i.kt)("wrapper",(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"ego4d-and-egoexo4d-challenge-2025"},"Ego4D and EgoExo4D Challenge 2025"),(0,i.kt)("h2",{id:"overview"},(0,i.kt)("strong",{parentName:"h2"},"Overview")),(0,i.kt)("p",null,"At ",(0,i.kt)("a",{parentName:"p",href:"https://egovis.github.io/cvpr25/"},"EgoVis")," workshop during CVPR 2025, we will host ",(0,i.kt)("strong",{parentName:"p"},"15")," challenges representing Ego4D and EgoExo4D benchmarks. This year we will have ",(0,i.kt)("strong",{parentName:"p"},"9")," challenges from Ego4D and ",(0,i.kt)("strong",{parentName:"p"},"6")," challenges from EgoExo4D dataset. Please find details below on the challenges:"),(0,i.kt)("h2",{id:"ego4d-challenges"},(0,i.kt)("strong",{parentName:"h2"},"Ego4D challenges")),(0,i.kt)("h3",{id:"episodic-memory"},(0,i.kt)("a",{parentName:"h3",href:"/docs/benchmarks/episodic-memory"},"Episodic memory"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1843/overview"},"Visual queries with 2D localization (VQ2D)"),": Given an egocentric video clip and an image crop depicting the query object, return the most recent occurrence of the object in the input video, in terms of contiguous bounding boxes (2D + temporal localization). ",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Quickstart: ",(0,i.kt)("a",{parentName:"li",href:"https://colab.research.google.com/drive/1vtVOQzLarBCspQjH5RtHZ8qzH0VZxrmZ?usp=sharing"},(0,i.kt)("img",{parentName:"a",src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open in Colab"}))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1629/overview"},"Natural language queries (NLQ)"),": Given a video clip and a query expressed in natural language, localize the temporal window within all the video history where the answer to the question is evident.",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Quickstart: ",(0,i.kt)("a",{parentName:"li",href:"https://colab.research.google.com/drive/1S1LTplak-Fno3lMumCLoIfzYsx_TfNes?usp=sharing"},(0,i.kt)("img",{parentName:"a",src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open in Colab"}))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1626/overview"},"Moments queries (MQ)"),": Given an egocentric video and an activity name (e.g., a \u201cmoment\u201d), localize all instances of that activity in the past video"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/2188/overview"},"Goal Step"),": Given an untrimmed egocentric video, identify the temporal action segment corresponding to a natural language description of the step. Specifically, predict the (start_time, end_time) for a given keystep description."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/2238/overview"},"Ego Schema"),": Given a very long-form video, evaluate the capabilities of modern vision and language systems.")),(0,i.kt)("h3",{id:"social-understanding"},(0,i.kt)("a",{parentName:"h3",href:"/docs/benchmarks/social"},"Social Understanding"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1624/overview"},"Looking at me"),": Given an egocentric video clip, identify whether someone in the scene is looking at the camera wearer."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1625/overview"},"Talking to me"),": Given an egocentric video clip, identify whether someone in the scene is talking to the camera wearer.")),(0,i.kt)("h3",{id:"forecasting"},(0,i.kt)("a",{parentName:"h3",href:"/docs/benchmarks/forecasting"},"Forecasting"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1623/overview"},"Short Term object interaction anticipation"),": Given a video clip, predict the next active objects, and, for each of them, predict the next action, and the time to contact.",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Quickstart: ",(0,i.kt)("a",{parentName:"li",href:"https://colab.research.google.com/drive/1Ok_6F1O6K8kX1S4sEnU62HoOBw_CPngR?usp=sharing"},(0,i.kt)("img",{parentName:"a",src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open in Colab"}))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1598/overview"},"Long-term activity prediction"),": Given a video clip, the goal is to predict what sequence of activities will happen in the future. For example, after kneading dough, list the actions that the baker will do next. ")),(0,i.kt)("p",null,"Other Ego4D challenges which are not part of CVPR 2025 workshop remain open on EvalAI website for submissions but are not eligible for prizes."),(0,i.kt)("h2",{id:"egoexo4d-challenges"},(0,i.kt)("strong",{parentName:"h2"},"EgoExo4D challenges")),(0,i.kt)("p",null,"Ego-Exo4D is a diverse, large-scale multi-modal multi view video dataset and benchmark challenge. Ego-Exo4D centers around simultaneously-captured ego-centric and exocentric video of skilled human activities (e.g., sports, music, dance, bike repair). "),(0,i.kt)("p",null,"Here are the specific challenge tracks we will host at ",(0,i.kt)("a",{parentName:"p",href:"https://egovis.github.io/cvpr25/"},"EgoVis workshop")," during CVPR 2025."),(0,i.kt)("h3",{id:"egopose-benchmark"},"EgoPose Benchmark"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/2245/overview"},"Ego-Pose Body"),": Given an egocentric video, estimate the 3D body pose of the camera-wearer. Specifically, predict the 3D position of the 17 annotated body joints for each frame. ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/EGO4D/ego-exo4d-egopose/tree/main/bodypose"},"[github]")," ",(0,i.kt)("a",{parentName:"li",href:"https://docs.ego-exo4d-data.org/tutorials/"},"[tutorials]")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/2249/overview"},"Ego-Pose Hands"),": Estimate the 3D locations of the defined hand joints for visible hand(s). Specifically, estimate the (x,y,z) coordinates of each joint in the egocentric coordinate frame. ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/EGO4D/ego-exo4d-egopose/tree/main/handpose"},"[github]")," ",(0,i.kt)("a",{parentName:"li",href:"https://docs.ego-exo4d-data.org/tutorials/"},"[tutorials]"))),(0,i.kt)("h3",{id:"relations-benchmark"},"Relations Benchmark"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/2288/overview"},"Correspondence"),": Given a pair of timesynchronized egocentric and exocentric videos, as well as a query object track in one of the views, the goal is to output the corresponding mask for the same object instance in the other view for all frames where the object is visible in both views. ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/EGO4D/ego-exo4d-relation/tree/main/correspondence/"},"[github]"))),(0,i.kt)("h3",{id:"keystep-benchmark"},"Keystep Benchmark"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/2273/overview"},"Fine-grained Keystep Recognition"),": The objective of this task is to predict the keystep label for a trimmed egocentric video clip. ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/EGO4D/ego-exo4d-keystep/tree/main/fine_grained/"},"[github]")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/2286/overview"},"Procedure Understanding"),": The objective of this task is to infer a procedure's underlying structure from observing natural videos of subjects performing the procedure. ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/EGO4D/ego-exo4d-keystep/tree/main/procedure_understanding"},"[github]"))),(0,i.kt)("h3",{id:"proficiency-benchmark"},"Proficiency Benchmark"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/2291/overview"},"Demonstrator Proficiency"),":Given synchronized egocentric and exocentric video of a demonstrator performing a task, classify the proficiency skill level of the demonstrator. ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/EGO4D/ego-exo4d-proficiency/blob/main/demonstrator_proficiency/"},"github"))),(0,i.kt)("p",null,"Other EgoExo4D challenges which are not part of CVPR 2025 workshop remain open on EvalAI website for submissions but are not eligible for prizes."),(0,i.kt)("h2",{id:"dataset"},"Dataset"),(0,i.kt)("p",null,"Ego4D challenge participants will use Ego4D\u2019s annotated data set of more than 3,670 hours of video data, capturing the daily-life scenarios of more than 900 unique individuals from nine different countries around the world. Unique train, validation and unannotated test sets are available to download per challenge at ",(0,i.kt)("a",{parentName:"p",href:"https://ego4d-data.org/docs/"},"https://ego4d-data.org/docs/"),". This year's challenge we will continue to use Ego4D v2.0 which contains ~2X train and val annotations for Forecasting, Hands & Objects and NLQ, a number of corrections and usability enhancements, and two new related dataset enhancements (Ego Schema and Goal Step). The test set remains the same as previous versions of the challenge. More details can be found ",(0,i.kt)("a",{parentName:"p",href:"https://ego4d-data.org/docs/updates/"},"here"),". "),(0,i.kt)("p",null,"EgoExo4D challenge participants will be using EgoExo4D dataset for these challenges. Please find the ",(0,i.kt)("a",{parentName:"p",href:"https://docs.ego-exo4d-data.org/"},"documentation")," here about the dataset."),(0,i.kt)("h2",{id:"participation-guidelines"},"Participation Guidelines"),(0,i.kt)("p",null,"Participate in the contest by registering on the ",(0,i.kt)("a",{parentName:"p",href:"https://eval.ai/"},"EvalAI challenge page")," and creating a team. All participants must register as a part of a \u201cparticipating team\u201d on EvalAI to ensure the submission limits are honored. Participants will upload their predictions in the format specified for the specific challenge, and will be evaluated on AWS instances by comparing to ground truth predictions. Instructions for training, local evaluation, and online submission are provided at EvalAI. Please refer to the individual EvalAI pages for each challenge for submission guidelines, task specifications, and evaluation criteria."),(0,i.kt)("h2",{id:"dates"},"Dates"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Ego4D challenges will launch on March 5, 2025 with the leaderboard closing on May 19, 2025. "),(0,i.kt)("li",{parentName:"ul"},"EgoExo4D challenges will launch on March 5, 2025 with the leaderboard closing on May 19, 2025. "),(0,i.kt)("li",{parentName:"ul"},"Winners for both will be announced at the ",(0,i.kt)("a",{parentName:"li",href:"https://egovis.github.io/cvpr24/"},"Second Joint Egocentric Vision Workshop")," at CVPR 2025. ")),(0,i.kt)("h2",{id:"competition-rules-and-prize-information"},"Competition Rules and Prize Information"),(0,i.kt)("p",null,"Competition rules can be found ",(0,i.kt)("a",{parentName:"p",href:"pathname:///tc.pdf"},"here"),". Additionally, we are thrilled that FAIR is able to offer the following prize thresholds for challenges:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"First place: $500"),(0,i.kt)("li",{parentName:"ul"},"Second place: $300"),(0,i.kt)("li",{parentName:"ul"},"Third place: $200")),(0,i.kt)("h2",{id:"challenge-reports"},"Challenge Reports"),(0,i.kt)("p",null,"In addition to the submission on EvalAI, participants must submit a report describing their method to the workshop CMT ",(0,i.kt)("a",{parentName:"p",href:"https://cmt3.research.microsoft.com/EgoVis2025/"},"link"),". In addition to your method and results, please remember to include examples of positive and negative results (limitations) of your model. These validation reports will be evaluated by challenge hosts from the Ego4D consortium before winner determination can be made. Similarly, challenge validation reports, research code from winning entries, and names of participants from the winning teams for all successful submissions must be shared publicly with the research community.  More details can be found on the ",(0,i.kt)("a",{parentName:"p",href:"https://egovis.github.io/cvpr25/"},"EgoVis workshop page"),". "),(0,i.kt)("h2",{id:"acknowledgements"},"Acknowledgements"),(0,i.kt)("p",null,"The Ego4D and EgoExo4D challenges would not have been possible without the infrastructure and support of the ",(0,i.kt)("a",{parentName:"p",href:"https://eval.ai/team"},"EvalAI team"),". Thank you!"),(0,i.kt)("h3",{id:"organizers"},"Organizers"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Xizi Wang")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Suyog Jain")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Andrew Westbury")),(0,i.kt)("li",{parentName:"ul"},"Chen Zhao"),(0,i.kt)("li",{parentName:"ul"},"Merey Ramazanova"),(0,i.kt)("li",{parentName:"ul"},"Francesco Ragusa"),(0,i.kt)("li",{parentName:"ul"},"Seminara Luigi "),(0,i.kt)("li",{parentName:"ul"},"Tushar Nagarajan"),(0,i.kt)("li",{parentName:"ul"},"Karttikeya Mangalam"),(0,i.kt)("li",{parentName:"ul"},"Raiymbek Akshulakov"),(0,i.kt)("li",{parentName:"ul"},"Sherry Xue "),(0,i.kt)("li",{parentName:"ul"},"Jinxu Zhang"),(0,i.kt)("li",{parentName:"ul"},"Shan Shu"),(0,i.kt)("li",{parentName:"ul"},"Gabriel P\xe9rez Santamaria"),(0,i.kt)("li",{parentName:"ul"},"Juanita Puentes"),(0,i.kt)("li",{parentName:"ul"},"Maria Camila Escobar Palomeque"),(0,i.kt)("li",{parentName:"ul"},"Arjun Somayazulu"),(0,i.kt)("li",{parentName:"ul"},"Sanjay Haresh"),(0,i.kt)("li",{parentName:"ul"},"Yale Song"),(0,i.kt)("li",{parentName:"ul"},"Antonino Furnari"),(0,i.kt)("li",{parentName:"ul"},"Manolis Savva"),(0,i.kt)("li",{parentName:"ul"},"Giovanni Maria Farinella"),(0,i.kt)("li",{parentName:"ul"},"Pablo Arbelaez"),(0,i.kt)("li",{parentName:"ul"},"Jianbo Shi"),(0,i.kt)("li",{parentName:"ul"},"Kristen Grauman")),(0,i.kt)("h2",{id:"past-challenges--winners"},"Past Challenges / Winners"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("a",{parentName:"strong",href:"https://egovis.github.io/cvpr24/"},"CVPR Workshop 2024"))," (June 17, 2024)"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("a",{parentName:"strong",href:"https://ego4d-data.org/workshops/cvpr23/"},"CVPR Workshop 2023"))," (June 19, 2023)"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("a",{parentName:"strong",href:"https://ego4d-data.org/workshops/eccv22/"},"ECCV Workshop 2022"))," (Oct 24, 2022)"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("a",{parentName:"strong",href:"https://ego4d-data.org/workshops/cvpr22/"},"CVPR Workshop 2022"))," (June 19, 2022)"))}h.isMDXComponent=!0}}]);