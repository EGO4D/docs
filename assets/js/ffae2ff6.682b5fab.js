"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[712],{3905:(t,e,a)=>{a.d(e,{Zo:()=>m,kt:()=>g});var n=a(7294);function r(t,e,a){return e in t?Object.defineProperty(t,e,{value:a,enumerable:!0,configurable:!0,writable:!0}):t[e]=a,t}function l(t,e){var a=Object.keys(t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(t);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),a.push.apply(a,n)}return a}function i(t){for(var e=1;e<arguments.length;e++){var a=null!=arguments[e]?arguments[e]:{};e%2?l(Object(a),!0).forEach((function(e){r(t,e,a[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(a,e))}))}return t}function o(t,e){if(null==t)return{};var a,n,r=function(t,e){if(null==t)return{};var a,n,r={},l=Object.keys(t);for(n=0;n<l.length;n++)a=l[n],e.indexOf(a)>=0||(r[a]=t[a]);return r}(t,e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(t);for(n=0;n<l.length;n++)a=l[n],e.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(t,a)&&(r[a]=t[a])}return r}var s=n.createContext({}),p=function(t){var e=n.useContext(s),a=e;return t&&(a="function"==typeof t?t(e):i(i({},e),t)),a},m=function(t){var e=p(t.components);return n.createElement(s.Provider,{value:e},t.children)},d="mdxType",k={inlineCode:"code",wrapper:function(t){var e=t.children;return n.createElement(n.Fragment,{},e)}},u=n.forwardRef((function(t,e){var a=t.components,r=t.mdxType,l=t.originalType,s=t.parentName,m=o(t,["components","mdxType","originalType","parentName"]),d=p(a),u=r,g=d["".concat(s,".").concat(u)]||d[u]||k[u]||l;return a?n.createElement(g,i(i({ref:e},m),{},{components:a})):n.createElement(g,i({ref:e},m))}));function g(t,e){var a=arguments,r=e&&e.mdxType;if("string"==typeof t||r){var l=a.length,i=new Array(l);i[0]=u;var o={};for(var s in e)hasOwnProperty.call(e,s)&&(o[s]=e[s]);o.originalType=t,o[d]="string"==typeof t?t:r,i[1]=o;for(var p=2;p<l;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},5646:(t,e,a)=>{a.r(e),a.d(e,{assets:()=>s,contentTitle:()=>i,default:()=>d,frontMatter:()=>l,metadata:()=>o,toc:()=>p});var n=a(7462),r=(a(7294),a(3905));const l={title:"Annotation Guidelines",sidebar_position:1},i=void 0,o={unversionedId:"data/annotation-guidelines",id:"data/annotation-guidelines",title:"Annotation Guidelines",description:"This page contains context mostly on the annotation guidelines used in each tasks.  Please also see annotations for the specific formats and benchmark tasks for more detail on the tasks themselves.  And please read the paper here for the most comprehensive introduction.",source:"@site/docs/data/annotation-guidelines.md",sourceDirName:"data",slug:"/data/annotation-guidelines",permalink:"/docs/data/annotation-guidelines",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"Annotation Guidelines",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Welcome To EGO4D!",permalink:"/docs/"},next:{title:"Metadata",permalink:"/docs/data/metadata"}},s={},p=[{value:"Annotations tl;dr",id:"annotations-tldr",level:2},{value:"Narrations",id:"narrations",level:2},{value:"Benchmark Annotations",id:"benchmark-annotations",level:2},{value:"Episodic Memory",id:"episodic-memory",level:2},{value:"Natural Language Queries",id:"natural-language-queries",level:3},{value:"Moments",id:"moments",level:3},{value:"Visual Object Queries",id:"visual-object-queries",level:3},{value:"Forecasting + Hands &amp; Objects (FHO)",id:"forecasting--hands--objects-fho",level:2},{value:"Stage 1 - Critical Frames",id:"stage-1---critical-frames",level:3},{value:"Stage 2 - Pre-condition",id:"stage-2---pre-condition",level:3},{value:"Stage 3 - Post-condition",id:"stage-3---post-condition",level:3},{value:"Audio-Visual Diarization &amp; Social (AVS)",id:"audio-visual-diarization--social-avs",level:2},{value:"AV Step 0: Automated Face &amp; Head Detection",id:"av-step-0-automated-face--head-detection",level:3},{value:"AV Step 1: Face &amp; Head Tracks Correction",id:"av-step-1-face--head-tracks-correction",level:3},{value:"AV Step 2: Speaker Labeling and AV anchor extraction",id:"av-step-2-speaker-labeling-and-av-anchor-extraction",level:3},{value:"AV Step 3: Speech Segmentation (Per Speaker)",id:"av-step-3-speech-segmentation-per-speaker",level:3},{value:"AV Step 4: Transcription",id:"av-step-4-transcription",level:3},{value:"AV Step 5: Correcting Speech Transcriptions [WIP]",id:"av-step-5-correcting-speech-transcriptions-wip",level:3},{value:"Social Step 1: Camera-Wearer Attention",id:"social-step-1-camera-wearer-attention",level:3},{value:"Social Step 2: Speech Target Classification",id:"social-step-2-speech-target-classification",level:3}],m={toc:p};function d(t){let{components:e,...l}=t;return(0,r.kt)("wrapper",(0,n.Z)({},m,l,{components:e,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"This page contains context mostly on the annotation guidelines used in each tasks.  Please also see ",(0,r.kt)("a",{parentName:"p",href:"/docs/data/annotations-schemas"},"annotations")," for the specific formats and ",(0,r.kt)("a",{parentName:"p",href:"/docs/benchmarks/overview"},"benchmark tasks")," for more detail on the tasks themselves.  And please ",(0,r.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2110.07058"},"read the paper here")," for the most comprehensive introduction."),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"Numbers quoted below are those available at the time of writing this documentation; authoritative information is found in our arxiv paper.")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Devices:")),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(2509).Z,width:"2048",height:"783"})),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Scenario breakdown:")),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(4534).Z,width:"2048",height:"703"})),(0,r.kt)("h2",{id:"annotations-tldr"},"Annotations tl;dr"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Task")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Output")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Volume")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"Pre-annotations")),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#pre-annotations-narrations"},"Narrations")),(0,r.kt)("td",{parentName:"tr",align:null},"Dense written sentence narrations in English & a summary of the whole video clip"),(0,r.kt)("td",{parentName:"tr",align:null},"Full Dataset")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"Episodic Memory (EM)")),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#natural-language-queries"},"Natural Language Queries")),(0,r.kt)("td",{parentName:"tr",align:null},"N free-form natural language queries per video (N=length of video in minutes) selected from a list of query templates + temporal response window from which answers can be deduced"),(0,r.kt)("td",{parentName:"tr",align:null},"~","240h")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#moments"},"Moments")),(0,r.kt)("td",{parentName:"tr",align:null},"Temporal localizations of high level events in a long video clip from a provided taxonomy"),(0,r.kt)("td",{parentName:"tr",align:null},"~","300h")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#visual-object-queries"},"Visual Object Queries")),(0,r.kt)("td",{parentName:"tr",align:null},"For N=3 ",(0,r.kt)("strong",{parentName:"td"},"query objects")," (freely chosen and ",(0,r.kt)("strong",{parentName:"td"},"named")," by the annotator) such that each appears at least twice at separate times in a single video, annotations include: ",(0,r.kt)("br",null)," ","(","1",")"," ",(0,r.kt)("strong",{parentName:"td"},"response track"),": bounding boxes over time for one continuous occurrence of the query object; ",(0,r.kt)("br",null)," ","(","2",")"," ",(0,r.kt)("strong",{parentName:"td"},"query frame"),": a frame that ",(0,r.kt)("em",{parentName:"td"},"does")," ",(0,r.kt)("em",{parentName:"td"},"not")," contain the query object, sometime after the response track but before any subsequent occurrence of the object; ",(0,r.kt)("br",null)," ","(","3",")"," ",(0,r.kt)("strong",{parentName:"td"},"visual crop"),":  bounding box of a single frame from another occurrence of the same object elsewhere in the video (before or after the originally marked instance)"),(0,r.kt)("td",{parentName:"tr",align:null},"~","403h")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"Forecasting + Hands & Objects (FHO)")),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#stage-1---critical-frames"},"1 Critical Frames")),(0,r.kt)("td",{parentName:"tr",align:null},"Pre-condition (PRE), CONTACT, point of no return (PNR), and post-condition (Post) frames for each narrated action in a video"),(0,r.kt)("td",{parentName:"tr",align:null},"~","120h")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#stage-2---pre-condition"},"2 Pre-condition")),(0,r.kt)("td",{parentName:"tr",align:null},"Bounding boxes and roles for hands (right/left) and objects (objects of change and tools) for each frame from CONTACT to PRE"),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#stage-3---post-condition"},"3 Post-condition")),(0,r.kt)("td",{parentName:"tr",align:null},"Bounding boxes and roles for hands and objects for each frame from CONTACT to POST"),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"Audio-Visual Diarization & Social (AVS)")),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#av-step-0-automated-face-head-detection"},"AV0: Automated Face & Head Detection")),(0,r.kt)("td",{parentName:"tr",align:null},"Automated overlaid bounding boxes for faces in video clips"),(0,r.kt)("td",{parentName:"tr",align:null},"50h")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#av-step-1-face-head-tracks-correction"},"AV1: Face & Head Tracks Correction")),(0,r.kt)("td",{parentName:"tr",align:null},"Manually adjusted overlaid bounding boxes for faces in video clips"),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#av-step-2-speaker-labeling-and-av-anchor-extraction"},"AV2: Speaker Labeling and AV anchor extraction")),(0,r.kt)("td",{parentName:"tr",align:null},"Anonymous Person IDs for each Face Track in video clip"),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#av-step-3-speech-segmentation-per-speaker"},"AV3: Speech Segmentation (Per Speaker)")),(0,r.kt)("td",{parentName:"tr",align:null},"Temporal segments for voice activity for the camera wearer and for each Person ID"),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#av-step-4-transcription"},"AV4: Transcription")),(0,r.kt)("td",{parentName:"tr",align:null},"Video clip audio transcriptions"),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#av-step-5-correcting-speech-transcriptions-wip"},"AV5: Correcting Speech Transcriptions")),(0,r.kt)("td",{parentName:"tr",align:null},"Corrected Speech Transcription annotations matching voice activity segments and Person IDs from AV2"),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#s-step-1-camera-wearer-attention"},"S1: Camera-Wearer Attention")),(0,r.kt)("td",{parentName:"tr",align:null},"Temporal segments in which a person is looking at the camera wearer"),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#s-step-2-speech-target-classification"},"S2: Speech Target Classification")),(0,r.kt)("td",{parentName:"tr",align:null},"Temporal segments in which a person is talking to the camera wearer"),(0,r.kt)("td",{parentName:"tr",align:null})))),(0,r.kt)("h2",{id:"narrations"},"Narrations"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Objective:")," Annotator provides dense written sentence narrations in\nEnglish on a first-person video clip of length 10-30 minutes + a summary\nof the whole video."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Motivation:")," Understand what data is available and which data to push\nthrough which annotation phases. Provide a starting point for forming a\ntaxonomy of labels for actions and objects."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Tags:")),(0,r.kt)("p",null,"There are four flags that annotators use in the sentence boxes:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"#unsure")," to denote they are unsure about a specific statement"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"#summary")," to denote they are giving the overall video"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"#C")," to denote the sentence is an action done by the camera wearer (the person who recorded the video while wearing a camera on their head)"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"#O")," to denote that the sentence is an action done by someone other than the camera wearer")),(0,r.kt)("p",null,"Note that every sentence will have either ",(0,r.kt)("inlineCode",{parentName:"p"},"#C")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"#O"),".  Only some sentences (or none) may have ",(0,r.kt)("inlineCode",{parentName:"p"},"#unsure"),".  Only one sentence for the entire video clip will have ",(0,r.kt)("inlineCode",{parentName:"p"},"#summary"),"."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotation task:")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"#")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Sub-step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Example")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Narrate the Complete Video with Temporal Sentences")),(0,r.kt)("td",{parentName:"tr",align:null},"Watch the video from the beginning until something new occurs. ",(0,r.kt)("hr",null),'At that time, pause the video, mark the temporal window for which the sentence applies, then "narrate" what you see in the video by typing in a simple sentence into the free-form text input. ',(0,r.kt)("hr",null)," Next, resume watching the video. Once you recognize an action to narrate, immediately pause again and repeat."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"[set the start time as the point when the person has the knife and the tomato, and the end time as the point when the person has finished chopping, then type]",":"),' "C is chopping a tomato" into the text input.',(0,r.kt)("br",null),'("C" refers to the camera wearer).')),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Provide a Summary of the Entire Video")),(0,r.kt)("td",{parentName:"tr",align:null},"As needed, watch the entire video on fast forward to recall the content of the entire video.",(0,r.kt)("br",null),(0,r.kt)("br",null),"Provide a short summary in text about the contents of the entire video (1-3 sentences).",(0,r.kt)("br",null),(0,r.kt)("br",null),"This summary should convey the main setting(s) of the video clip (e.g., an apartment, a restaurant, a shop, etc.) as well as an overview of what happened."),(0,r.kt)("td",{parentName:"tr",align:null},'#summary C fixed their breakfast, ate it, then got dressed and left the house."')))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotated video examples:")),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(1637).Z,width:"1714",height:"968"}),(0,r.kt)("img",{src:a(2611).Z,width:"1263",height:"908"})),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotation Stats")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Total hours narrated:")," 3670")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Unique scenarios:")," 51"))),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(4307).Z,width:"1380",height:"560"})),(0,r.kt)("h2",{id:"benchmark-annotations"},"Benchmark Annotations"),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(467).Z,width:"1248",height:"702"})),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Target")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"#")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Benchmark task")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Research Goal")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"Places")),(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#episodic-memory"},"Episodic Memory")),(0,r.kt)("td",{parentName:"tr",align:null},"Allow an user to ask free-form, natural language questions, with the answer brought back after analyzing past video (",(0,r.kt)("em",{parentName:"td"},"When was the last time I changed the batteries in the smoke detector?"),").")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"Objects")),(0,r.kt)("td",{parentName:"tr",align:null},"2"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#forecasting--hands--objects-fho"},"Forecasting")),(0,r.kt)("td",{parentName:"tr",align:null},"To intelligently deliver notifications to a user, an AR system must understand how an action or piece of information may impact the future state of the world.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"3"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#forecasting--hands--objects-fho"},"Hands-Object interaction")),(0,r.kt)("td",{parentName:"tr",align:null},"AR applications, e.g. providing users instructions in their egocentric real-world view to accomplish tasks (e.g., cooking a recipe).")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"People")),(0,r.kt)("td",{parentName:"tr",align:null},"4"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#audio-visual-diarization--social-avs"},"Audio-visual Diarization")),(0,r.kt)("td",{parentName:"tr",align:null},"To effectively aid people in daily life scenarios, augmented reality must be able to detect and track sounds, responding to users queries or information needs.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"5"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"#audio-visual-diarization--social-avs"},"Social interactions")),(0,r.kt)("td",{parentName:"tr",align:null},"Recognize people's interactions, their roles, and their attention within collaborative and competitive scenarios within a range of social interactions captured in the Ego4D data.")))),(0,r.kt)("h2",{id:"episodic-memory"},"Episodic Memory"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Motivation"),": Augment human memory through personal semantic video\nindex for an always-on wearable camera"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Objective"),": Given long first-person video, ",(0,r.kt)("em",{parentName:"p"},"localize")," answers for\nqueries about objects and events from first-person experience"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Who did I sit by at the party? Where are my keys? When did I change the batteries? How often did I read to my child last week? Did I leave the window open?",".","..")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Query types (annotation sub-tasks)"),":"),(0,r.kt)("p",null,"a.  Natural language queries (response = temporal)"),(0,r.kt)("p",null,"b.  Moments queries (response = temporal)"),(0,r.kt)("p",null,"c.  Visual/object queries (response = temporal+spatial)"),(0,r.kt)("h3",{id:"natural-language-queries"},"Natural Language Queries"),(0,r.kt)("h3",{id:""},(0,r.kt)("img",{src:a(5228).Z,width:"1100",height:"314"})),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Objective:")," Create and annotate ",(0,r.kt)("strong",{parentName:"p"},"N (N=length of video in minutes)"),"\ninteresting questions and their corresponding answers for the given\nvideo."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotation Task:")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"#")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Sub-step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Example")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"0"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Annotator watches video")),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Asks free-form natural language query at end of video, selecting from list of query templates.")),(0,r.kt)("td",{parentName:"tr",align:null},"- Select an interesting query template & template category. ",(0,r.kt)("br",null),"- Paraphrase question in the past tense."),(0,r.kt)("td",{parentName:"tr",align:null},"- ",(0,r.kt)("ins",null,"Template"),': "What ',(0,r.kt)("span",{style:{color:"#cb4b16"}},(0,r.kt)("strong",null,"X"))," is ",(0,r.kt)("span",{style:{color:"#268bd2"}},(0,r.kt)("strong",null,"Y")),'?" ',(0,r.kt)("br",null),"- ",(0,r.kt)("ins",null,"Template Category"),': "Objects" ',(0,r.kt)("br",null)," - ",(0,r.kt)("ins",null,"Paraphrased query"),': "What ',(0,r.kt)("span",{style:{color:"#cb4b16"}},(0,r.kt)("strong",null,"color"))," shirt did the ",(0,r.kt)("span",{style:{color:"#268bd2"}},(0,r.kt)("strong",null,"person performing on the road")),' wear?"')),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"Using ",(0,r.kt)("strong",{parentName:"td"},'"free-form"')," text, fill the ",(0,r.kt)("strong",{parentName:"td"},"query slots")," (X, Y, ...) in the template to form a meaningful question equivalent to the paraphrase."),(0,r.kt)("td",{parentName:"tr",align:null},'First free-form query slot: "',(0,r.kt)("span",{style:{color:"#cb4b16"}},(0,r.kt)("strong",null,"color")),'" ',(0,r.kt)("br",null),(0,r.kt)("br",null),'Second free-form query slot: "',(0,r.kt)("span",{style:{color:"#268bd2"}},(0,r.kt)("strong",null,"the shirt of the person performing on the road")),'"')),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"Pick the closest verb for each of the slots in the respective drop-down menus"),(0,r.kt)("td",{parentName:"tr",align:null},"- ",(0,r.kt)("ins",null,"Paraphrased query"),": What ",(0,r.kt)("span",{style:{color:"#cb4b16"}},(0,r.kt)("strong",null,"instrument"))," was ",(0,r.kt)("span",{style:{color:"#268bd2"}},(0,r.kt)("strong",null,"the musician playing")),"? ",(0,r.kt)("br",null),"- ",(0,r.kt)("ins",null,"First verb drop-down selection"),': "',"[VERB NOT APPLICABLE]",'" ',(0,r.kt)("br",null),' Second verb drop-down selection:  "',(0,r.kt)("strong",{parentName:"td"},"play"),'"')),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Identifies the temporal response window from which answer can be deduced")),(0,r.kt)("td",{parentName:"tr",align:null},"Seek in the video to the temporal window where the response to the natural language query can be deduced visually. ",(0,r.kt)("br",null),(0,r.kt)("br",null)," Specify query to have only one valid, contiguous temporal window response."),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"3"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Repeat this process N=length of video in minutes creating N diverse language queries")),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null})))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotation Stats:")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Total hours annotated:")," ","~","240 (x2; one for each vendor)")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Distribution over question types:")))),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(9003).Z,width:"2048",height:"696"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Scenario breakdown:"))),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(5182).Z,width:"1162",height:"435"})),(0,r.kt)("h3",{id:"moments"},"Moments"),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(33).Z,width:"1017",height:"230"})),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Objective:")," Localize high level events in a long video clip ","-","-\nmarking any instance of provided activity categories with a temporal\nwindow and the activity's name."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Motivation:"),' Learn to detect activities or "moments" and their\ntemporal extent in the video. In the context of episodic memory, the\nimplicit query from a user would be "When is the last time I did X?",\nand the response from the system would be to show the time window where\nactivity X was last seen.'),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotation Task:")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"#")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Sub-step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Example")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Review the Taxonomy")),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(6305).Z,width:"1092",height:"940"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Annotate the Video")),(0,r.kt)("td",{parentName:"tr",align:null},"1. Play the video until you observe an activity, then pause.",(0,r.kt)("br",null),"2. Draw a temporal window around the time span where the activity occurs. ",(0,r.kt)("br",null)," 3. Select from the dropdown list the name for that activity. ",(0,r.kt)("br",null)," 4. Play the video from the start of the previous activity, repeat steps 1-3."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(8767).Z,width:"2500",height:"1338"}))))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotation Stats:")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Total hours annotated:")," ","~","328 (x3 annotators)")),(0,r.kt)("h3",{id:"visual-object-queries"},"Visual Object Queries"),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(1905).Z,width:"874",height:"168"})),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Objective:")," Localize past instances of a given object that appears at\nleast twice in different parts of the video."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Motivation:"),' Support an object search application for video in which\na user asks at time T "where did I last see X?", and the system scans\nback in the video history starting at query frame T, finds the most\nrecent instance of X, and outlines it in a short track',(0,r.kt)("strong",{parentName:"p"},".")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotation Task:")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"#")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Sub-step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Example")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Identify ",(0,r.kt)("strong",{parentName:"em"},"query objects"))),(0,r.kt)("td",{parentName:"tr",align:null},"Preview the entire video.",(0,r.kt)("br",null),"Identify a set of N=3 interesting objects to label as queries (= objects that appear at least twice at distinct non-contiguous parts of the video clip)"),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Select a ",(0,r.kt)("strong",{parentName:"em"},"response track"))),(0,r.kt)("td",{parentName:"tr",align:null},"- Select one occurrence of the query object.",(0,r.kt)("br",null)," - Mark the query object with a bounding box over time, from the frame the object enters the field of view until it leaves the field of view, for that object occurrence."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(8490).Z,width:"480",height:"270"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"3"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Select a ",(0,r.kt)("strong",{parentName:"em"},"query frame"))),(0,r.kt)("td",{parentName:"tr",align:null},"- Select a frame that does not contain the query object, sometime far after that object occurrence, but before any subsequent occurrence of the object.",(0,r.kt)("br",null)," - Mark the time point with a large bounding box."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(7195).Z,width:"393",height:"259"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"4"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Select a ",(0,r.kt)("strong",{parentName:"em"},"visual crop"))),(0,r.kt)("td",{parentName:"tr",align:null},"- Find another occurrence of the same object elsewhere in the video (before or after the originally marked instance from Step 2). ",(0,r.kt)("br",null)," - Draw a bounding box in one frame around that object."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(3672).Z,width:"423",height:"280"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"5"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},(0,r.kt)("strong",{parentName:"em"},"Name the object")," using the ",(0,r.kt)("strong",{parentName:"em"},"free text")," box")),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"6"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Repeat Steps 1-5 three times for the same video clip and different objects")),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null})))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotation Stats:")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Total hours annotated:")," ","~","432")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("img",{src:a(1348).Z,width:"285",height:"284"}))),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Scenario breakdown:"))),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(9017).Z,width:"1117",height:"280"})),(0,r.kt)("h2",{id:"forecasting--hands--objects-fho"},"Forecasting + Hands & Objects (FHO)"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Objective:")," Recognize object state changes temporally and spatially\n(HO); predict these interactions spatially and temporally before they\nhappen (F)."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Motivation:")," Understanding and anticipating human-object\ninteractions."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Annotation Stats")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Scenario Distribution")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Labeled videos: 1,074 ",(0,r.kt)("br",null),(0,r.kt)("br",null)," Labeled clips: 1,672 ",(0,r.kt)("br",null),(0,r.kt)("br",null)," Labeled hours: 116.274 ",(0,r.kt)("br",null),(0,r.kt)("br",null)," Number of scenarios: 53 ",(0,r.kt)("br",null),(0,r.kt)("br",null)," Number of universities: 7 ",(0,r.kt)("br",null),(0,r.kt)("br",null)," Number of participants: 397 ",(0,r.kt)("br",null),(0,r.kt)("br",null)," Num interactions: 91,002 ",(0,r.kt)("br",null),(0,r.kt)("br",null)," Num rejected: 18,839 ",(0,r.kt)("br",null),(0,r.kt)("br",null)," Num with state change: 70,718 ",(0,r.kt)("img",{width:"500px"})),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(7573).Z,width:"2000",height:"800"}))))),(0,r.kt)("h3",{id:"stage-1---critical-frames"},"Stage 1 - Critical Frames"),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(4434).Z,width:"899",height:"235"})),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Objective:")," Annotator watches an egocentric video and marks\npre-condition (PRE), contact, point of no return (PNR), and\npost-condition (Post) frames."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotation Task:")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"#")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Sub-step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Example")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Read the narrated action to be labeled")),(0,r.kt)("td",{parentName:"tr",align:null},"1. Reject videos that do not contain hand-object interactions",(0,r.kt)("br",null)," 2. Reject videos that not contain the narrated action"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Example:"),' "C glides hand planer along the wood"',(0,r.kt)("br",null),(0,r.kt)("img",{src:a(4680).Z,width:"2500",height:"1344"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Select the verb corresponding to the narration")),(0,r.kt)("td",{parentName:"tr",align:null},"- If an appropriate verb is not available, select OTHER from the dropdown and type in the verb in the text box."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(5739).Z,width:"2048",height:"1083"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"3"),(0,r.kt)("td",{parentName:"tr",align:null},"*Select the ",(0,r.kt)("strong",{parentName:"td"},"state change type")," present in the video"),(0,r.kt)("td",{parentName:"tr",align:null},"- Select from one of 8 options from the dropdown"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(4597).Z,width:"2048",height:"1103"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"4"),(0,r.kt)("td",{parentName:"tr",align:null},"*Mark the ",(0,r.kt)("strong",{parentName:"td"},"CONTACT")," (only if present), ",(0,r.kt)("strong",{parentName:"td"},"PRE")," and ",(0,r.kt)("strong",{parentName:"td"},"POST")," frames."),(0,r.kt)("td",{parentName:"tr",align:null},"- Find the CONTACT frame ",(0,r.kt)("br",null)," - Pause the video ",(0,r.kt)("br",null),' - Select the "Contact Frame" from the dropdown ',(0,r.kt)("br",null)," - Repeat the same protocol for PRE and POST frames."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(7906).Z,width:"2500",height:"1342"}))))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"PRE, CONTACT, PNR, POST examples:")),(0,r.kt)("p",null,'a.  Example: "light blowtorch"'),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("img",{src:a(6112).Z,width:"2500",height:"525"}))),(0,r.kt)("p",null,'b.  Example: "put down wood" (object already in hands, no CONTACT frame)'),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("img",{src:a(7058).Z,width:"2500",height:"701"}))),(0,r.kt)("h3",{id:"stage-2---pre-condition"},"Stage 2 - Pre-condition"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Objective:")," Label bounding boxes and roles for hands (right/left) and\nobjects (objects of change and tools)."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotation Task:")),(0,r.kt)("ins",null,"Note"),": clips annotated from previous stage play in reverse from CONTACT to PRE frame:",(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"#")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Sub-step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Example")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Read the narrated action to be labeled")),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Example"),': "C straightens the cloth" ',(0,r.kt)("img",{src:a(5198).Z,width:"1904",height:"986"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Label the contact frame (first frame shown)")),(0,r.kt)("td",{parentName:"tr",align:null},"Label right and left hands (if visible), by correcting the existing bounding box or adding a new one."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(6463).Z,width:"1904",height:"976"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"Label the ",(0,r.kt)("strong",{parentName:"td"},"object(s) of change"),":",(0,r.kt)("br",null),(0,r.kt)("br",null)," - Draw the ",(0,r.kt)("ins",null,"bounding box")," ",(0,r.kt)("br",null)," - Mark the object as Object of change ",(0,r.kt)("br",null)," - Select the ",(0,r.kt)("ins",null,"name of the object")," from list provided ",(0,r.kt)("br",null)," - Select ",(0,r.kt)("ins",null,"instance ID")," (for multiple objects of the same type) ",(0,r.kt)("br",null)," - Repeat for each object of change"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(8773).Z,width:"1900",height:"968"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"Label the ",(0,r.kt)("strong",{parentName:"td"},"tool")," (if present): Draw the ",(0,r.kt)("ins",null,"bounding box")," ",(0,r.kt)("br",null)," - Mark the object as Tool ",(0,r.kt)("br",null)," - Select the ",(0,r.kt)("ins",null,"name of the tool")," from list provided ",(0,r.kt)("br",null)," - Select ",(0,r.kt)("ins",null,"instance ID")," (for multiple objects of the same type)"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(3122).Z,width:"1906",height:"974"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"3"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Label the remaining frames")),(0,r.kt)("td",{parentName:"tr",align:null},"Go to the next frame ",(0,r.kt)("br",null)," - Adjust the hand boxes ",(0,r.kt)("br",null)," - Adjust the object of change box ",(0,r.kt)("br",null)," - Adjust the tool box (if present) ",(0,r.kt)("br",null)," - Repeat for the remaining frames"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(3639).Z,width:"1898",height:"982"}))))),(0,r.kt)("h3",{id:"stage-3---post-condition"},"Stage 3 - Post-condition"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Objective:")," Label bounding boxes and roles for hands and objects\n(from Contact to Post frame)."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotation Task:"),"\n","[Note]",": clips annotated from Stage 1 play from CONTACT to POST\nframe:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"#")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Sub-step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Example")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Read the narrated action to be labeled")),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Example:"),' "C straightens the cloth"',(0,r.kt)("br",null)," ",(0,r.kt)("img",{src:a(5198).Z,width:"1904",height:"986"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Check the contact frame (first frame shown)")),(0,r.kt)("td",{parentName:"tr",align:null},"Contact frame will already be labeled with: ",(0,r.kt)("br",null)," - Left hand (if visible) ",(0,r.kt)("br",null)," - Right hand (if visible) ",(0,r.kt)("br",null)," - Active object ",(0,r.kt)("br",null)," - Tool (if applicable)"),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"3"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Label the remaining frames")),(0,r.kt)("td",{parentName:"tr",align:null},"- Go to the next frame ",(0,r.kt)("br",null)," - Adjust (or add) the hand boxes ",(0,r.kt)("br",null)," - Adjust the object of change box ",(0,r.kt)("br",null)," - Adjust the tool box (if present) ",(0,r.kt)("br",null)," - Repeat for the remaining frames"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(7991).Z,width:"1906",height:"982"}))))),(0,r.kt)("h2",{id:"audio-visual-diarization--social-avs"},"Audio-Visual Diarization & Social (AVS)"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Objective:")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"AV"),": Locate each speaker spatially and temporally, segment and transcribe the speech content (in a given video), assign each speaker an anonymous label.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"S:")," predict the following social cues:"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Who is talking to the camera wearer at each time segment")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Who is looking at the camera wearer at each time segment"))))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Motivation:")," Understand conversational behavior from the naturalistic\negocentric perspective; capture low level detection, segmentation and\ntracking attributes of people\\'s interactions in a scene, and more high\nlevel (intent/emotions driven) attributes that drive social and group\nconversations in the real world."),(0,r.kt)("h3",{id:"av-step-0-automated-face--head-detection"},"AV Step 0: Automated Face & Head Detection"),(0,r.kt)("p",null,"A face detection algorithm is run on the given input video to detect all\nthe faces. The resulting bounding boxes are going to be populated and\noverlaid on the input video."),(0,r.kt)("h3",{id:"av-step-1-face--head-tracks-correction"},"AV Step 1: Face & Head Tracks Correction"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Objective:")," Have a correct face bounding box around all the faces\nvisible in the video"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotation Task:")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"#")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Sub-step")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"For each frame in the video, identify all subjects in the frame and check to see if they have bounding boxes.")),(0,r.kt)("td",{parentName:"tr",align:null},"1. Subject has a bounding box (bbox): ",(0,r.kt)("br",null),"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","a. Bbox is PASSING  \u2192  Move onto the next subject in the frame. ",(0,r.kt)("br",null),"\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","b. Bbox is FAILING \u2192 Adjust/Re-draw the bbox (making sure the right face track is selected) ",(0,r.kt)("br",null)," 2. Subject doesn't have a bbox \u2192 Create a new bounding box and either assign it a new track or merge an existing face track. ",(0,r.kt)("br",null)," 3. Bbox does not capture a face \u2192 Delete bbox.")))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Examples:")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null}),(0,r.kt)("th",{parentName:"tr",align:null}))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"Passing")," Bbox"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(851).Z,width:"1276",height:"715"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"Failing")," Bbox"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(2831).Z,width:"1276",height:"711"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"Missing")," Bbox"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(3766).Z,width:"1276",height:"715"}))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Bbox to be ",(0,r.kt)("strong",{parentName:"td"},"deleted")),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("img",{src:a(2230).Z,width:"1276",height:"718"}))))),(0,r.kt)("h3",{id:"av-step-2-speaker-labeling-and-av-anchor-extraction"},"AV Step 2: Speaker Labeling and AV anchor extraction"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Objective:")," Assign each Face Track",(0,r.kt)("sup",{parentName:"p",id:"fnref-1"},(0,r.kt)("a",{parentName:"sup",href:"#fn-1",className:"footnote-ref"},"1"))," (from Step 1) a 'Person ID'\n(for each new subject which has an interaction with the camera-wearer or\nis present in the camera for 500+ frames)."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotation Task:")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"#")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Sub-step")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Identify the 'Next Track' and go to the first frame of this track.")),(0,r.kt)("td",{parentName:"tr",align:null},"1. Toggle On the 'Out-of-Frame' Track List ",(0,r.kt)("br",null)," 2. Select the next Track from the list ",(0,r.kt)("br",null)," 3. Click 'First Key Frame'")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Assign this Track a unique \u2018Person ID\u2019 (e.g. Person 1, Person 2, ect)")),(0,r.kt)("td",{parentName:"tr",align:null},"1. Use the drop down menu to select a Person ID ",(0,r.kt)("br",null)," 2. Each time this person appears in the video, assign their Track # to their designated Person ID")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"3"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Repeat steps 1-4 until all tracks have Person ID\u2019s assigned.")),(0,r.kt)("td",{parentName:"tr",align:null})))),(0,r.kt)("p",null,"|--------------------------|"),(0,r.kt)("h3",{id:"av-step-3-speech-segmentation-per-speaker"},"AV Step 3: Speech Segmentation (Per Speaker)"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Objective:")," Label voice activity for all subjects in the video."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotation:")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"#")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Sub-step")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Label voice activity for the ",(0,r.kt)("strong",{parentName:"em"},"camera wearer")," first and then for each Person ID.")),(0,r.kt)("td",{parentName:"tr",align:null},"1. Annotate the video using the time segment tool.\u202f",(0,r.kt)("br",null)," 2. Start an annotation when a person makes a sound (speech, coughing, sigh, any utterance).\u202f",(0,r.kt)("br",null)," 3. Stop an annotation when a person stops making sounds.\u202f",(0,r.kt)("br",null)," 4. Do not stop an annotation if a person starts making sound again within 1 second after they stopped.\u202f",(0,r.kt)("br",null)," 5. Label the segment according to the Person ID displayed in the bounding box around their head.\u202f",(0,r.kt)("br",null)," 6. Repeat the process for all sounds made by the people in the video.")))),(0,r.kt)("p",null,"|--------------------------|"),(0,r.kt)("h3",{id:"av-step-4-transcription"},"AV Step 4: Transcription"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Objective:")," Transcribe voice activity for all subjects in the video."),(0,r.kt)("h3",{id:"av-step-5-correcting-speech-transcriptions-wip"},"AV Step 5: Correcting Speech Transcriptions ","[","WIP","]"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Objective:")," Correcting Speech Transcription annotation from Step 4."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotation Task:")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"#")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Sub-step")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"0"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Pre-load the annotation tool.")),(0,r.kt)("td",{parentName:"tr",align:null},"The task begins with the pre-load of the following things: ",(0,r.kt)("br",null)," ","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","- Output of AV Step 3 (Speech Segmentation per Person ID) ",(0,r.kt)("br",null)," ","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","- Output of AV Step 4 (Human transcriptions) ",(0,r.kt)("br",null)," ","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0","- Automatic transcriptions from ASR algorithms.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"For each human transcription chunk, identify the corresponding person IDs with voice activity on.")),(0,r.kt)("td",{parentName:"tr",align:null},"For each person with the active voice activity: ",(0,r.kt)("br",null)," ","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0"," - Listen to the video",(0,r.kt)("br",null)," ","\xa0","\xa0","\xa0","\xa0","\xa0","\xa0"," - If the person\u2019s speech is = to the content in the transcription chunk, then copy this speech content from transcript into a new dialog box/tag that corresponds to the person.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Repeat Step 1 for the machine generated transcription chunks")),(0,r.kt)("td",{parentName:"tr",align:null})))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Examples:")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"< To Be Uploaded >")))),(0,r.kt)("h3",{id:"social-step-1-camera-wearer-attention"},"Social Step 1: Camera-Wearer Attention"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Objective"),": Annotate temporal segments in which a person is looking\nat the camera wearer."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotation Task:")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"#")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Sub-step")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Watch the video and find the time when someone is looking at the camera wearer")),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Annotate the time segment using the time segment tool:")),(0,r.kt)("td",{parentName:"tr",align:null},"1. Start an annotation when a person start to look at the camera wearer. ",(0,r.kt)("br",null)," 2. Stop an annotation when a person stops looking at the camera wearer. ",(0,r.kt)("br",null)," 3. Label the segment according to the Person ID displayed in the bounding box around their head. ",(0,r.kt)("br",null)," 4. Repeat the process for all cases in the video.")))),(0,r.kt)("p",null,"|--------------------------|"),(0,r.kt)("h3",{id:"social-step-2-speech-target-classification"},"Social Step 2: Speech Target Classification"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Objective"),": Given already annotated AV Voice Activity segmentation,\nthe annotator is going to annotate the particular speech segments in\nwhich the person is talking to the camera wearer."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Annotation Task:")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"#")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Step")),(0,r.kt)("th",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"th"},"Sub-step")))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Watch the video with AV voice segmentation results (start-end time, person ID)")),(0,r.kt)("td",{parentName:"tr",align:null})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("em",{parentName:"td"},"Annotate segments where someone is talking to the camera wearer. Repeat the process for all cases in the video.")),(0,r.kt)("td",{parentName:"tr",align:null},"1. Identify a segment in which someone is talking to the camera wearer. ",(0,r.kt)("br",null)," 2. Click the time segment, then you can see the Voice activity annotation information on the left side bar. ",(0,r.kt)("br",null),' 3. Click the drop down box below the "Target of Speech."',(0,r.kt)("br",null),(0,r.kt)("img",{src:a(5994).Z,width:"506",height:"162"})," ",(0,r.kt)("br",null),' 4. In the dropdown menu, select "Camera-Wearer" if the speech is only toward the camera wearer. ',(0,r.kt)("br",null),' 5. Choose "Camera-Wearer and others" if the speech segment is toward multiple people including the camera wearer (e.g., talking to multiple audience members). ',(0,r.kt)("br",null)," 6. Repeat the process for all relevant segments.")))))}d.isMDXComponent=!0},1348:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image1-cb95f7a6ce8d8cc57a8a376a4870bd4b.png"},33:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image10-a15f59ac4828168c130fc993a5dba16d.png"},3672:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image11-95c1f2627bc7363d7ed098704364aecf.png"},2831:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image12-738add5129b302a3b521db6fdfec78d5.png"},4307:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image13-1dbe07a190ef030848b79fe86ab52b9f.png"},6305:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image15-ee6c6c67f35813d37548298c6b540674.png"},4534:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image16-9e55f40cbd0b57fc516ff43a2d52324d.png"},467:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image18-fec68ada7e115e2b5ddad1b313a3d283.png"},7573:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image19-00e19a2a62a562005bd744ae7965764a.png"},9003:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image2-73d744aacb706a6c6be825d0f333a008.png"},2230:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image20-681ac05983a459c200bdef80495a9985.png"},9017:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image21-997834909d39c1d2ea8729613e8cf735.png"},4597:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image22-22d30630133c337e26b3fee2612bbe32.png"},3766:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image23-66ce8bdd734d6ef2b7bce5867fd1577d.png"},5198:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image25-77c8e6deeeb932a4fb79d1a6dfcd7a11.png"},3122:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image27-989a9cfc53d58fd644b80e92d393b3fe.png"},5739:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image28-3c520df2678da7c9cc0c1061b3aef497.png"},6463:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image29-0ae02bb25421fa38252fc8eeaed8aab3.png"},5182:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image3-db38883c4c8f547284a969c5d3197adf.png"},8490:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image30-d06bb98836a451e9e6da02c38ce456b0.gif"},2611:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image32-8b0a8c1616304b84176927470f098846.png"},1905:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image33-930945bc750e68e528ae8219b42fce82.png"},1637:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image35-91b5eb2d1a6d9ba45ec1f3bba497a145.png"},8773:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image36-7889dc79cf3aeae56e00ae659a9656b6.png"},7991:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image37-a8be03c374b2562ab33199b2810d3666.png"},6112:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image38-80d977484213083a627b3a0d642d91b4.jpg"},7058:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image39-097e7ec2f7bd174679c70429ec7d2aee.jpg"},5228:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image4-8e3ced36f3852c4e3022c81523a33885.png"},3639:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image41-37cfe3266c456c9accac870986c15d68.png"},8767:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image45-4154b0e8b1843819b876742ac2343582.png"},4680:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image46-dc4bb0c499a85443479df0b6efa01372.png"},7906:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image47-dd612c0b66e10b5c869fc6cef6539089.png"},4434:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image5-1c194e80ab2a55776812a699b248b5ee.png"},851:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image6-f8efcbef2b533452d325ccc01b140cbb.png"},2509:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image7-e0a2bd6a7b876b3194e9a7bbf876256c.png"},5994:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image8-eb96212c0c12409b9412cb2e466c393b.png"},7195:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/image9-037f03bb67828cdc6e1e450fef4891c5.png"}}]);