"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[551],{3905:(e,a,t)=>{t.d(a,{Zo:()=>p,kt:()=>m});var n=t(7294);function i(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function r(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function l(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?r(Object(t),!0).forEach((function(a){i(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function o(e,a){if(null==e)return{};var t,n,i=function(e,a){if(null==e)return{};var t,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||(i[t]=e[t]);return i}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=n.createContext({}),c=function(e){var a=n.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):l(l({},a),e)),t},p=function(e){var a=c(e.components);return n.createElement(s.Provider,{value:a},e.children)},h="mdxType",d={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},u=n.forwardRef((function(e,a){var t=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),h=c(t),u=i,m=h["".concat(s,".").concat(u)]||h[u]||d[u]||r;return t?n.createElement(m,l(l({ref:a},p),{},{components:t})):n.createElement(m,l({ref:a},p))}));function m(e,a){var t=arguments,i=a&&a.mdxType;if("string"==typeof e||i){var r=t.length,l=new Array(r);l[0]=u;var o={};for(var s in a)hasOwnProperty.call(a,s)&&(o[s]=a[s]);o.originalType=e,o[h]="string"==typeof e?e:i,l[1]=o;for(var c=2;c<r;c++)l[c]=t[c];return n.createElement.apply(null,l)}return n.createElement.apply(null,t)}u.displayName="MDXCreateElement"},8546:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>s,contentTitle:()=>l,default:()=>d,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var n=t(7462),i=(t(7294),t(3905));const r={sidebar_position:10},l="Ego4D Challenge 2023",o={unversionedId:"challenge",id:"challenge",title:"Ego4D Challenge 2023",description:"Overview",source:"@site/ego4d/challenge.md",sourceDirName:".",slug:"/challenge",permalink:"/docs/ego4d/challenge",draft:!1,tags:[],version:"current",sidebarPosition:10,frontMatter:{sidebar_position:10},sidebar:"tutorialSidebar",previous:{title:"Model Zoo",permalink:"/docs/ego4d/model-zoo"},next:{title:"Updates",permalink:"/docs/ego4d/updates"}},s={},c=[{value:"Overview",id:"overview",level:2},{value:"Episodic memory:",id:"episodic-memory",level:3},{value:"Hands and Objects:",id:"hands-and-objects",level:3},{value:"Audio-Visual Diarization:",id:"audio-visual-diarization",level:3},{value:"Social Understanding:",id:"social-understanding",level:3},{value:"Forecasting:",id:"forecasting",level:3},{value:"Dataset",id:"dataset",level:2},{value:"Participation Guidelines",id:"participation-guidelines",level:2},{value:"Dates",id:"dates",level:2},{value:"Competition Rules and Prize Information",id:"competition-rules-and-prize-information",level:2},{value:"Challenge Reports",id:"challenge-reports",level:2},{value:"Acknowledgements",id:"acknowledgements",level:2},{value:"Organizers",id:"organizers",level:3},{value:"Past Challenges / Winners",id:"past-challenges--winners",level:2}],p={toc:c},h="wrapper";function d(e){let{components:a,...t}=e;return(0,i.kt)(h,(0,n.Z)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"ego4d-challenge-2023"},"Ego4D Challenge 2023"),(0,i.kt)("h2",{id:"overview"},"Overview"),(0,i.kt)("p",null,"In CVPR 2023, we will host ",(0,i.kt)("strong",{parentName:"p"},"14")," challenges including 2 new challenges (EgoTracks & PACO Zero-Shot), representing each of Ego4D\u2019s five benchmarks. These are:"),(0,i.kt)("h3",{id:"episodic-memory"},(0,i.kt)("a",{parentName:"h3",href:"/docs/ego4d/benchmarks/episodic-memory"},"Episodic memory"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1843/overview"},"Visual queries with 2D localization (VQ2D)")," and ",(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1646/overview"},"Visual Queries 3D localization (VQ3D)"),": Given an egocentric video clip and an image crop depicting the query object, return the most recent occurrence of the object in the input video, in terms of contiguous bounding boxes (2D + temporal localization) or the 3D displacement vector from the camera to the object in the environment. ",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Quickstart: ",(0,i.kt)("a",{parentName:"li",href:"https://colab.research.google.com/drive/1vtVOQzLarBCspQjH5RtHZ8qzH0VZxrmZ?usp=sharing"},(0,i.kt)("img",{parentName:"a",src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open in Colab"}))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1629/overview"},"Natural language queries (NLQ)"),": Given a video clip and a query expressed in natural language, localize the temporal window within all the video history where the answer to the question is evident.",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Quickstart: ",(0,i.kt)("a",{parentName:"li",href:"https://colab.research.google.com/drive/1S1LTplak-Fno3lMumCLoIfzYsx_TfNes?usp=sharing"},(0,i.kt)("img",{parentName:"a",src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open in Colab"}))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1626/overview"},"Moments queries (MQ)"),": Given an egocentric video and an activity name (e.g., a \u201cmoment\u201d), localize all instances of that activity in the past video"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1969/overview"},"EgoTracks"),": Given an egocentric video and a visual template of an object, localize the bounding box containing the object in each frame of the video along with a confidence score representing the presence of the object. ",(0,i.kt)("strong",{parentName:"li"},"[NEW for 2023]")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1970/overview"},"PACO Zero-Shot:")," Retrieve the bounding box of a specific object instance from a dataset, based on a textual query describing the instance. Query is composed using object and part attributes describing the object of interest. ",(0,i.kt)("strong",{parentName:"li"},"[NEW for 2023]"))),(0,i.kt)("h3",{id:"hands-and-objects"},(0,i.kt)("a",{parentName:"h3",href:"/docs/ego4d/benchmarks/hands-and-objects"},"Hands and Objects"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1622/overview"},"Temporal localization"),": Given an egocentric video clip, localize temporally the key frames that indicate an object state change."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1627/overview"},"Object state change classification"),": Given an egocentric video clip, indicate the presence or absence of an object state change.")),(0,i.kt)("h3",{id:"audio-visual-diarization"},(0,i.kt)("a",{parentName:"h3",href:"/docs/ego4d/benchmarks/av-diarization"},"Audio-Visual Diarization"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1640/overview"},"Audio-visual speaker diarization"),": Given an egocentric video clip, identify which person spoke and when they spoke."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1637/overview"},"Speech transcription"),": Given an egocentric video clip, transcribe the speech of each person.")),(0,i.kt)("h3",{id:"social-understanding"},(0,i.kt)("a",{parentName:"h3",href:"/docs/ego4d/benchmarks/social"},"Social Understanding"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1625/overview"},"Talking to me"),": Given an egocentric video clip, identify whether someone in the scene is talking to the camera wearer."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1624/overview"},"Looking at me"),": Given an egocentric video clip, identify whether someone in the scene is looking at the camera wearer.")),(0,i.kt)("h3",{id:"forecasting"},(0,i.kt)("a",{parentName:"h3",href:"/docs/ego4d/benchmarks/forecasting"},"Forecasting"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1623/overview"},"Short-term hand object prediction"),": Given a video clip, predict the next active objects, and, for each of them, predict the next action, and the time to contact.",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Quickstart: ",(0,i.kt)("a",{parentName:"li",href:"https://colab.research.google.com/drive/1Ok_6F1O6K8kX1S4sEnU62HoOBw_CPngR?usp=sharing"},(0,i.kt)("img",{parentName:"a",src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open in Colab"}))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1598/overview"},"Long-term activity prediction"),": Given a video clip, the goal is to predict what sequence of activities will happen in the future. For example, after kneading dough, list the actions that the baker will do next. ")),(0,i.kt)("p",null,"Other Ego4D challenges which are not part of CVPR 2023 workshop remain open on EvalAI website for submissions but are not eligible for prizes."),(0,i.kt)("h2",{id:"dataset"},"Dataset"),(0,i.kt)("p",null,"Ego4D challenge participants will use Ego4D\u2019s annotated data set of more than 3,670 hours of video data, capturing the daily-life scenarios of more than 900 unique individuals from nine different countries around the world. Unique train, validation and unannotated test sets are available to download per challenge at ",(0,i.kt)("a",{parentName:"p",href:"https://ego4d-data.org/docs/"},"https://ego4d-data.org/docs/"),"."),(0,i.kt)("p",null,"This year's challenge will use Ego4D v2.0 which contains ~2X train and val annotations for Forecasting, Hands & Objects and NLQ, a number of corrections and usability enhancements, and two new related dataset enhancements (PACO & EgoTracks). The test set remains the same as previous versions of the challenge. More details can be found ",(0,i.kt)("a",{parentName:"p",href:"https://ego4d-data.org/docs/updates/"},"here"),". We have also updated the baselines for NLQ, MQ, VQ2D and forecasting tasks leveraging more training data available in Ego4D v2.0 release. "),(0,i.kt)("h2",{id:"participation-guidelines"},"Participation Guidelines"),(0,i.kt)("p",null,"Participate in the contest by registering on the ",(0,i.kt)("a",{parentName:"p",href:"https://eval.ai/"},"EvalAI challenge page")," and create a team. All participants must register as a part of a \u201cparticipating team\u201d on EvalAI to ensure the submission limits are honored. Participants will upload their predictions in the format specified for the specific challenge, and will be evaluated on AWS instances by comparing to ground truth predictions. Instructions for training, local evaluation, and online submission are provided at EvalAI. Please refer to the individual EvalAI pages for each challenge for submission guidelines, task specifications, and evaluation criteria."),(0,i.kt)("h2",{id:"dates"},"Dates"),(0,i.kt)("p",null,"The challenge will launch on March 1, 2023 with the leaderboard closing on May 19, 2023. Winners will be announced at the ",(0,i.kt)("a",{parentName:"p",href:"https://sites.google.com/view/ego4d-epic-cvpr2023-workshop/"},"Joint International 3rd Ego4D and 11th EPIC Workshop")," at CVPR 2023. Top performing teams may be invited to speak at the workshop."),(0,i.kt)("h2",{id:"competition-rules-and-prize-information"},"Competition Rules and Prize Information"),(0,i.kt)("p",null,"Competition rules can be found ",(0,i.kt)("a",{parentName:"p",href:"pathname:///tc.pdf"},"here"),". Additionally, we are thrilled that FAIR is able to offer the following prize thresholds per challenges:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"First place: $1500"),(0,i.kt)("li",{parentName:"ul"},"Second place: $1000"),(0,i.kt)("li",{parentName:"ul"},"Third place: $500")),(0,i.kt)("h2",{id:"challenge-reports"},"Challenge Reports"),(0,i.kt)("p",null,"In addition to the submission on EvalAI, participants must submit a report describing their method to the workshop CMT (link TBD). In addition to your method and results, please remember to include examples of positive and negative results (limitations) of your model. These validation reports will be evaluated by challenge hosts from the Ego4D consortium before winner determination can be made. Similarly, challenge validation reports, research code from winning entries, and names of participants from the winning teams for all successful submissions must be shared publicly with the research community.   "),(0,i.kt)("h2",{id:"acknowledgements"},"Acknowledgements"),(0,i.kt)("p",null,"The Ego4D challenge would not have been possible without the infrastructure and support of the ",(0,i.kt)("a",{parentName:"p",href:"https://eval.ai/team"},"EvalAI team"),". Thank you!"),(0,i.kt)("h3",{id:"organizers"},"Organizers"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Suyog Jain")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Rohit Girdhar")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Andrew Westbury")),(0,i.kt)("li",{parentName:"ul"},"Santhosh Kumar Ramakrishnan"),(0,i.kt)("li",{parentName:"ul"},"Chen Zhao"),(0,i.kt)("li",{parentName:"ul"},"Merey Ramazanova"),(0,i.kt)("li",{parentName:"ul"},"Satwik Kottur"),(0,i.kt)("li",{parentName:"ul"},"Mengmeng Xu"),(0,i.kt)("li",{parentName:"ul"},"Vincent Cartillier "),(0,i.kt)("li",{parentName:"ul"},"Yifei Huang"),(0,i.kt)("li",{parentName:"ul"},"Qichen Fu"),(0,i.kt)("li",{parentName:"ul"},"Siddhant Bansal"),(0,i.kt)("li",{parentName:"ul"},"Hao Jiang"),(0,i.kt)("li",{parentName:"ul"},"Vamsi Ithapu"),(0,i.kt)("li",{parentName:"ul"},"Jachym Kolar"),(0,i.kt)("li",{parentName:"ul"},"Christian Fuegen"),(0,i.kt)("li",{parentName:"ul"},"Leda Sari"),(0,i.kt)("li",{parentName:"ul"},"Eric Zhongcong Xu "),(0,i.kt)("li",{parentName:"ul"},"Zachary Chavis "),(0,i.kt)("li",{parentName:"ul"},"Wenqi Jia"),(0,i.kt)("li",{parentName:"ul"},"Miao Liu"),(0,i.kt)("li",{parentName:"ul"},"Antonino Furnari"),(0,i.kt)("li",{parentName:"ul"},"Francesco Ragusa "),(0,i.kt)("li",{parentName:"ul"},"Tushar Nagarajan"),(0,i.kt)("li",{parentName:"ul"},"Dima Damen"),(0,i.kt)("li",{parentName:"ul"},"Giovanni Maria Farinella"),(0,i.kt)("li",{parentName:"ul"},"Michael Wray"),(0,i.kt)("li",{parentName:"ul"},"Hao Tang"),(0,i.kt)("li",{parentName:"ul"},"Kevin Liang"),(0,i.kt)("li",{parentName:"ul"},"Weiyao Wang"),(0,i.kt)("li",{parentName:"ul"},"Vladan Petrovic"),(0,i.kt)("li",{parentName:"ul"},"Anmol Kalia"),(0,i.kt)("li",{parentName:"ul"},"Vignesh Ramanathan"),(0,i.kt)("li",{parentName:"ul"},"Dhruv Mahajan"),(0,i.kt)("li",{parentName:"ul"},"Gene Byrne"),(0,i.kt)("li",{parentName:"ul"},"Matt Feiszli"),(0,i.kt)("li",{parentName:"ul"},"Kristen Grauman")),(0,i.kt)("h2",{id:"past-challenges--winners"},"Past Challenges / Winners"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("a",{parentName:"strong",href:"https://ego4d-data.org/workshops/eccv22/"},"ECCV Workshop 2022"))," (Oct 24, 2022)"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("a",{parentName:"strong",href:"https://ego4d-data.org/workshops/cvpr22/"},"CVPR Workshop 2022"))," (June 19, 2022)"))}d.isMDXComponent=!0}}]);