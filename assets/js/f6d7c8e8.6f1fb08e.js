"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[88],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return d}});var r=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var c=r.createContext({}),l=function(e){var t=r.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},u=function(e){var t=l(e.components);return r.createElement(c.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},p=r.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,c=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=l(n),d=i,h=p["".concat(c,".").concat(d)]||p[d]||m[d]||o;return n?r.createElement(h,a(a({ref:t},u),{},{components:n})):r.createElement(h,a({ref:t},u))}));function d(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,a=new Array(o);a[0]=p;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:i,a[1]=s;for(var l=2;l<o;l++)a[l]=n[l];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}p.displayName="MDXCreateElement"},1858:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return s},contentTitle:function(){return c},metadata:function(){return l},toc:function(){return u},default:function(){return p}});var r=n(7462),i=n(3366),o=(n(7294),n(3905)),a=["components"],s={sidebar_position:2},c="Episodic Memory",l={unversionedId:"benchmarks/episodic-memory",id:"benchmarks/episodic-memory",isDocsHomePage:!1,title:"Episodic Memory",description:"Benchmark Repo//github.com/EGO4D/episodic-memory",source:"@site/docs/benchmarks/episodic-memory.md",sourceDirName:"benchmarks",slug:"/benchmarks/episodic-memory",permalink:"/docs/benchmarks/episodic-memory",editUrl:"https://https://ego4d-data.org/docs/benchmarks/episodic-memory.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Benchmarks Overview",permalink:"/docs/benchmarks/overview"},next:{title:"Forecasting",permalink:"/docs/benchmarks/forecasting"}},u=[{value:"Motivation",id:"motivation",children:[],level:2},{value:"Task Definition",id:"task-definition",children:[],level:2},{value:"Annotations",id:"annotations",children:[],level:2}],m={toc:u};function p(e){var t=e.components,n=(0,i.Z)(e,a);return(0,o.kt)("wrapper",(0,r.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"episodic-memory"},"Episodic Memory"),(0,o.kt)("p",null,"Benchmark Repo: ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/EGO4D/episodic-memory"},"https://github.com/EGO4D/episodic-memory")),(0,o.kt)("h2",{id:"motivation"},"Motivation"),(0,o.kt)("p",null,"Egocentric video from a wearable camera records the who/what/when/where of an individual\u2019s daily\nlife experience. This makes it ideal for what Tulving called episodic memory: specific first-person experiences (\u201cwhat did I eat and who did I sit by on my first flight to France?\u201d), to be distinguished from semantic memory (\u201cwhat\u2019s the capital of France?\u201d). An augmented reality assistant that processes the egocentric video stream could give us super-human memory if it could appropriately index our visual experience and answer queries."),(0,o.kt)("h2",{id:"task-definition"},"Task Definition"),(0,o.kt)("p",null,"Given an egocentric video and a query, the Ego4D Episodic Memory task requires localizing where the answer can be seen within the user\u2019s past video. We consider three query types: "),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Natural language queries (NLQ), in which the query is expressed in text (e.g., \u201cWhat\ndid I put in the drawer?\u201d), and the output response is the temporal window where the answer is visible or deducible."),(0,o.kt)("li",{parentName:"ol"},"Visual queries (VQ), in which the query is a static image of an object, and the output response localizes the object the last time it was seen in the video, both temporally and spatially. The spatial response is a 2D bounding box on the object, and optionally a 3D displacement vector from the current camera position to the object\u2019s 3D bounding box. VQ captures how a user might teach the system an object with an image example, then later ask for its location (\u201cWhere is this ","[picture of my keys]","?\u201d). "),(0,o.kt)("li",{parentName:"ol"},"Moments queries (MQ), in which the query is the name of a high-level activity or \u201cmoment\u201d, and the response consists of all temporal windows where the activity occurs (e.g., \u201cWhen did I read to my children?\u201d). ")),(0,o.kt)("h2",{id:"annotations"},"Annotations"),(0,o.kt)("p",null,"For language queries, we devised a set of 13 template questions meant to span things a user might ask to augment their memory, such as \u201cwhat is the state of object X?\u201d, e.g., \u201cdid I leave the window open?\u201d. Annotators express the queries in free-form natural language, and also provide the slot filling (e.g., X = window). "),(0,o.kt)("p",null,"For moments, we established a taxonomy of 110 activities in a data-driven, semi-automatic manner by mining the narration summaries. Moments capture high-level activities in the camera wearer\u2019s day, e.g., setting the table is a moment, whereas pick up is an action in our ",(0,o.kt)("a",{parentName:"p",href:"/docs/benchmarks/forecasting"},"Forecasting benchmark"),"."),(0,o.kt)("p",null,"For NLQ and VQ, we ask annotators to generate language/visual queries and couple them with the \u201cresponsetrack\u201d in the video. For MQ, we provide the taxonomy of labels and ask annotators to label clips with each and every temporal segment containing a moment instance. In total, we have \u223c74K total queries spanning 800 hours of video."),(0,o.kt)("h1",{id:"visual-queries"},"Visual Queries"),(0,o.kt)("h1",{id:"natural-language-queries"},"Natural Language Queries"),(0,o.kt)("h1",{id:"moments"},"Moments"))}p.isMDXComponent=!0}}]);