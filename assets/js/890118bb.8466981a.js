"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[359],{3905:function(e,a,t){t.d(a,{Zo:function(){return p},kt:function(){return d}});var n=t(7294);function i(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function l(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function r(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?l(Object(t),!0).forEach((function(a){i(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):l(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function o(e,a){if(null==e)return{};var t,n,i=function(e,a){if(null==e)return{};var t,n,i={},l=Object.keys(e);for(n=0;n<l.length;n++)t=l[n],a.indexOf(t)>=0||(i[t]=e[t]);return i}(e,a);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)t=l[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=n.createContext({}),c=function(e){var a=n.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):r(r({},a),e)),t},p=function(e){var a=c(e.components);return n.createElement(s.Provider,{value:a},e.children)},h={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},u=n.forwardRef((function(e,a){var t=e.components,i=e.mdxType,l=e.originalType,s=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),u=c(t),d=i,m=u["".concat(s,".").concat(d)]||u[d]||h[d]||l;return t?n.createElement(m,r(r({ref:a},p),{},{components:t})):n.createElement(m,r({ref:a},p))}));function d(e,a){var t=arguments,i=a&&a.mdxType;if("string"==typeof e||i){var l=t.length,r=new Array(l);r[0]=u;var o={};for(var s in a)hasOwnProperty.call(a,s)&&(o[s]=a[s]);o.originalType=e,o.mdxType="string"==typeof e?e:i,r[1]=o;for(var c=2;c<l;c++)r[c]=t[c];return n.createElement.apply(null,r)}return n.createElement.apply(null,t)}u.displayName="MDXCreateElement"},9138:function(e,a,t){t.r(a),t.d(a,{frontMatter:function(){return o},contentTitle:function(){return s},metadata:function(){return c},toc:function(){return p},default:function(){return u}});var n=t(7462),i=t(3366),l=(t(7294),t(3905)),r=["components"],o={sidebar_position:9},s="Ego4D Challenge 2022",c={unversionedId:"challenge",id:"challenge",isDocsHomePage:!1,title:"Ego4D Challenge 2022",description:"We'll be holding an office hours zoom to discuss the challenge, future of the dataset, any issues or really anything at all - 10AM EST on 9/14: Forum Link",source:"@site/docs/challenge.md",sourceDirName:".",slug:"/challenge",permalink:"/docs/challenge",tags:[],version:"current",sidebarPosition:9,frontMatter:{sidebar_position:9},sidebar:"tutorialSidebar",previous:{title:"Privacy and Ethics",permalink:"/docs/privacy"},next:{title:"FAQ",permalink:"/docs/FAQ"}},p=[{value:"Overview",id:"overview",children:[{value:"Episodic memory:",id:"episodic-memory",children:[],level:3},{value:"Hands and Objects:",id:"hands-and-objects",children:[],level:3},{value:"Audio-Visual Diarization & Social:",id:"audio-visual-diarization--social",children:[],level:3},{value:"Forecasting:",id:"forecasting",children:[],level:3}],level:2},{value:"Dataset",id:"dataset",children:[],level:2},{value:"Participation Guidelines",id:"participation-guidelines",children:[],level:2},{value:"Dates",id:"dates",children:[],level:2},{value:"Competition Rules and Prize Information",id:"competition-rules-and-prize-information",children:[],level:2},{value:"Challenge Reports",id:"challenge-reports",children:[],level:2},{value:"Acknowledgements",id:"acknowledgements",children:[{value:"Organizers",id:"organizers",children:[],level:3}],level:2}],h={toc:p};function u(e){var a=e.components,t=(0,i.Z)(e,r);return(0,l.kt)("wrapper",(0,n.Z)({},h,t,{components:a,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"ego4d-challenge-2022"},"Ego4D Challenge 2022"),(0,l.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,l.kt)("div",{parentName:"div",className:"admonition-heading"},(0,l.kt)("h5",{parentName:"div"},(0,l.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,l.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,l.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"Office Hours Zoom 9/14/22 10AM EST")),(0,l.kt)("div",{parentName:"div",className:"admonition-content"},(0,l.kt)("p",{parentName:"div"},"We'll be holding an office hours zoom to discuss the challenge, future of the dataset, any issues or really anything at all - 10AM EST on 9/14: ",(0,l.kt)("a",{parentName:"p",href:"https://discuss.ego4d-data.org/t/9-14-zoom-office-hours/106"},"Forum Link")))),(0,l.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,l.kt)("div",{parentName:"div",className:"admonition-heading"},(0,l.kt)("h5",{parentName:"div"},(0,l.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,l.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,l.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"Challenge Ends 9/18/22")),(0,l.kt)("div",{parentName:"div",className:"admonition-content"},(0,l.kt)("p",{parentName:"div"},"Please note, the challenge end date has been changed to September 18th to allow for more time for report submission and evaluation."),(0,l.kt)("p",{parentName:"div"},"Also note for VQ specifically, you'll need to follow instructions ",(0,l.kt)("a",{parentName:"p",href:"https://eval.ai/web/challenges/challenge-page/1843/overview"},"here")," to download updated annotations for the challenge."))),(0,l.kt)("h2",{id:"overview"},"Overview"),(0,l.kt)("p",null,"In 2022, we will host 16 challenges, representing each of Ego4D\u2019s five benchmarks. These are: "),(0,l.kt)("h3",{id:"episodic-memory"},(0,l.kt)("a",{parentName:"h3",href:"/docs/benchmarks/episodic-memory"},"Episodic memory"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1843/overview"},"Visual queries with 2D localization")," and ",(0,l.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1646/overview"},"VQ 3D localization"),": Given an egocentric video clip and an image crop depicting the query object, return the last time the object was seen in the input video, in terms of the tracked bounding box (2D + temporal localization) or the 3D displacement vector from the camera to the object in the environment. ",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://colab.research.google.com/drive/1vtVOQzLarBCspQjH5RtHZ8qzH0VZxrmZ?usp=sharing"},(0,l.kt)("img",{parentName:"a",src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open in Colab"}))," Quickstart"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1629/overview"},"Natural language queries"),": Given a video clip and a query expressed in natural language, localize the temporal window within all the video history where the answer to the question is evident.   ",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://colab.research.google.com/drive/1S1LTplak-Fno3lMumCLoIfzYsx_TfNes?usp=sharing"},(0,l.kt)("img",{parentName:"a",src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open in Colab"}))," Quickstart"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1626/overview"},"Moments queries"),": Given an egocentric video and an activity name (e.g., a \u201cmoment\u201d), localize all instances of that activity in the past video ")),(0,l.kt)("h3",{id:"hands-and-objects"},(0,l.kt)("a",{parentName:"h3",href:"/docs/benchmarks/hands-and-objects"},"Hands and Objects"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1622/overview"},"Temporal localization"),": Given an egocentric video clip, localize temporally the key frames that indicate an object state change."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1627/overview"},"Object state change classification"),": Given an egocentric video clip, indicate the presence or absence of an object state change."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1632/overview"},"State change object detection"),": Given an egocentric video clip, identify the objects whose states are changing and outline them with bounding boxes. ")),(0,l.kt)("h3",{id:"audio-visual-diarization--social"},(0,l.kt)("a",{parentName:"h3",href:"/docs/benchmarks/av-diarization"},"Audio-Visual Diarization & Social"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1633/overview"},"Audio-visual localization"),": Given an egocentric video clip, localize the speakers in the visual field of view. "),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1640/overview"},"Audio-visual speaker diarization"),": Given an egocentric video clip, identify which person spoke and when they spoke."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1641/overview"},"Audio-only Diarization Challenge"),": Given an egocentric video clip, identify which person spoke and when they spoke based on audio alone."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1637/overview"},"Speech transcription"),": Given an egocentric video clip, transcribe the speech of each person."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1625/overview"},"Talking to me"),": Given an egocentric video clip, identify whether someone in the scene is talking to the camera wearer."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1624/overview"},"Looking at me"),": Given an egocentric video clip, identify whether someone in the scene is looking at the camera wearer.")),(0,l.kt)("h3",{id:"forecasting"},(0,l.kt)("a",{parentName:"h3",href:"/docs/benchmarks/forecasting"},"Forecasting"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1630/overview"},"Hand forecasting"),": Given a short preceding video clip, predict where the hand will be visible in the future, in terms of a bounding box center in keyframes.  "),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1623/overview"},"Short-term hand object prediction"),": Given a video clip, predict the next active objects, the next action, and the time to contact. ",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://colab.research.google.com/drive/1Ok_6F1O6K8kX1S4sEnU62HoOBw_CPngR?usp=sharing"},(0,l.kt)("img",{parentName:"a",src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open in Colab"}))," Quickstart"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://eval.ai/web/challenges/challenge-page/1598/overview"},"Long-term activity prediction"),": Given a video clip, the goal is to predict what sequence of activities will happen in the future? For example, after kneading dough, what will the baker do next?  ")),(0,l.kt)("h2",{id:"dataset"},"Dataset"),(0,l.kt)("p",null,"Ego4D challenge participants will use Ego4D\u2019s annotated data set of more than 3,670 hours of video data, capturing the daily-life scenarios of more than 900 unique individuals from nine different countries around the world. Unique train, validation and unannotated test sets are available to download per challenge at ",(0,l.kt)("a",{parentName:"p",href:"https://ego4d-data.org/docs/"},"https://ego4d-data.org/docs/"),". "),(0,l.kt)("h2",{id:"participation-guidelines"},"Participation Guidelines"),(0,l.kt)("p",null,"Participate in the contest by registering on the ",(0,l.kt)("a",{parentName:"p",href:"https://eval.ai/"},"EvalAI challenge page")," and create a team. All participants must register as a part of a \u201cparticipating team\u201d on EvalAI to ensure the submission limits are honored. Participants will upload their predictions in the format specified for the specific challenge, and will be evaluated on AWS instance by comparing to ground truth predictions. Instructions for training, local evaluation, and online submission are provided at EvalAI. Please refer to the individual EvalAI pages for each challenge for submission guidelines, task specifications, and evaluation criteria."),(0,l.kt)("h2",{id:"dates"},"Dates"),(0,l.kt)("p",null,"Top performing teams will be invited to speak at our accepted CVPR and ECCV workshops in June and October.  For the current ECCV challenge set, submissions will be due Sept 18th, 2022. "),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"October 1, 2022 (",(0,l.kt)("a",{parentName:"strong",href:"https://ego4d-data.org/Workshop/ECCV22/"},"ECCV Workshop"),", phase 2)")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Visual Queries"),(0,l.kt)("li",{parentName:"ul"},"Natural Language Queries"),(0,l.kt)("li",{parentName:"ul"},"PNR Temporal Localization"),(0,l.kt)("li",{parentName:"ul"},"Object State Change Classification"),(0,l.kt)("li",{parentName:"ul"},"Short-term Object Interaction Anticipation"),(0,l.kt)("li",{parentName:"ul"},"Long-term Action Anticipation"),(0,l.kt)("li",{parentName:"ul"},"Moments Queries "),(0,l.kt)("li",{parentName:"ul"},"3D Localization "),(0,l.kt)("li",{parentName:"ul"},"State Change Object Detection "),(0,l.kt)("li",{parentName:"ul"},"Localization and Tracking "),(0,l.kt)("li",{parentName:"ul"},"Diarization (Audio)"),(0,l.kt)("li",{parentName:"ul"},"Diarization (Audio+Video)"),(0,l.kt)("li",{parentName:"ul"},"Transcription "),(0,l.kt)("li",{parentName:"ul"},"Looking at Me"),(0,l.kt)("li",{parentName:"ul"},"Talking to Me"),(0,l.kt)("li",{parentName:"ul"},"Future Hand Prediction")),(0,l.kt)("h2",{id:"competition-rules-and-prize-information"},"Competition Rules and Prize Information"),(0,l.kt)("p",null,"Competition rules can be found ",(0,l.kt)("a",{parentName:"p",href:"https://ego4d-interactive-fig1.s3.eu-west-2.amazonaws.com/tc.pdf"},"here"),". Additionally, we are thrilled that FAIR is able to offer the following prize thresholds per challenges: "),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"First place: $3,000 "),(0,l.kt)("li",{parentName:"ul"},"Second place: $2,000"),(0,l.kt)("li",{parentName:"ul"},"Third place: $1,000")),(0,l.kt)("h2",{id:"challenge-reports"},"Challenge Reports"),(0,l.kt)("p",null,"In addition to the submission on EvalAI, participants must submit a report describing their method to the workshop CMT (link TBD). In addition to your method and results, please remember to include examples of positive and negative results (limitations) of your model."),(0,l.kt)("h2",{id:"acknowledgements"},"Acknowledgements"),(0,l.kt)("p",null,"The Ego4D challenge would not have been possible without the infrastructure and support of the ",(0,l.kt)("a",{parentName:"p",href:"https://eval.ai/team"},"EvalAI team"),". Thank you! "),(0,l.kt)("h3",{id:"organizers"},"Organizers"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Rohit Girdhar")),(0,l.kt)("li",{parentName:"ul"},"Santhosh Kumar Ramakrishnan\t"),(0,l.kt)("li",{parentName:"ul"},"Chen Zhao"),(0,l.kt)("li",{parentName:"ul"},"Merey Ramazanova"),(0,l.kt)("li",{parentName:"ul"},"Satwik Kottur\t"),(0,l.kt)("li",{parentName:"ul"},"Mengmeng Xu"),(0,l.kt)("li",{parentName:"ul"},"Vincent Cartillier\t"),(0,l.kt)("li",{parentName:"ul"},"Yifei Huang\t"),(0,l.kt)("li",{parentName:"ul"},"Qichen Fu\t"),(0,l.kt)("li",{parentName:"ul"},"Siddhant Bansal\t"),(0,l.kt)("li",{parentName:"ul"},"Hao Jiang\t"),(0,l.kt)("li",{parentName:"ul"},"Vamsi Ithapu"),(0,l.kt)("li",{parentName:"ul"},"Jachym Kolar"),(0,l.kt)("li",{parentName:"ul"},"Christian Fuegen"),(0,l.kt)("li",{parentName:"ul"},"Leda Sari"),(0,l.kt)("li",{parentName:"ul"},"Eric Zhongcong Xu\t "),(0,l.kt)("li",{parentName:"ul"},"Yunyi Zhu  "),(0,l.kt)("li",{parentName:"ul"},"Murong Ma "),(0,l.kt)("li",{parentName:"ul"},"Zachary Chavis\t"),(0,l.kt)("li",{parentName:"ul"},"Wenqi Jia"),(0,l.kt)("li",{parentName:"ul"},"Miao Liu"),(0,l.kt)("li",{parentName:"ul"},"Antonino Furnari\t"),(0,l.kt)("li",{parentName:"ul"},"Tushar Nagarajan"),(0,l.kt)("li",{parentName:"ul"},"Karttikeya Mangalam "),(0,l.kt)("li",{parentName:"ul"},"Dima Damen"),(0,l.kt)("li",{parentName:"ul"},"Giovanni Maria Farinella"),(0,l.kt)("li",{parentName:"ul"},"Michael Wray"),(0,l.kt)("li",{parentName:"ul"},"Gene Byrne"),(0,l.kt)("li",{parentName:"ul"},"Andrew Westbury")))}u.isMDXComponent=!0}}]);