"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[689],{3905:function(e,t,n){n.d(t,{Zo:function(){return h},kt:function(){return u}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),l=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},h=function(e){var t=l(e.components);return a.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},p=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,h=c(e,["components","mdxType","originalType","parentName"]),p=l(n),u=r,b=p["".concat(s,".").concat(u)]||p[u]||d[u]||o;return n?a.createElement(b,i(i({ref:t},h),{},{components:n})):a.createElement(b,i({ref:t},h))}));function u(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=p;var c={};for(var s in t)hasOwnProperty.call(t,s)&&(c[s]=t[s]);c.originalType=e,c.mdxType="string"==typeof e?e:r,i[1]=c;for(var l=2;l<o;l++)i[l]=n[l];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}p.displayName="MDXCreateElement"},4086:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return c},contentTitle:function(){return s},metadata:function(){return l},toc:function(){return h},default:function(){return p}});var a=n(7462),r=n(3366),o=(n(7294),n(3905)),i=["components"],c={sidebar_position:4},s="Hand & Object Interactions",l={unversionedId:"benchmarks/hands-and-objects",id:"benchmarks/hands-and-objects",isDocsHomePage:!1,title:"Hand & Object Interactions",description:"Benchmark Repo//github.com/EGO4D/hands-and-objects",source:"@site/docs/benchmarks/hands-and-objects.md",sourceDirName:"benchmarks",slug:"/benchmarks/hands-and-objects",permalink:"/docs/benchmarks/hands-and-objects",editUrl:"https://https://ego4d-data.org/docs/benchmarks/hands-and-objects.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Forecasting",permalink:"/docs/benchmarks/forecasting"},next:{title:"AV Diarization",permalink:"/docs/benchmarks/av-diarization"}},h=[{value:"Motivation",id:"motivation",children:[],level:2},{value:"Task Definition",id:"task-definition",children:[],level:2},{value:"Annotations",id:"annotations",children:[],level:2}],d={toc:h};function p(e){var t=e.components,n=(0,r.Z)(e,i);return(0,o.kt)("wrapper",(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"hand--object-interactions"},"Hand & Object Interactions"),(0,o.kt)("p",null,"Benchmark Repo: ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/EGO4D/hands-and-objects"},"https://github.com/EGO4D/hands-and-objects")),(0,o.kt)("h2",{id:"motivation"},"Motivation"),(0,o.kt)("p",null,"While Episodic Memory aims to make past video queryable, Hands & Objects aims to understand the camera-wearers present activity \u2013 in terms of interactions with objects. Specifically, the Hands & Objects benchmark captures how the camera-wearer changes the state of an object by using or manipulating it \u2013 which we call an object state change. Though cutting a piece of lumber in half can be achieved through many methods (e.g., various tools, force, speeds, grasps, end effectors), all should be recognized as the same state change."),(0,o.kt)("h2",{id:"task-definition"},"Task Definition"),(0,o.kt)("p",null,"Object state changes can be viewed along temporal, spatial, and semantic axes, leading to these three tasks: "),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Point-of-no-return temporal localization: given a short video clip of a state change, the goal is to estimate the keyframe that contains the point-of\u2013no-return (PNR) (the time at which a state change begins)")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"State change object detection: given three temporal frames (pre, post, PNR), the goal is to regress the bounding box of the object undergoing a state change ")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Object state change classification: Given a short video clip, the goal is to classify whether an object state change has taken place or not "))),(0,o.kt)("h2",{id:"annotations"},"Annotations"),(0,o.kt)("p",null,"We select the data to annotate based on activities that are likely to involve hand-object interactions (e.g., knitting, carpentry, baking, etc.). We start by labeling each narrated hand-object interaction. For each, we label three moments in time (pre, PNR, post) and the bounding boxes for the hands, tools, and objects in each of the three frames. We also annotate the state change types (remove, burn, etc.), action verbs, and nouns for the objects."))}p.isMDXComponent=!0}}]);