<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<title data-react-helmet="true">Ego4D Data overview | Ego4D</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://EGO4D.github.io//docs/data/data-overview/"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Ego4D Data overview | Ego4D"><meta data-react-helmet="true" name="description" content="WIP: Includes internal links, broken markdown, etc to be resolved"><meta data-react-helmet="true" property="og:description" content="WIP: Includes internal links, broken markdown, etc to be resolved"><link data-react-helmet="true" rel="shortcut icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://EGO4D.github.io//docs/data/data-overview/"><link data-react-helmet="true" rel="alternate" href="https://EGO4D.github.io//docs/data/data-overview/" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://EGO4D.github.io//docs/data/data-overview/" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.f4e51aac.css">
<link rel="preload" href="/assets/js/runtime~main.bbcd932f.js" as="script">
<link rel="preload" href="/assets/js/main.3dd1840e.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/ego-4d-logo.png" alt="Ego4d Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/ego-4d-logo.png" alt="Ego4d Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">Ego4d Docs</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/intro/">Start Here</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/EGO4D/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ðŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ðŸŒž</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_lDyR"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_i9tI" type="button"></button><aside class="docSidebarContainer_0YBq"><div class="sidebar_a3j0"><nav class="menu thin-scrollbar menu_cyFh"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro/">Welcome To Ego4d!</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">Ego4d Basics</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">Benchmark Tasks</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#">Data Overview</a><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/data/data-overview/">Ego4D Data overview</a></li></ul></li></ul></nav></div></aside><main class="docMainContainer_r8cw"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_zHA2"><div class="docItemContainer_oiyr"><article><div class="tocCollapsible_aw-L theme-doc-toc-mobile tocMobile_Tx6Y"><button type="button" class="clean-btn tocCollapsibleButton_zr6a">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Ego4D Data overview</h1></header><p><strong>WIP: Includes internal links, broken markdown, etc to be resolved</strong></p><p><strong><a href="#background">Background</a> 1</strong></p><blockquote><p><a href="#key-information">Key Information</a> 1</p><p><a href="#annotations-tldr">Annotations tl;dr</a> 2</p></blockquote><p><strong><a href="#pre-annotations-narrations">Pre-annotations: Narrations</a> 4</strong></p><p><strong><a href="#annotations">Annotations</a> 7</strong></p><blockquote><p><a href="#episodic-memory">Episodic Memory</a> 8</p><p><a href="#natural-language-queries">Natural Language Queries</a> 9</p><p><a href="#moments">Moments</a> 12</p><p><a href="#visual-object-queries">Visual Object Queries</a> 14</p><p><a href="#forecasting-hands-objects-fho">Forecasting + Hands &amp; Objects (FHO)</a>
16</p><p><a href="#stage-1---critical-frames">Stage 1 - Critical Frames</a> 16</p><p><a href="#stage-2---pre-condition">Stage 2 - Pre-condition</a> 18</p><p><a href="#stage-3---post-condition">Stage 3 - Post-condition</a> 20</p><p><a href="#audio-visual-diarization-social-avs">Audio-Visual Diarization &amp; Social (AVS)</a> 20</p><p><a href="#av-step-0-automated-face-head-detection">AV Step 0: Automated Face &amp; Head
Detection</a> 21</p><p><a href="#av-step-1-face-head-tracks-correction">AV Step 1: Face &amp; Head Tracks
Correction</a> 21</p><p><a href="#av-step-2-speaker-labeling-and-av-anchor-extraction">AV Step 2: Speaker Labeling and AV anchor
extraction</a> 23</p><p><a href="#av-step-3-speech-segmentation-per-speaker">AV Step 3: Speech Segmentation (Per
Speaker)</a> 24</p><p><a href="#av-step-4-transcription">AV Step 4: Transcription</a> 25</p><p><a href="#av-step-5-correcting-speech-transcriptions-wip">AV Step 5: Correcting Speech Transcriptions
<!-- -->[<!-- -->WIP<!-- -->]</a> 25</p><p><a href="#s-step-1-camera-wearer-attention">S Step 1: Camera-Wearer Attention</a>
26</p><p><a href="#s-step-2-speech-target-classification">S Step 2: Speech Target
Classification</a> 27</p></blockquote><p><strong><a href="#links">Links</a> 28</strong></p><header><h1>Background</h1></header><p><strong>One-liner:</strong> Building a densely-annotated dataset of <!-- -->~<!-- -->10,000 hours of
ego-centric video for public release.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="key-information">Key Information<a aria-hidden="true" class="hash-link" href="#key-information" title="Direct link to heading">â€‹</a></h2><ul><li><p>~<!-- -->2,600 hours (Jul 21) of unscripted, in-the-wild video data across</p><blockquote><p>7 countries from 10 different partner groups (+ 400 hours from
[<a href="https://www.internalfb.com/intern/wiki/Ego4D/Ego4D_Track_1/" target="_blank" rel="noopener noreferrer">FRL&#x27;s Track
1</a>)</p></blockquote></li><li><p>746 unique camera-wears recording 120 different scenarios, with</p><blockquote><p>hundreds of different actions and objects</p></blockquote></li><li><p>2.5M dense textual &quot;narrations&quot; (= individual text sentences</p><blockquote><p>describing <!-- -->~<!-- -->2,600 hours of video data)\&quot;</p></blockquote></li></ul><p><strong>Devices:</strong></p><p><img src="/assets/images/image7-e0a2bd6a7b876b3194e9a7bbf876256c.png"></p><p><strong>Scenario breakdown:</strong>
<a href="https://fburl.com/datainsights/y2v5taqv" target="_blank" rel="noopener noreferrer">https://fburl.com/datainsights/y2v5taqv</a></p><p><img src="/assets/images/image16-9e55f40cbd0b57fc516ff43a2d52324d.png"></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="annotations-tldr">Annotations tl;dr<a aria-hidden="true" class="hash-link" href="#annotations-tldr" title="Direct link to heading">â€‹</a></h2><table><thead><tr><th><strong>Task</strong></th><th><strong>Output</strong></th><th><strong>Volume</strong></th></tr></thead><tbody><tr><td><strong>Pre-annotations</strong></td><td></td><td></td></tr><tr><td><a href="#pre-annotations-narrations">Narrations</a></td><td>Dense written sentence narrations in English &amp; a summary of the whole video clip</td><td>Full Dataset</td></tr><tr><td><strong>Episodic Memory</strong></td><td></td><td></td></tr><tr><td><a href="#natural-language-queries">Natural Language Queries</a></td><td>N free-form natural anguage queries per video (N=length of video in minutes) selected from a list of query templates + temporal response window from which answers can be deduced</td><td>~<!-- -->240h</td></tr><tr><td><a href="#moments">Moments</a></td><td>Temporal localizations of high level events in a long video clip from a provided taxonomy</td><td>~<!-- -->300h</td></tr><tr><td><a href="#visual-object-queries">Visual Object Queries</a></td><td>For N=3 <strong>query objects</strong> (freely chosen and <strong>named</strong> by the annotator) such that each appears at least twice at separate times in a single video, annotations include: <br> <!-- -->(<!-- -->1<!-- -->)<!-- --> <strong>response track</strong>: bounding boxes over time for one continuous occurrence of the query object; <br> <!-- -->(<!-- -->2<!-- -->)<!-- --> <strong>query frame</strong>: a frame that <em>does</em> <em>not</em> contain the query object, sometime after the response track but before any subsequent occurrence of the object; <br> <!-- -->(<!-- -->3<!-- -->)<!-- --> <strong>visual crop</strong>:  bounding box of a single frame from another occurrence of the same object elsewhere in the video (before or after the originally marked instance)</td><td>~<!-- -->403h</td></tr><tr><td><strong>Forecasting + Hands &amp; Objects (FHO)</strong></td><td></td><td></td></tr><tr><td><a href="#stage-1---critical-frames">1 Critical Frames</a></td><td>Pre-condition (PRE), CONTACT, point of no return (PNR), and post-condition (Post) frames for each narrated action in a video</td><td>~<!-- -->120h</td></tr><tr><td><a href="#stage-2---pre-condition">2 Pre-condition</a></td><td>Bounding boxes and roles for hands (right/left) and objects (objects of change and tools) for each frame from CONTACT to PRE</td><td></td></tr><tr><td><a href="#stage-3---post-condition">3 Post-condition</a></td><td>Bounding boxes and roles for hands and objects for each frame from CONTACT to POST</td><td></td></tr><tr><td><strong><a href="#audio-visual-diarization--social-avs">Audio-Visual Diarization &amp; Social</a> (AVS)</strong></td><td></td><td></td></tr><tr><td><a href="#av-step-0-automated-face-head-detection">AV0: Automated Face &amp; Head Detection</a></td><td>Automated overlaid bounding boxes for faces in video clips</td><td>50h</td></tr><tr><td><a href="#av-step-1-face-head-tracks-correction">AV1: Face &amp; Head Tracks Correction</a></td><td>Manually adjusted overlaid bounding boxes for faces in video clips</td><td></td></tr><tr><td><a href="#av-step-2-speaker-labeling-and-av-anchor-extraction">AV2: Speaker Labeling and AV anchor extraction</a></td><td>Anonymous Person IDs for each Face Track in video clip</td><td></td></tr><tr><td><a href="#av-step-3-speech-segmentation-per-speaker">AV3: Speech Segmentation (Per Speaker)</a></td><td>Temporal segments for voice activity for the camera wearer and for each Person ID</td><td></td></tr><tr><td><a href="#av-step-4-transcription">AV4: Transcription</a></td><td>Video clip audio transcriptions</td><td></td></tr><tr><td><a href="#av-step-5-correcting-speech-transcriptions-wip">AV5: Correcting Speech Transcriptions</a></td><td>Corrected Speech Transcription annotations matching voice activity segments and Person IDs from AV2</td><td></td></tr><tr><td><a href="#s-step-1-camera-wearer-attention">S1: Camera-Wearer Attention</a></td><td>Temporal segments in which a person is looking at the camera wearer</td><td></td></tr><tr><td><a href="#s-step-2-speech-target-classification">S2: Speech Target Classification</a></td><td>Temporal segments in which a person is talking to the camera wearer</td><td></td></tr></tbody></table><header><h1>Pre-annotations: Narrations</h1></header><p><strong>Objective:</strong> Annotator provides dense written sentence narrations in
English on a first-person video clip of length 10-30 minutes + a summary
of the whole video.</p><p><strong>Motivation:</strong> Understand what data is available and which data to push
through which annotation phases. Provide a starting point for forming a
taxonomy of labels for actions and objects.</p><p><strong>Annotation task:</strong></p><p>|--------|------------------|------------------|------------------|
| <strong>#</strong> | <strong>Step</strong>         | <strong>Sub-step</strong>     | <strong>Example</strong>      |
+========+==================+==================+==================+
| 1      | <em>Narrate the     | Watch the video  | </em>[<!-- -->set the start |
|        | Complete Video   | from the         | time as the      |
|        | with Temporal    | beginning until  | point when the   |
|        | Sentences<em>       | something new    | person has the   |
|        |                  | occurs.          | knife and the    |
|        |                  |                  | tomato, and the  |
|        |                  |                  | end time as the  |
|        |                  |                  | point when the   |
|        |                  |                  | person has       |
|        |                  |                  | finished         |
|        |                  |                  | chopping, then   |
|        |                  |                  | type</em>]<!-- -->: &quot;C is   |
|        |                  |                  | chopping a       |
|        |                  |                  | tomato&quot; into the |
|        |                  |                  | text input. (&quot;C&quot; |
|        |                  |                  | refers to the    |
|        |                  |                  | camera wearer).  |
|--------|------------------|------------------|------------------|
|        |                  | At that time,    |                  |
|        |                  | pause the video, |                  |
|        |                  | mark the         |                  |
|        |                  | <em>temporal        |                  |
|        |                  | window</em> for      |                  |
|        |                  | which the        |                  |
|        |                  | sentence         |                  |
|        |                  | applies, then    |                  |
|        |                  | &quot;narrate&quot; what   |                  |
|        |                  | you see in the   |                  |
|        |                  | video by typing  |                  |
|        |                  | in a simple      |                  |
|        |                  | sentence into    |                  |
|        |                  | the free-form    |                  |
|        |                  | text input.      |                  |
|--------|------------------|------------------|------------------|
|        |                  | Next, resume     |                  |
|        |                  | watching the     |                  |
|        |                  | video. Once you  |                  |
|        |                  | recognize an     |                  |
|        |                  | action to        |                  |
|        |                  | narrate,         |                  |
|        |                  | immediately      |                  |
|        |                  | pause again and  |                  |
|        |                  | repeat.          |                  |
|--------|------------------|------------------|------------------|
| 2      | <em>Provide a       | As needed, watch | <!-- -->#<!-- -->summary C      |
|        | Summary of the   | the entire video | fixed their      |
|        | Entire Video</em>    | on fast forward  | breakfast, ate   |
|        |                  | to recall the    | it, then got     |
|        |                  | content of the   | dressed and left |
|        |                  | entire video.    | the house.&quot;      |
|        |                  |                  |                  |
|        |                  | Provide a short  |                  |
|        |                  | summary in text  |                  |
|        |                  | about the        |                  |
|        |                  | contents of the  |                  |
|        |                  | entire video     |                  |
|        |                  | (1-3 sentences). |                  |
|        |                  |                  |                  |
|        |                  | This summary     |                  |
|        |                  | should convey    |                  |
|        |                  | the main         |                  |
|        |                  | setting(s) of    |                  |
|        |                  | the video clip   |                  |
|        |                  | (e.g., an        |                  |
|        |                  | apartment, a     |                  |
|        |                  | restaurant, a    |                  |
|        |                  | shop, etc.) as   |                  |
|        |                  | well as an       |                  |
|        |                  | overview of what |                  |
|        |                  | happened.        |                  |
|--------|------------------|------------------|------------------|</p><p><strong>UI Examples</strong></p><ul><li><strong>Narration:</strong></li></ul><p><img src="/assets/images/image42-9e5ac4e4b20ec34bb336528981701d9a.png">{width=&quot;5.429300087489064in&quot;
height=&quot;2.9843755468066493in&quot;}</p><ul><li><strong>Summary:</strong></li></ul><blockquote><p><img src="/assets/images/image44-35c45438632a232cfa14668d3b427e51.png">{width=&quot;5.974200568678915in&quot;
height=&quot;3.307292213473316in&quot;}</p></blockquote><p><strong>Annotated videos examples:</strong></p><p>[<a href="https://drive.google.com/file/d/14NrVdpYT2RyJU_rKG99AkIToIwNO6JEY/view?usp=sharing" target="_blank" rel="noopener noreferrer">Example
reel</a></p><p><img src="/assets/images/image35-91b5eb2d1a6d9ba45ec1f3bba497a145.png">{width=&quot;3.0196161417322833in&quot;
height=&quot;2.0753379265091865in&quot;}<img src="/assets/images/image32-8b0a8c1616304b84176927470f098846.png">{width=&quot;2.903805774278215in&quot;
height=&quot;2.076558398950131in&quot;}</p><p><strong>Annotation Stats (Jul 21)</strong></p><ul><li><p><strong>Total hours narrated:</strong> 2700</p></li><li><p><strong>Unique scenarios:</strong> 51</p><blockquote><p>([<a href="https://fburl.com/datainsights/oqrnhisc" target="_blank" rel="noopener noreferrer">breakdown</a>)</p></blockquote></li></ul><p><img src="/assets/images/image13-1dbe07a190ef030848b79fe86ab52b9f.png">{width=&quot;6.5in&quot; height=&quot;2.638888888888889in&quot;}</p><p><strong>Links:</strong></p><ul><li><p>Narration</p><blockquote><p><a href="https://docs.google.com/document/d/1kHHgJFQM2wbm2M81GjyicoZnGfgix7vnIHhEfkcB0_8/edit#heading=h.zb77nl2kkug0" target="_blank" rel="noopener noreferrer">notes</a>
and
<a href="https://l.workplace.com/l.php?u=https%3A%2F%2Fdocs.google.com%2Fdocument%2Fd%2F1avjdALDI3x1jrnY3DiCv72GGZJzXSuRRMjbmcQURKS0%2Fedit%3Fusp%3Dsharing&amp;h=AT0SmFl7unFKCRMTaAd_2TRlp8Wc7pA0eZEBsRyDqTA5z_vaAxftnRJsGAtJa1PBX60OS0M98dEFj7bBuOuy797sFTls8HXCPzIrjfegkk1gxJOO3elVYcWiVdl2NOF3W0zO1TepzJVPHXx3HL_K&amp;__tn__=-UK-y-R&amp;c%5B0%5D=AT2NBxuTggKaQcYTPVeWQRgFrhv33HhiCNjdu1zHc2EqWu_nDJd2thvGRuiSyviJKTcpIlGgCiweuV-3X_1fkLBC10oNlFIBGlLAj6sX1A_twCFkuq2dCpP__mpZm_HrKDiRn-BIDyrsuNnymY9Kq2LGA3382FSPPwXvZdK2TX8ksWeu8KPPsuxOdCoRRQwJMmy6" target="_blank" rel="noopener noreferrer">instructions</a></p></blockquote></li><li><p>[Narration</p><blockquote><p>analysis](<a href="https://docs.google.com/presentation/d/1es_hniyef5bGhtMyeSZkBfbdeXKwElPBjT0YXxr7BvA/edit?usp=sharing" target="_blank" rel="noopener noreferrer">https://docs.google.com/presentation/d/1es_hniyef5bGhtMyeSZkBfbdeXKwElPBjT0YXxr7BvA/edit?usp=sharing</a>)</p></blockquote></li><li><p>[Scenario</p><blockquote><p>breakdown](<a href="https://l.workplace.com/l.php?u=https%3A%2F%2Fdocs.google.com%2Fspreadsheets%2Fd%2F1w1dW1-IyqvufD-X_3SFSBBdLAhOy7JZbNSbhZjDel6I%2Fedit%23gid%3D0&amp;h=AT0JEXfOJJdon1Fc0QP5lnJKL4hMqBvLTnHJPXJleRbSCBAuFMzwiFGE07hyz_YKspJB68Do3y5pyan4963Npz9a4oIBpDhbT1515yy1pJ1uAV0gxJhvCp5EtO-ENJI1yCVueixUaClGp8SIizl6&amp;__tn__=-UK-y-R&amp;c%5B0%5D=AT2NBxuTggKaQcYTPVeWQRgFrhv33HhiCNjdu1zHc2EqWu_nDJd2thvGRuiSyviJKTcpIlGgCiweuV-3X_1fkLBC10oNlFIBGlLAj6sX1A_twCFkuq2dCpP__mpZm_HrKDiRn-BIDyrsuNnymY9Kq2LGA3382FSPPwXvZdK2TX8ksWeu8KPPsuxOdCoRRQwJMmy6" target="_blank" rel="noopener noreferrer">https://l.workplace.com/l.php?u=https%3A%2F%2Fdocs.google.com%2Fspreadsheets%2Fd%2F1w1dW1-IyqvufD-X_3SFSBBdLAhOy7JZbNSbhZjDel6I%2Fedit%23gid%3D0&amp;h=AT0JEXfOJJdon1Fc0QP5lnJKL4hMqBvLTnHJPXJleRbSCBAuFMzwiFGE07hyz_YKspJB68Do3y5pyan4963Npz9a4oIBpDhbT1515yy1pJ1uAV0gxJhvCp5EtO-ENJI1yCVueixUaClGp8SIizl6&amp;__tn__=-UK-y-R&amp;c%5B0%5D=AT2NBxuTggKaQcYTPVeWQRgFrhv33HhiCNjdu1zHc2EqWu_nDJd2thvGRuiSyviJKTcpIlGgCiweuV-3X_1fkLBC10oNlFIBGlLAj6sX1A_twCFkuq2dCpP__mpZm_HrKDiRn-BIDyrsuNnymY9Kq2LGA3382FSPPwXvZdK2TX8ksWeu8KPPsuxOdCoRRQwJMmy6</a>)</p></blockquote></li></ul><header><h1>Annotations</h1></header><p><img src="/assets/images/image18-fec68ada7e115e2b5ddad1b313a3d283.png"></p><p>  <strong>Target</strong>    <strong>#</strong>   <strong>Benchmark task</strong>                                                        <strong>Research Goal</strong></p><hr><p>  <strong>Places</strong>    1        [<a href="#tdkchjvksenv">Episodic Memory</a>                                   Allow an Assistant user to ask free-form, natural language questions, with the answer brought back after analyzing past video (<em>When was the last time I changed the batteries in the smoke detector?</em>).
<strong>Objects</strong>   2        [<a href="#forecasting-hands-objects-fho">Forecasting</a>                      To intelligently deliver notifications to a user, an AR system must understand how an action or piece of information may impact the future state of the world.
3        [<a href="#forecasting-hands-objects-fho">Hands-Object interaction</a>         AR applications, e.g. providing users instructions in their egocentric real-world view to accomplish tasks (e.g., cooking a recipe).
<strong>People</strong>    4        [<a href="#audio-visual-diarization-social-avs">Audio-visual Diarization</a>   To effectively aid people in daily life scenarios, augmented reality must be able to detect and track sounds, responding to users queries or information needs.
5        [<a href="#audio-visual-diarization-social-avs">Social interactions</a>        Recognize people&#x27;s interactions, their roles, and their attention within collaborative and competitive scenarios within a range of social interactions captured in the Ego4D data.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="episodic-memory">Episodic Memory<a aria-hidden="true" class="hash-link" href="#episodic-memory" title="Direct link to heading">â€‹</a></h2><p><strong>Motivation</strong>: Augment human memory through personal semantic video
index for an always-on wearable camera</p><p><strong>Objective</strong>: Given long first-person video, <em>localize</em> answers for
queries about objects and events from first-person experience</p><blockquote><p><em>Who did I sit by at the party? Where are my keys? When did I change
the batteries? How often did I read to my child last week? Did I leave
the window open?<!-- -->.<!-- -->..</em></p></blockquote><p><strong>Query types (annotation sub-tasks)</strong>:</p><p>a.  Natural language queries (response = temporal)</p><p>b.  Moments queries (response = temporal)</p><p>c.  Visual/object queries (response = temporal+spatial)</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="natural-language-queries">Natural Language Queries<a aria-hidden="true" class="hash-link" href="#natural-language-queries" title="Direct link to heading">â€‹</a></h3><h3><img src="/assets/images/image4-8e3ced36f3852c4e3022c81523a33885.png"></h3><p><strong>Objective:</strong> Create and annotate <strong>N (N=length of video in minutes)</strong>
interesting questions and their corresponding answers for the given
video.</p><p><strong>Annotation Task</strong> (see <a href="https://docs.google.com/document/d/1RGbm4BjKt8bZ6a95gYno8DJemuCZBhM2d6WWt_W3vLY/edit?usp=sharing" target="_blank" rel="noopener noreferrer">Annotation instructions</a>)<strong>:</strong></p><p>+--------+------------------+------------------+------------------+
| <strong>#</strong> | <strong>Step</strong>         | <strong>Sub-step</strong>     | <strong>Example</strong>      |
+========+==================+==================+==================+
| 0      | <em>Annotator       |                  |                  |
|        | watches video</em>   |                  |                  |
+--------+------------------+------------------+------------------+
| 1      | <em>Asks free-form  | -   Select an    | -                |
|        | natural language |     interesting  | <!-- -->[Template]<!-- -->: |
|        | query at end of  |     query        |     &quot;What <strong>X</strong>  |
|        | video, selecting |     template &amp;   |     is <strong>Y</strong>?&quot;   |
|        | from list of     |     template     |                  |
|        | query            |     category     | -   [Template    |
|        | templates.</em>      |                  |                  |
|        |                  | -   Paraphrase   |  <!-- -->[Category]<!-- -->: |
|        |                  |     question in  |     &quot;Objects&quot;    |
|        |                  |     the past     |                  |
|        |                  |     tense.       | -   <!-- -->[Paraphrased |
|        |                  |                  |     query:]<!-- --> |
|        |                  |                  |     &quot;What        |
|        |                  |                  |     <strong>color</strong>    |
|        |                  |                  |     shirt did    |
|        |                  |                  |     <strong>the person |
|        |                  |                  |     performing   |
|        |                  |                  |     on the       |
|        |                  |                  |     road</strong>       |
|        |                  |                  |     wear?&quot;       |
+--------+------------------+------------------+------------------+
|        |                  | Using            | First free-form  |
|        |                  | <strong>&quot;free-form&quot;</strong>  | query slot:      |
|        |                  | text, fill the   | &quot;<strong>color</strong>&quot;      |
|        |                  | <strong>query slots</strong>  |                  |
|        |                  | (X, Y, <!-- -->.<!-- -->..) in  | Second free-form |
|        |                  | the template to  | query slot:      |
|        |                  | form a           | &quot;<strong>the shirt of  |
|        |                  | meaningful       | the person       |
|        |                  | question         | performing on    |
|        |                  | equivalent to    | the road</strong>&quot;      |
|        |                  | the paraphrase.  |                  |
+--------+------------------+------------------+------------------+
|        |                  | Pick the closest | -   <!-- -->[Paraphrased |
|        |                  | verb for each of |     query]<!-- -->: |
|        |                  | the slots in the |     What         |
|        |                  | respective       |                  |
|        |                  | drop-down menus  |   <strong>instrument</strong> |
|        |                  |                  |     was <strong>the    |
|        |                  |                  |     musician     |
|        |                  |                  |     playing</strong>?   |
|        |                  |                  |                  |
|        |                  |                  | -   <!-- -->[First verb  |
|        |                  |                  |     drop-down selection]<!-- -->: |
|        |                  |                  |     &quot;<!-- -->[<!-- -->VERB NOT APPLICABLE<!-- -->]<!-- -->&quot; |
|        |                  |                  |                  |
|        |                  |                  | -   Second verb  |
|        |                  |                  |     drop-down    |
|        |                  |                  |     selection:   |
|        |                  |                  |     &quot;<strong>play</strong>&quot;   |
+--------+------------------+------------------+------------------+
| 2      | <em>Identifies the  | Seek in the      |                  |
|        | temporal         | video to the     |                  |
|        | response window  | temporal window  |                  |
|        | from which       | where the        |                  |
|        | answer can be    | response to the  |                  |
|        | deduced</em>         | natural language |                  |
|        |                  | query can be     |                  |
|        |                  | deduced visually |                  |
|        |                  |                  |                  |
|        |                  | Specify query to |                  |
|        |                  | have only one    |                  |
|        |                  | valid,           |                  |
|        |                  | contiguous       |                  |
|        |                  | temporal window  |                  |
|        |                  | response         |                  |
+--------+------------------+------------------+------------------+
| 3      | <em>Repeat this     |                  |                  |
|        | process N=length |                  |                  |
|        | of video in      |                  |                  |
|        | minutes creating |                  |                  |
|        | N diverse        |                  |                  |
|        | language         |                  |                  |
|        | queries</em>         |                  |                  |
+--------+------------------+------------------+------------------+</p><p><strong>UI Examples:</strong></p><p><img src="/assets/images/image43-7929512eb1545fa9915c9f6261447238.png"></p><p><strong>Annotated UI Example:</strong></p><p>  <strong>#</strong>   <strong>Name</strong>                                         <strong>Example</strong></p><hr><p>  1        Query Set Label                                  <em>Query Set 1</em>
2        Template Query Category                          <em>Object</em>
3        Template                                         <em>Where is object X before/after event Y?</em>
4        Paraphrased template in natural language         <em>Where were the blue pliers before I picked them up?</em>
5        First slot (X)                                   <em>Blue pliers</em>
6        Dominant Verb Taxonomy for the first slot (X)    Verb: <em>[<!-- -->VERB NOT APPLICABLE<!-- -->]</em>
7        Second slot (Y)                                  <em>I picked them up</em>
8        Dominant Verb Taxonomy for the second slot (Y)   <em>pick</em></p><p><img src="/assets/images/image31-219567f3e6b0e31e8b839676ab8fafbb.png"></p><p><strong>Annotated videos examples:</strong></p><p>[<a href="https://drive.google.com/file/d/14NrVdpYT2RyJU_rKG99AkIToIwNO6JEY/view?usp=sharing" target="_blank" rel="noopener noreferrer">Example (cooking, bike mechanic)</a></p><p><img src="/assets/images/image14-168aa13b5d867f63500a67faa6de9f14.png"></p><p><strong>Annotation Stats (Jul 21):</strong></p><ul><li><p><strong>Total hours annotated:</strong> <!-- -->~<!-- -->240 (x2; one for each vendor)</p></li><li><p><strong>Distribution over question types:</strong></p></li></ul><p><img src="/assets/images/image2-73d744aacb706a6c6be825d0f333a008.png"></p><ul><li><strong>Scenario breakdown:</strong></li></ul><p><img src="/assets/images/image3-db38883c4c8f547284a969c5d3197adf.png"></p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="moments">Moments<a aria-hidden="true" class="hash-link" href="#moments" title="Direct link to heading">â€‹</a></h3><p><img src="/assets/images/image10-a15f59ac4828168c130fc993a5dba16d.png"></p><p><strong>Objective:</strong> Localize high level events in a long video clip <!-- -->-<!-- -->-
marking any instance of provided activity categories with a temporal
window and the activity&#x27;s name.</p><p><strong>Motivation:</strong> Learn to detect activities or &quot;moments&quot; and their
temporal extent in the video. In the context of episodic memory, the
implicit query from a user would be &quot;When is the last time I did X?&quot;,
and the response from the system would be to show the time window where
activity X was last seen.</p><p><strong>Annotation Task</strong> (see <a href="https://docs.google.com/document/d/1lGtcGjxYOOQsf9SalVehEocqjsh26j0LbmhGucojkOw/edit" target="_blank" rel="noopener noreferrer">Annotation instructions</a>)<strong>:</strong></p><p>+--------+------------------+------------------+------------------+
| <strong>#</strong> | <strong>Step</strong>         | <strong>Sub-step</strong>     | <strong>Example</strong>      |
+========+==================+==================+==================+
| 1      | <em>Review the      |                  | ![]              |
|        | Taxonomy</em>        |                  | (media/image15.p |
|        |                  |                  | ng) |
+--------+------------------+------------------+------------------+
| 2      | <em>Annotate the    | 1.  Play the     | </em>See [<!-- -->[UI        |
|        | Video*           |     video until  | Examples]<!-- -->  |
|        |                  |     you observe  | (#llbjhw5qjwgg)* |
|        |                  |     an activity, |                  |
|        |                  |     then pause.  |                  |
|        |                  |                  |                  |
|        |                  | 2.  Draw a       |                  |
|        |                  |     temporal     |                  |
|        |                  |     window       |                  |
|        |                  |     around the   |                  |
|        |                  |     time span    |                  |
|        |                  |     where the    |                  |
|        |                  |     activity     |                  |
|        |                  |     occurs       |                  |
|        |                  |                  |                  |
|        |                  | 3.  Select from  |                  |
|        |                  |     the dropdown |                  |
|        |                  |     list the     |                  |
|        |                  |     name for     |                  |
|        |                  |     that         |                  |
|        |                  |     activity     |                  |
|        |                  |                  |                  |
|        |                  | 4.  Play the     |                  |
|        |                  |     video from   |                  |
|        |                  |     the start of |                  |
|        |                  |     the previous |                  |
|        |                  |     activity,    |                  |
|        |                  |     repeat steps |                  |
|        |                  |     1-3          |                  |
+--------+------------------+------------------+------------------+</p><p><strong>UI Examples:</strong></p><p><img src="/assets/images/image45-4154b0e8b1843819b876742ac2343582.png"></p><p><strong>Annotation Stats (Jul 21):</strong></p><ul><li><strong>Total hours annotated:</strong> <!-- -->~<!-- -->300 (x3 raters)</li></ul><h3 class="anchor anchorWithStickyNavbar_y2LR" id="visual-object-queries">Visual Object Queries<a aria-hidden="true" class="hash-link" href="#visual-object-queries" title="Direct link to heading">â€‹</a></h3><p><img src="/assets/images/image33-930945bc750e68e528ae8219b42fce82.png"></p><p><strong>Objective:</strong> Localize past instances of a given object that appears at
least twice in different parts of the video.</p><p><strong>Motivation:</strong> Support an object search application for video in which
a user asks at time T &quot;where did I last see X?&quot;, and the system scans
back in the video history starting at query frame T, finds the most
recent instance of X, and outlines it in a short track<strong>.</strong></p><p><strong>Annotation Task:</strong> (see <a href="https://docs.google.com/document/d/1Ks9qVQjTE16tJXsC3fh-64Rlkoc_qbQafxyu1c6uNYw/edit?usp=sharing" target="_blank" rel="noopener noreferrer">Annotation instructions</a>)</p><p>+--------+------------------+------------------+------------------+
| <strong>#</strong> | <strong>Step</strong>         | <strong>Sub-step</strong>     | <strong>Example</strong>      |
+========+==================+==================+==================+
| 1      | *Identify        | Preview the      |                  |
|        | <strong>query objects  | entire video     |                  |
|        | *</strong>              |                  |                  |
|        |                  | Identify a set   |                  |
|        |                  | of N=3           |                  |
|        |                  | interesting      |                  |
|        |                  | objects to label |                  |
|        |                  | as queries (=    |                  |
|        |                  | objects that     |                  |
|        |                  | appear at least  |                  |
|        |                  | twice at         |                  |
|        |                  | distinct         |                  |
|        |                  | non-contiguous   |                  |
|        |                  | parts of the     |                  |
|        |                  | video clip)      |                  |
+--------+------------------+------------------+------------------+
| 2      | *Select a        | -   Select one   | ![]              |
|        | <strong>response       |     occurrence   | (media/image30.g |
|        | track*</strong>         |     of the query | if){width=&quot;1.895 |
|        |                  |     object.      | 8333333333333in&quot; |
|        |                  |                  | height=&quot;1.0694   |
|        |                  | -   Mark the     | 444444444444in&quot;} |
|        |                  |     query object |                  |
|        |                  |     with a       |                  |
|        |                  |     bounding box |                  |
|        |                  |     over time,   |                  |
|        |                  |     from the     |                  |
|        |                  |     frame the    |                  |
|        |                  |     object       |                  |
|        |                  |     enters the   |                  |
|        |                  |     field of     |                  |
|        |                  |     view until   |                  |
|        |                  |     it leaves    |                  |
|        |                  |     the field of |                  |
|        |                  |     view, for    |                  |
|        |                  |     that object  |                  |
|        |                  |     occurrence.  |                  |
+--------+------------------+------------------+------------------+
| 3      | *Select a        | -   Select a     | <!-- -->![               |
|        | **query frame*** |     frame that   | ]<!-- -->(media/image9.p |
|        |                  |     does <em>not</em>   | ng){width=&quot;1.833 |
|        |                  |     contain the  | 3333333333333in&quot; |
|        |                  |     query        | height=&quot;1.25in&quot;} |
|        |                  |     object,      |                  |
|        |                  |     sometime far |                  |
|        |                  |     <em>after</em> that |                  |
|        |                  |     object       |                  |
|        |                  |     occurrence,  |                  |
|        |                  |     but <em>before</em> |                  |
|        |                  |     any          |                  |
|        |                  |     subsequent   |                  |
|        |                  |     occurrence   |                  |
|        |                  |     of the       |                  |
|        |                  |     object.      |                  |
|        |                  |                  |                  |
|        |                  | -   Mark the     |                  |
|        |                  |     time point   |                  |
|        |                  |     with a large |                  |
|        |                  |     bounding     |                  |
|        |                  |     box.         |                  |
+--------+------------------+------------------+------------------+
| 4      | *Select a        | -   Find another | ![]              |
|        | <strong>visual crop*</strong> |     occurrence   | (media/image11.p |
|        |                  |     of the same  | ng){width=&quot;1.895 |
|        |                  |     object       | 8333333333333in&quot; |
|        |                  |     elsewhere in | height=&quot;1.25in&quot;} |
|        |                  |     the video    |                  |
|        |                  |     (before or   |                  |
|        |                  |     after the    |                  |
|        |                  |     originally   |                  |
|        |                  |     marked       |                  |
|        |                  |     instance     |                  |
|        |                  |     from Step 2) |                  |
|        |                  |                  |                  |
|        |                  | -   Draw a       |                  |
|        |                  |     bounding box |                  |
|        |                  |     in           |                  |
|        |                  |     <!-- -->[one]<!-- -->{.ul}   |                  |
|        |                  |     frame around |                  |
|        |                  |     that object. |                  |
+--------+------------------+------------------+------------------+
| 5      | <strong>*Name</strong>        |                  |                  |
|        | <strong>the</strong>          |                  |                  |
|        | <strong>object</strong> using |                  |                  |
|        | the <strong>free       |                  |                  |
|        | text</strong> box<em>      |                  |                  |
+--------+------------------+------------------+------------------+
| 6      | </em>Repeat Steps    |                  |                  |
|        | 1-5 three times  |                  |                  |
|        | for the same     |                  |                  |
|        | video clip and   |                  |                  |
|        | different        |                  |                  |
|        | objects*         |                  |                  |
+--------+------------------+------------------+------------------+</p><p><strong>Annotation Stats (Jul 21):</strong></p><ul><li><strong>Total hours annotated:</strong> <!-- -->~<!-- -->403</li></ul><blockquote><p><img src="/assets/images/image1-cb95f7a6ce8d8cc57a8a376a4870bd4b.png"></p></blockquote><ul><li><strong>Scenario breakdown:</strong></li></ul><p><img src="/assets/images/image21-997834909d39c1d2ea8729613e8cf735.png"></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="forecasting--hands--objects-fho">Forecasting + Hands &amp; Objects (FHO)<a aria-hidden="true" class="hash-link" href="#forecasting--hands--objects-fho" title="Direct link to heading">â€‹</a></h2><p><strong>Objective:</strong> Recognize object state changes temporally and spatially
(HO); predict these interactions spatially and temporally before they
happen (F).</p><p><strong>Motivation:</strong> Understanding and anticipating human-object
interactions.</p><p><strong>Scenario Distribution:</strong></p><p><img src="/assets/images/image19-00e19a2a62a562005bd744ae7965764a.png"></p><p>|----------------------------------|----------------------------------|
| <strong>Annotation</strong>                   | <strong>Scenario Distribution:</strong>       |
| (Aug 21):**                      |                                  |
|                                  |                                  |
| Labeled videos: 1,074            |                                  |
|                                  |                                  |
| Labeled clips: 1,672             |                                  |
|                                  |                                  |
| Labeled hours: 116.274           |                                  |
|                                  |                                  |
| Number of scenarios: 53          |                                  |
|                                  |                                  |
| Number of universities: 7        |                                  |
|                                  |                                  |
| Number of participants: 397      |                                  |
|                                  |                                  |
| Num interactions: 91,002         |                                  |
|                                  |                                  |
| Num rejected: 18,839             |                                  |
|                                  |                                  |
| Num with state change: 70,718    |                                  |
|                                  |                                  |
+----------------------------------+----------------------------------+</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="stage-1---critical-frames">Stage 1 - Critical Frames<a aria-hidden="true" class="hash-link" href="#stage-1---critical-frames" title="Direct link to heading">â€‹</a></h3><p><img src="/assets/images/image5-1c194e80ab2a55776812a699b248b5ee.png">{width=&quot;4.882327209098863in&quot;
height=&quot;1.276246719160105in&quot;}</p><p><strong>Objective:</strong> Annotator watches an egocentric video and marks
pre-condition (PRE), contact, point of no return (PNR), and
post-condition (Post) frames.</p><p><strong>Annotation Task</strong> (See [<a href="https://docs.google.com/document/d/13BmI98M_4gzd31vYAtQ8wRSLHggpnrts0gOVDTrWnDM/edit?usp=sharing" target="_blank" rel="noopener noreferrer">Annotation instructions</a>)<strong>:</strong></p><p>+--------+------------------+------------------+------------------+
| <strong>#</strong> | <strong>Step</strong>         | <strong>Sub-step</strong>     | <strong>Example</strong>      |
+========+==================+==================+==================+
| 1      | <em>Read the        | 1<!-- -->.<!-- --> Reject       | </em>Example:<em> &quot;C    |
|        | narrated action  | videos that do   | glides hand      |
|        | to be labeled</em>   | not contain      | planer along the |
|        |                  | hand-object      | wood&quot;![]         |
|        |                  | interactions     | (media/image46.p |
|        |                  |                  | ng){width=&quot;1.895 |
|        |                  | 2<!-- -->.<!-- --> Reject       | 8333333333333in&quot; |
|        |                  | videos that not  | height=&quot;1.0277   |
|        |                  | contain the      | 777777777777in&quot;} |
|        |                  | narrated action  |                  |
+--------+------------------+------------------+------------------+
| 2      | <em>Select the verb | -   If an        | ![]              |
|        | corresponding to |     appropriate  | (media/image28.p |
|        | the narration</em>   |     verb is not  | ng){width=&quot;1.895 |
|        |                  |     available,   | 8333333333333in&quot; |
|        |                  |     select OTHER | height=&quot;1.0in&quot;}  |
|        |                  |     from the     |                  |
|        |                  |     dropdown and |                  |
|        |                  |     type in the  |                  |
|        |                  |     verb in the  |                  |
|        |                  |     text box.    |                  |
+--------+------------------+------------------+------------------+
| 3      | <em>Select the      | -   Select one   | ![]              |
|        | <strong>state change   |     &gt; of 8       | (media/image22.p |
|        | type</strong> present   |     &gt; options    | ng){width=&quot;1.895 |
|        | in the video</em>    |     &gt; from the   | 8333333333333in&quot; |
|        |                  |     &gt; dropdown   | height=&quot;1.0277   |
|        |                  |                  | 777777777777in&quot;} |
+--------+------------------+------------------+------------------+
| 4      | <em>Mark the        | -   Find the     | ![]              |
|        | <strong>CONTACT</strong>      |     &gt; CONTACT    | (media/image47.p |
|        | (only if         |     &gt; frame      | ng){width=&quot;1.895 |
|        | present),</em>       |                  | 8333333333333in&quot; |
|        | <strong>PRE</strong> and      | -   Pause the    | height=&quot;1.0138   |
|        | <strong>POST</strong> frames. |     &gt; video      | 888888888888in&quot;} |
|        |                  |                  |                  |
|        |                  | -   Select the   |                  |
|        |                  |     &gt; &quot;Contact   |                  |
|        |                  |     &gt; Frame&quot;     |                  |
|        |                  |     &gt; from the   |                  |
|        |                  |     &gt; dropdown   |                  |
|        |                  |                  |                  |
|        |                  | -   Repeat the   |                  |
|        |                  |     &gt; same       |                  |
|        |                  |     &gt; protocol   |                  |
|        |                  |     &gt; for PRE    |                  |
|        |                  |     &gt; and POST   |                  |
|        |                  |     &gt; frames.    |                  |
+--------+------------------+------------------+------------------+</p><p><strong>PRE, CONTACT, PNR, POST examples:</strong></p><p>a.  Example: &quot;light blowtorch&quot;</p><blockquote><p><img src="/assets/images/image38-80d977484213083a627b3a0d642d91b4.jpg">{width=&quot;5.646893044619422in&quot;
height=&quot;1.1927088801399826in&quot;}</p></blockquote><p>b.  Example: &quot;put down wood&quot; (object already in hands, no CONTACT</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#393A34"><span class="token plain">&gt; frame)![](media/image39.jpg){width=&quot;4.238373797025372in&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&gt; height=&quot;1.1927088801399826in&quot;}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>c.  [[VIDEO</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#393A34"><span class="token plain">&gt; EXAMPLES](https://drive.google.com/file/d/1Fvg6ddceiVAbOru69XXB3PExuZAPd7ad/view?usp=sharing)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_y2LR" id="stage-2---pre-condition">Stage 2 - Pre-condition<a aria-hidden="true" class="hash-link" href="#stage-2---pre-condition" title="Direct link to heading">â€‹</a></h3><p><strong>Objective:</strong> Label bounding boxes and roles for hands (right/left) and
objects (objects of change and tools).</p><p><strong>Annotation Task</strong> (see [<a href="https://docs.google.com/document/d/1bjbjJVFEUnl_GnTFmjfZry49_7c7DdR_RBotyjLoGgM/edit?usp=sharing" target="_blank" rel="noopener noreferrer">Annotation
instructions</a>
and [<a href="https://drive.google.com/file/d/14gXr6yMb815L79jp0QN_n2X9e_OXpqIa/view" target="_blank" rel="noopener noreferrer">video
tutorial</a>)
<!-- -->[Note]<!-- -->{.ul}: clips annotated from previous stage play in reverse from
CONTACT to PRE frame:</p><p>+--------+------------------+------------------+------------------+
| <strong>#</strong> | <strong>Step</strong>         | <strong>Sub-step</strong>     | <strong>Example</strong>      |
+========+==================+==================+==================+
| 1      | <em>Read the        |                  | </em>Example:<em> &quot;C    |
|        | narrated action  |                  | straightens the  |
|        | to be labeled</em>   |                  | cloth&quot;![]        |
|        |                  |                  | (media/image25.p |
|        |                  |                  | ng){width=&quot;1.895 |
|        |                  |                  | 8333333333333in&quot; |
|        |                  |                  | height=&quot;0.9861   |
|        |                  |                  | 111111111112in&quot;} |
+--------+------------------+------------------+------------------+
| 2      | <em>Label the       | Label <strong>right    | ![]              |
|        | contact frame    | and left hands</strong> | (media/image29.p |
|        | (first frame     | (if visible), by | ng){width=&quot;1.895 |
|        | shown)</em>          | correcting the   | 8333333333333in&quot; |
|        |                  | existing         | height=&quot;0.9722   |
|        |                  | bounding box or  | 222222222222in&quot;} |
|        |                  | adding a new     |                  |
|        |                  | one.             |                  |
+--------+------------------+------------------+------------------+
|        |                  | â€‹â€‹Label the      | ![]              |
|        |                  | <strong>object(s) of   | (media/image36.p |
|        |                  | change</strong>:        | ng){width=&quot;1.895 |
|        |                  |                  | 8333333333333in&quot; |
|        |                  | -   Draw the     | height=&quot;0.9722   |
|        |                  |     <!-- -->[bounding    | 222222222222in&quot;} |
|        |                  |     box]<!-- -->{.ul}    |                  |
|        |                  |                  |                  |
|        |                  | -   Mark the     |                  |
|        |                  |     object as    |                  |
|        |                  |     Object of    |                  |
|        |                  |     change       |                  |
|        |                  |                  |                  |
|        |                  | -   Select the   |                  |
|        |                  |     <!-- -->[name of the |                  |
|        |                  |     object]<!-- -->{.ul} |                  |
|        |                  |     from list    |                  |
|        |                  |     provided     |                  |
|        |                  |                  |                  |
|        |                  | -   Select       |                  |
|        |                  |     <!-- -->[instance    |                  |
|        |                  |     ID]<!-- -->{.ul}     |                  |
|        |                  |     (for         |                  |
|        |                  |     multiple     |                  |
|        |                  |     objects of   |                  |
|        |                  |     the same     |                  |
|        |                  |     type)        |                  |
|        |                  |                  |                  |
|        |                  | -   Repeat for   |                  |
|        |                  |     each object  |                  |
|        |                  |     of change    |                  |
+--------+------------------+------------------+------------------+
|        |                  | &gt; Label the      | ![]              |
|        |                  | &gt; <strong>tool</strong> (if   | (media/image27.p |
|        |                  | &gt; present):      | ng){width=&quot;1.895 |
|        |                  |                  | 8333333333333in&quot; |
|        |                  | -   Draw the     | height=&quot;0.9722   |
|        |                  |     &gt; <!-- -->[bounding  | 222222222222in&quot;} |
|        |                  |     &gt; box]<!-- -->{.ul}  |                  |
|        |                  |                  |                  |
|        |                  | -   Mark the     |                  |
|        |                  |     &gt; object as  |                  |
|        |                  |     &gt; Tool       |                  |
|        |                  |                  |                  |
|        |                  | -   Select the   |                  |
|        |                  |     &gt; <!-- -->[name of   |                  |
|        |                  |     &gt; the        |                  |
|        |                  |     &gt; tool]<!-- -->{.ul} |                  |
|        |                  |     &gt; from list  |                  |
|        |                  |     &gt; provided   |                  |
|        |                  |                  |                  |
|        |                  | -   Select       |                  |
|        |                  |     &gt; <!-- -->[instance  |                  |
|        |                  |     &gt; ID]<!-- -->{.ul}   |                  |
|        |                  |     &gt; (for       |                  |
|        |                  |     &gt; multiple   |                  |
|        |                  |     &gt; objects of |                  |
|        |                  |     &gt; the same   |                  |
|        |                  |     &gt; type)      |                  |
+--------+------------------+------------------+------------------+
| 3      | <em>Label the       | -   Go to the    | ![]              |
|        | remaining        |     &gt; next frame | (media/image41.p |
|        | frames</em>          |                  | ng){width=&quot;1.895 |
|        |                  | -   Adjust the   | 8333333333333in&quot; |
|        |                  |     &gt; hand boxes | height=&quot;0.9861   |
|        |                  |                  | 111111111112in&quot;} |
|        |                  | -   Adjust the   |                  |
|        |                  |     &gt; object of  |                  |
|        |                  |     &gt; change box |                  |
|        |                  |                  |                  |
|        |                  | -   Adjust the   |                  |
|        |                  |     &gt; tool box   |                  |
|        |                  |     &gt; (if        |                  |
|        |                  |     &gt; present)   |                  |
|        |                  |                  |                  |
|        |                  | -   Repeat for   |                  |
|        |                  |     &gt; the        |                  |
|        |                  |     &gt; remaining  |                  |
|        |                  |     &gt; frames     |                  |
+--------+------------------+------------------+------------------+</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="stage-3---post-condition">Stage 3 - Post-condition<a aria-hidden="true" class="hash-link" href="#stage-3---post-condition" title="Direct link to heading">â€‹</a></h3><p><strong>Objective:</strong> Label bounding boxes and roles for hands and objects
(from Contact to Post frame).</p><p><strong>Annotation Task</strong> (see [<a href="https://docs.google.com/document/d/18kSRpBNhYirvlFDF6MplpkiRLstGh5BMko4DKHwq_9o/edit?usp=sharing" target="_blank" rel="noopener noreferrer">Annotation
instructions</a>)
<!-- -->[Note]<!-- -->{.ul}: clips annotated from Stage 1 play from CONTACT to POST
frame:</p><p>+--------+------------------+------------------+------------------+
| <strong>#</strong> | <strong>Step</strong>         | <strong>Sub-step</strong>     | <strong>Example</strong>      |
+========+==================+==================+==================+
| 1      | <em>Read the        |                  | </em>Example:<em> &quot;C    |
|        | narrated action  |                  | straightens the  |
|        | to be labeled</em>   |                  | cloth&quot;![]        |
|        |                  |                  | (media/image25.p |
|        |                  |                  | ng){width=&quot;1.895 |
|        |                  |                  | 8333333333333in&quot; |
|        |                  |                  | height=&quot;0.9861   |
|        |                  |                  | 111111111112in&quot;} |
+--------+------------------+------------------+------------------+
| 2      | <em>Check the       | Contact frame    |                  |
|        | contact frame    | will already be  |                  |
|        | (first frame     | labeled with:    |                  |
|        | shown)</em>          |                  |                  |
|        |                  | -   Left hand    |                  |
|        |                  |     &gt; (if        |                  |
|        |                  |     &gt; visible)   |                  |
|        |                  |                  |                  |
|        |                  | -   Right hand   |                  |
|        |                  |     &gt; (if        |                  |
|        |                  |     &gt; visible)   |                  |
|        |                  |                  |                  |
|        |                  | -   Active       |                  |
|        |                  |     &gt; object     |                  |
|        |                  |                  |                  |
|        |                  | -   Tool (if     |                  |
|        |                  |                  |                  |
|        |                  |    &gt; applicable) |                  |
+--------+------------------+------------------+------------------+
|        |                  |                  |                  |
+--------+------------------+------------------+------------------+
| 3      | <em>Label the       | -   Go to the    | ![]              |
|        | remaining        |     &gt; next frame | (media/image37.p |
|        | frames</em>          |                  | ng){width=&quot;1.895 |
|        |                  | -   Adjust (or   | 8333333333333in&quot; |
|        |                  |     &gt; add) the   | height=&quot;0.9722   |
|        |                  |     &gt; hand boxes | 222222222222in&quot;} |
|        |                  |                  |                  |
|        |                  | -   Adjust the   |                  |
|        |                  |     &gt; object of  |                  |
|        |                  |     &gt; change box |                  |
|        |                  |                  |                  |
|        |                  | -   Adjust the   |                  |
|        |                  |     &gt; tool box   |                  |
|        |                  |     &gt; (if        |                  |
|        |                  |     &gt; present)   |                  |
|        |                  |                  |                  |
|        |                  | -   Repeat for   |                  |
|        |                  |     &gt; the        |                  |
|        |                  |     &gt; remaining  |                  |
|        |                  |     &gt; frames     |                  |
+--------+------------------+------------------+------------------+</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="audio-visual-diarization--social-avs">Audio-Visual Diarization &amp; Social (AVS)<a aria-hidden="true" class="hash-link" href="#audio-visual-diarization--social-avs" title="Direct link to heading">â€‹</a></h2><p><strong>Objective:</strong></p><ul><li><p><strong>AV</strong>: Locate each speaker spatially and temporally, segment and</p><blockquote><p>transcribe the speech content (in a given video), assign each
speaker an anonymous label. [<a href="https://docs.google.com/document/d/188OjXu_UvwB2vLX5SestusNuhdao7QcZ11BbuKd5-8U/edit?usp=sharing" target="_blank" rel="noopener noreferrer">Audio Visual Detection &amp; Tracking
Annotations Summary
<!-- -->[<!-- -->Updated<!-- -->]</a></p></blockquote></li><li><p><strong>S:</strong> predict the following social cues:</p><ul><li><p>Who is talking to the camera wearer at each time segment</p></li><li><p>Who is looking at the camera wearer at each time segment</p></li></ul></li></ul><p><strong>Motivation:</strong> Understand conversational behavior from the naturalistic
egocentric perspective; capture low level detection, segmentation and
tracking attributes of people\&#x27;s interactions in a scene, and more high
level (intent/emotions driven) attributes that drive social and group
conversations in the real world.</p><p>+--------------------------------+----------------------------+
| <strong>Annotation stats (Aug 21):</strong> | <strong>Scenario Distribution:</strong> |
|                                |                            |
| TBU                            |                            |
|                                |                            |
| Source:                        |                            |
+--------------------------------+----------------------------+</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="av-step-0-automated-face--head-detection">AV Step 0: Automated Face &amp; Head Detection<a aria-hidden="true" class="hash-link" href="#av-step-0-automated-face--head-detection" title="Direct link to heading">â€‹</a></h3><p>A face detection algorithm is run on the given input video to detect all
the faces. The resulting bounding boxes are going to be populated and
overlaid on the input video.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="av-step-1-face--head-tracks-correction">AV Step 1: Face &amp; Head Tracks Correction<a aria-hidden="true" class="hash-link" href="#av-step-1-face--head-tracks-correction" title="Direct link to heading">â€‹</a></h3><p><strong>Objective:</strong> Have a correct face bounding box around all the faces
visible in the video</p><p><strong>Annotation Task</strong> (see [<a href="https://docs.google.com/document/d/1mgPTHJWJt1HWmOiM-UQOp-rc8S7zvnkw/edit?usp=sharing&amp;ouid=109871152660798629950&amp;rtpof=true&amp;sd=true" target="_blank" rel="noopener noreferrer">Annotation
instructions</a>):</p><p>+----------------------+----------------------+----------------------+
| <strong>#</strong>               | <strong>Step</strong>             | <strong>Sub-step</strong>         |
+======================+======================+======================+
| 1                    | <em>For each frame in   | 1.  </em>Subject <strong>has</strong> |
|                      | the video, identify  |     &gt; a bounding box |
|                      | all subjects in the  |     &gt; (bbox):<em>       |
|                      | frame and check to   |                      |
|                      | see if they have     |     a.  </em>Bbox is     |
|                      | bounding boxes.<em>     |                      |
|                      |                      |        &gt; <strong>PASSING</strong> |
|                      |                      |         &gt; â†’ Move     |
|                      |                      |         &gt; onto the   |
|                      |                      |         &gt; next       |
|                      |                      |         &gt; subject in |
|                      |                      |         &gt; the frame</em> |
|                      |                      |                      |
|                      |                      |     b.  <em>Bbox is     |
|                      |                      |                      |
|                      |                      |        &gt; <strong>FAILING</strong> |
|                      |                      |         &gt; â†’          |
|                      |                      |                      |
|                      |                      |     &gt; Adjust/Re-draw |
|                      |                      |         &gt; the bbox   |
|                      |                      |         &gt; (making    |
|                      |                      |         &gt; sure the   |
|                      |                      |         &gt; right face |
|                      |                      |         &gt; track is   |
|                      |                      |         &gt; selected)</em> |
|                      |                      |                      |
|                      |                      | 2.  <em>Subject         |
|                      |                      |     &gt; <strong>doesn&#x27;t      |
|                      |                      |     &gt; have</strong> a bbox  |
|                      |                      |     &gt; â†’ Create a new |
|                      |                      |     &gt; bounding box   |
|                      |                      |     &gt; and either     |
|                      |                      |     &gt; assign it a    |
|                      |                      |     &gt; new track o    |
|                      |                      |     &gt; merge an       |
|                      |                      |     &gt; existing face  |
|                      |                      |     &gt; track.</em>        |
|                      |                      |                      |
|                      |                      | 3.  <em>Bbox does not   |
|                      |                      |     &gt; capture a face |
|                      |                      |     &gt; â†’ Delete       |
|                      |                      |     &gt; bbox.</em>         |
+----------------------+----------------------+----------------------+
| <strong>Examples:</strong>        |                      |                      |
+----------------------+----------------------+----------------------+
| <strong>Passing</strong> Bbox     | <!-- -->![image.png]<!-- -->(media   |                      |
|                      | /image6.png){width=&quot; |                      |
|                      | 4.104166666666667in&quot; |                      |
|                      | height=&quot;2.           |                      |
|                      | 2916666666666665in&quot;} |                      |
+----------------------+----------------------+----------------------+
| <strong>Failing</strong> bbox     | <!-- -->![image.png]<!-- -->(media/  |                      |
|                      | image12.png){width=&quot; |                      |
|                      | 4.104166666666667in&quot; |                      |
|                      | height=&quot;2.           |                      |
|                      | 2916666666666665in&quot;} |                      |
+----------------------+----------------------+----------------------+
| <strong>Missing</strong> bbox     | <!-- -->![image.png]<!-- -->(media/  |                      |
|                      | image23.png){width=&quot; |                      |
|                      | 4.104166666666667in&quot; |                      |
|                      | height=&quot;2.           |                      |
|                      | 2916666666666665in&quot;} |                      |
+----------------------+----------------------+----------------------+
| Bbox to be           | <!-- -->![image.png]<!-- -->(media/  |                      |
| <strong>deleted</strong>          | image20.png){width=&quot; |                      |
|                      | 4.104166666666667in&quot; |                      |
|                      | height=&quot;2.           |                      |
|                      | 3055555555555554in&quot;} |                      |
+----------------------+----------------------+----------------------+</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="av-step-2-speaker-labeling-and-av-anchor-extraction">AV Step 2: Speaker Labeling and AV anchor extraction<a aria-hidden="true" class="hash-link" href="#av-step-2-speaker-labeling-and-av-anchor-extraction" title="Direct link to heading">â€‹</a></h3><p><strong>Objective:</strong> Assign each Face Track<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup> (from Step 1) a &#x27;Person ID&#x27;
(for each new subject which has an interaction with the camera-wearer or
is present in the camera for 500+ frames).</p><p><strong>Annotation Task</strong> (see [<a href="https://fb-my.sharepoint.com/:w:/p/sallyyoo/EXSVyiXDcypOjOdvgE24Rq0BmSU2iEDVHDneItZblllefQ?e=1nU14r" target="_blank" rel="noopener noreferrer">Annotation
instructions</a>):</p><p>+----------------------+----------------------+----------------------+
| <strong>#</strong>               | <strong>Step</strong>             | <strong>Sub-step</strong>         |
+======================+======================+======================+
| 1                    | <em>Identify the &#x27;Next  | 1.  </em>Toggle On the   |
|                      | Track&#x27; and go to the |     &#x27;Out-of-Frame&#x27;   |
|                      | first frame of this  |     Track List<em>      |
|                      | track.</em>              |                      |
|                      |                      | 2.  <em>Select the next |
|                      |                      |     Track from the   |
|                      |                      |     list</em>            |
|                      |                      |                      |
|                      |                      | 3.  <em>Click &#x27;First    |
|                      |                      |     Key Frame&#x27;</em>      |
+----------------------+----------------------+----------------------+
| 2                    | <em>Assign this Track a | 1.  </em>Use the drop    |
|                      | unique &#x27;Person ID&#x27;   |     down menu to     |
|                      | (e.g. Person 1,      |     select a Person  |
|                      | Person 2, ect)<em>      |     ID</em>              |
|                      |                      |                      |
|                      |                      | 2.  <em>Each time this  |
|                      |                      |     person appears   |
|                      |                      |     in the video,    |
|                      |                      |     assign their     |
|                      |                      |     Track <!-- -->#<!-- --> to      |
|                      |                      |     their designated |
|                      |                      |     Person ID</em>       |
+----------------------+----------------------+----------------------+
| 3                    | <em>Repeat steps 1-4    |                      |
|                      | until all tracks     |                      |
|                      | have Person ID&#x27;s     |                      |
|                      | assigned.</em>           |                      |
+----------------------+----------------------+----------------------+
| <strong>Examples:</strong>        |                      |                      |
+----------------------+----------------------+----------------------+
| [<!-- -->[AV - Step 2 -      |                      |                      |
| Person ID Example    |                      |                      |
| Annot                |                      |                      |
| ation.mp4]<!-- -->(htt |                      |                      |
| ps://drive.google.co |                      |                      |
| m/file/d/1R1R1BXXMTx |                      |                      |
| FsZKl98tM73gIWRrMCZj |                      |                      |
| O2/view?usp=sharing) |                      |                      |
|                      |                      |                      |
| ![](media/           |                      |                      |
| image26.png){width=&quot; |                      |                      |
| 6.203125546806649in&quot; |                      |                      |
| height=&quot;2            |                      |                      |
| .077746062992126in&quot;} |                      |                      |
+----------------------+----------------------+----------------------+</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="av-step-3-speech-segmentation-per-speaker">AV Step 3: Speech Segmentation (Per Speaker)<a aria-hidden="true" class="hash-link" href="#av-step-3-speech-segmentation-per-speaker" title="Direct link to heading">â€‹</a></h3><p><strong>Objective:</strong> Label voice activity for all subjects in the video.</p><p><strong>Annotation Task</strong> (see [<a href="https://fb-my.sharepoint.com/:w:/p/sallyyoo/EXSVyiXDcypOjOdvgE24Rq0BmSU2iEDVHDneItZblllefQ?e=1nU14r" target="_blank" rel="noopener noreferrer">Annotation
instructions</a>):</p><p>+----------------------+----------------------+----------------------+
| <strong>#</strong>               | <strong>Step</strong>             | <strong>Sub-step</strong>         |
+======================+======================+======================+
| 1                    | <em>Label voice         | 1.  Annotate the     |
|                      | activity for the     |     video using the  |
|                      | <strong>camera wearer</strong>    |     time segment     |
|                      | first and then for   |     toolâ€¯            |
|                      | each Person ID.</em>     |                      |
|                      |                      | 2.  Start an         |
|                      |                      |     annotation when  |
|                      |                      |     a person makes a |
|                      |                      |     sound (speech,   |
|                      |                      |     coughing, sigh,  |
|                      |                      |     <strong>any            |
|                      |                      |     utterance</strong>).    |
|                      |                      |                      |
|                      |                      | 3.  Stop an          |
|                      |                      |     annotation when  |
|                      |                      |     a person stops   |
|                      |                      |     making sounds.   |
|                      |                      |                      |
|                      |                      | 4.  Do not stop an   |
|                      |                      |     annotation if a  |
|                      |                      |     person starts    |
|                      |                      |     making sound     |
|                      |                      |     again within 1   |
|                      |                      |     second after     |
|                      |                      |     they stopped     |
|                      |                      |                      |
|                      |                      | 5.  Label the        |
|                      |                      |     segment          |
|                      |                      |     according to the |
|                      |                      |     Person ID        |
|                      |                      |     displayed in the |
|                      |                      |     bounding box     |
|                      |                      |     around their     |
|                      |                      |     head             |
|                      |                      |                      |
|                      |                      | 6.  Repeat the       |
|                      |                      |     process for all  |
|                      |                      |     sounds made by   |
|                      |                      |     the people in    |
|                      |                      |     the video.       |
+----------------------+----------------------+----------------------+
| <strong>Examples:</strong>        |                      |                      |
+----------------------+----------------------+----------------------+
| [<!-- -->[AV - Step 3 -      |                      |                      |
| Voice Activity       |                      |                      |
| Annotation           |                      |                      |
| Ex                   |                      |                      |
| ample.mp4]<!-- -->(htt |                      |                      |
| ps://drive.google.co |                      |                      |
| m/file/d/19zHRx6nC-l |                      |                      |
| i7wC0ivImC5b-KCjLMES |                      |                      |
| Pt/view?usp=sharing) |                      |                      |
|                      |                      |                      |
| ![](media/           |                      |                      |
| image34.png){width=&quot; |                      |                      |
| 6.239583333333333in&quot; |                      |                      |
| height=&quot;2            |                      |                      |
| .236111111111111in&quot;} |                      |                      |
+----------------------+----------------------+----------------------+</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="av-step-4-transcription">AV Step 4: Transcription<a aria-hidden="true" class="hash-link" href="#av-step-4-transcription" title="Direct link to heading">â€‹</a></h3><p><strong>Objective:</strong> Transcribe voice activity for all subjects in the video.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="av-step-5-correcting-speech-transcriptions-wip">AV Step 5: Correcting Speech Transcriptions <!-- -->[<!-- -->WIP<!-- -->]<a aria-hidden="true" class="hash-link" href="#av-step-5-correcting-speech-transcriptions-wip" title="Direct link to heading">â€‹</a></h3><p><strong>Objective:</strong> Correcting Speech Transcription annotation from Step 4.</p><p><strong>Annotation Task</strong> (see [<a href="https://docs.google.com/document/d/1Wi-dRM9sKPtRdjdLIxGYxJYjfU72on3css-8IjrKAOc/edit?usp=sharing" target="_blank" rel="noopener noreferrer">Annotation
instructions</a>
<!-- -->[<!-- -->WIP<!-- -->]<!-- -->):</p><p>+---------------+-------------------------+-------------------------+
| <strong>#</strong>        | <strong>Step</strong>                | <strong>Sub-step</strong>            |
+===============+=========================+=========================+
| 0             | <em>Pre-load the           | The task begins with    |
|               | annotation tool.</em>       | the pre-load of the     |
|               |                         | following things:       |
|               |                         |                         |
|               |                         | -   Output of AV Step 3 |
|               |                         |     &gt; (Speech           |
|               |                         |     &gt; Segmentation per  |
|               |                         |     &gt; Person ID)        |
|               |                         |                         |
|               |                         | -   Output of AV Step 4 |
|               |                         |     &gt; (Human            |
|               |                         |     &gt; transcriptions)   |
|               |                         |                         |
|               |                         | -   Automatic           |
|               |                         |     &gt; transcriptions    |
|               |                         |     &gt; from ASR          |
|               |                         |     &gt; algorithms.       |
+---------------+-------------------------+-------------------------+
| 1             | <em>For each human         | For each person with    |
|               | transcription chunk,    | the active voice        |
|               | identify the            | activity:               |
|               | corresponding person    |                         |
|               | IDs with voice activity | -   Listen to the video |
|               | on</em>                     |                         |
|               |                         | -   If the person&#x27;s     |
|               |                         |     &gt; speech is = to    |
|               |                         |     &gt; the content in    |
|               |                         |     &gt; the transcription |
|               |                         |     &gt; chunk, then copy  |
|               |                         |     &gt; this speech       |
|               |                         |     &gt; content from      |
|               |                         |     &gt; transcript into a |
|               |                         |     &gt; new dialog        |
|               |                         |     &gt; box/tag that      |
|               |                         |     &gt; corresponds to    |
|               |                         |     &gt; the person.       |
+---------------+-------------------------+-------------------------+
| 2             | <em>Repeat Step 1 for the  |                         |
|               | machine generated       |                         |
|               | transcription chunks.</em>  |                         |
+---------------+-------------------------+-------------------------+
| <strong>Examples:</strong> |                         |                         |
+---------------+-------------------------+-------------------------+
| TBU           |                         |                         |
+---------------+-------------------------+-------------------------+</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="s-step-1-camera-wearer-attention">S Step 1: Camera-Wearer Attention<a aria-hidden="true" class="hash-link" href="#s-step-1-camera-wearer-attention" title="Direct link to heading">â€‹</a></h3><p><strong>Objective</strong>: Annotate temporal segments in which a person is looking
at the camera wearer.</p><p><strong>Annotation Task</strong> (see [<a href="https://docs.google.com/document/d/1CqgM73xrYuva5eKSfTmM4Tby7tso1jXX/edit?usp=sharing&amp;ouid=109871152660798629950&amp;rtpof=true&amp;sd=true" target="_blank" rel="noopener noreferrer">Annotation
instructions</a>):</p><p>+----------------------+----------------------+----------------------+
| <strong>#</strong>               | <strong>Step</strong>             | <strong>Sub-step</strong>         |
+======================+======================+======================+
| 1                    | <em>Watch the video and |                      |
|                      | find the time when   |                      |
|                      | someone is looking   |                      |
|                      | at the camera        |                      |
|                      | wearer</em>              |                      |
+----------------------+----------------------+----------------------+
| 2                    | <em>Annotate the time   | 1.  Start an         |
|                      | segment using the    |     annotation when  |
|                      | time segment tool:â€¯</em> |     a person start   |
|                      |                      |     to look at the   |
|                      |                      |     camera wearer    |
|                      |                      |                      |
|                      |                      | 2.  Stop an          |
|                      |                      |     annotation when  |
|                      |                      |     a person stops   |
|                      |                      |     looking at the   |
|                      |                      |     camera wearer    |
|                      |                      |                      |
|                      |                      | 3.  Label the        |
|                      |                      |     segment          |
|                      |                      |     according to the |
|                      |                      |     Person ID        |
|                      |                      |     displayed in the |
|                      |                      |     bounding box     |
|                      |                      |     around their     |
|                      |                      |     head             |
|                      |                      |                      |
|                      |                      | 4.  Repeat the       |
|                      |                      |     process for all  |
|                      |                      |     cases in the     |
|                      |                      |     video.           |
+----------------------+----------------------+----------------------+
| <strong>Examples:</strong>        |                      |                      |
+----------------------+----------------------+----------------------+
| [<!-- -->[social_annotation  |                      |                      |
| _demo.mp4]<!-- -->(htt |                      |                      |
| ps://drive.google.co |                      |                      |
| m/file/d/10Z0Ge0bIXJ |                      |                      |
| NbhUZ61iT0bckCj1Vno- |                      |                      |
| G7/view?usp=sharing) |                      |                      |
|                      |                      |                      |
| ![](media/           |                      |                      |
| image17.png){width=&quot; |                      |                      |
| 6.239583333333333in&quot; |                      |                      |
| height=&quot;3.           |                      |                      |
| 3775820209973753in&quot;} |                      |                      |
+----------------------+----------------------+----------------------+</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="s-step-2-speech-target-classification">S Step 2: Speech Target Classification<a aria-hidden="true" class="hash-link" href="#s-step-2-speech-target-classification" title="Direct link to heading">â€‹</a></h3><p><strong>Objective</strong>: Given already annotated AV Voice Activity segmentation,
the annotator is going to annotate the particular speech segments in
which the person is talking to the camera wearer.</p><p><strong>Annotation Task</strong> (see [<a href="https://docs.google.com/document/d/1wnJqZESJpQrwaCkdFZWm8Makmb5bF4r5/edit?usp=sharing&amp;ouid=109871152660798629950&amp;rtpof=true&amp;sd=true" target="_blank" rel="noopener noreferrer">Annotation
instructions</a>):</p><p>+----------------------+----------------------+----------------------+
| <strong>#</strong>               | <strong>Step</strong>             | <strong>Sub-step</strong>         |
+======================+======================+======================+
| 1                    | <em>Watch the video     |                      |
|                      | with AV voice        |                      |
|                      | segmentation results |                      |
|                      | (start-end time,     |                      |
|                      | person ID)</em>          |                      |
+----------------------+----------------------+----------------------+
| 2                    | <em>Annotate segments   | 1.  </em>Identify a      |
|                      | where someone is     |     &gt; segment in     |
|                      | talking to the       |     &gt; which someone  |
|                      | camera wearer<em>       |     &gt; is talking to  |
|                      |                      |     &gt; the camera     |
|                      | </em>Repeat the process  |     &gt; wearer<em>        |
|                      | for all cases in the |                      |
|                      | video.</em>              | 2.  <em>Click the time  |
|                      |                      |     &gt; segment, then  |
|                      |                      |     &gt; you can see    |
|                      |                      |     &gt; the Voice      |
|                      |                      |     &gt; activity       |
|                      |                      |     &gt; annotation     |
|                      |                      |     &gt; information on |
|                      |                      |     &gt; the left side  |
|                      |                      |     &gt; bar.</em>          |
|                      |                      |                      |
|                      |                      | 3.  <em>Click the drop  |
|                      |                      |     &gt; down box below |
|                      |                      |     &gt; the &quot;Target of |
|                      |                      |                      |
|                      |                      |  &gt; Speech&quot;</em>![](media |
|                      |                      | /image8.png){width=&quot; |
|                      |                      | 3.198122265966754in&quot; |
|                      |                      |     &gt; height=&quot;1      |
|                      |                      | .030867235345582in&quot;} |
|                      |                      |                      |
|                      |                      | 4.  <em>In the dropdown |
|                      |                      |     &gt; menu, select   |
|                      |                      |                      |
|                      |                      |    &gt; &quot;Camera-Wearer&quot; |
|                      |                      |     &gt; if the speech  |
|                      |                      |     &gt; is only toward |
|                      |                      |     &gt; the camera     |
|                      |                      |     &gt; wearer.</em>       |
|                      |                      |                      |
|                      |                      | 5.  <em>Choose          |
|                      |                      |     &gt; &quot;Camera-Wearer |
|                      |                      |     &gt; and others&quot; if |
|                      |                      |     &gt; the speech     |
|                      |                      |     &gt; segment is     |
|                      |                      |     &gt; toward         |
|                      |                      |     &gt; multiple       |
|                      |                      |     &gt; people         |
|                      |                      |     &gt; including the  |
|                      |                      |     &gt; camera wearer  |
|                      |                      |     &gt; (e.g., talking |
|                      |                      |     &gt; to multiple    |
|                      |                      |     &gt; audience       |
|                      |                      |     &gt; members).</em>     |
|                      |                      |                      |
|                      |                      | 6.  <em>Repeat the      |
|                      |                      |     &gt; process for    |
|                      |                      |     &gt; all relevant   |
|                      |                      |     &gt; segments.</em>     |
+----------------------+----------------------+----------------------+
| <strong>Examples:</strong>        |                      |                      |
+----------------------+----------------------+----------------------+
| [<!-- -->[social_step2_ex    |                      |                      |
| ample.mp4]<!-- -->(htt |                      |                      |
| ps://drive.google.co |                      |                      |
| m/file/d/1KUuaEr86sa |                      |                      |
| nTGI0-oNAYlgrvxru1TN |                      |                      |
| fw/view?usp=sharing) |                      |                      |
|                      |                      |                      |
| ![](media/           |                      |                      |
| image40.png){width=&quot; |                      |                      |
| 6.239583333333333in&quot; |                      |                      |
| height=&quot;3.           |                      |                      |
| 6666666666666665in&quot;} |                      |                      |
+----------------------+----------------------+----------------------+</p><header><h1>Links</h1></header><p>[<!-- -->1<!-- -->]<!-- --> [<a href="https://www.internalfb.com/intern/wiki/Ego4D/Ego4D_FAIR_Annotations_(WIP)/" target="_blank" rel="noopener noreferrer">Ego4D - Track 2 - FAIR Public Dataset
(Wiki)</a></p><p>[<!-- -->2<!-- -->]<!-- --> [<a href="https://fb.workplace.com/notes/462529454842221" target="_blank" rel="noopener noreferrer">Ego4D - Track 2 - FAIR Public Dataset
(Note)</a></p><p>[<!-- -->3<!-- -->]<!-- --> [<a href="https://www.internalfb.com/intern/wiki/Ego4D/Ego4D_FAIR_Annotations_(WIP)/Annotation_Activities_(TODO)/" target="_blank" rel="noopener noreferrer">Annotation Activities
(Diagram)</a></p><p>[<!-- -->4<!-- -->]<!-- --> [<a href="https://www.internalfb.com/intern/unidash/dashboard/ego4d_track_2/exploration/" target="_blank" rel="noopener noreferrer">Ego4D Track 2 Data
Dashboard</a></p><p>[<!-- -->5<!-- -->]<!-- --> [<a href="https://fb.workplace.com/groups/1417482988432448/?ref=invite_via_link&amp;invite_short_link_key=g%2F6uvAelt0x%2FgfeyqgSv" target="_blank" rel="noopener noreferrer">EGO4D: Egocentric Live 4D Perception (Workplace
Group)</a></p><table><tr><td></td><td></td></tr></table><div class="footnotes"><hr><ol><li id="fn-1">Face track = a temporal sequence of bounding boxes enclosing the
face of a person in contiguous frames (one bounding box per frame).<a href="#fnref-1" class="footnote-backref">â†©</a></li></ol></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/edit/main/website/docs/data/data-overview.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_mS5F" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_mt2f"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/benchmarks/Social/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« <!-- -->Social</div></a></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></div></div><div class="col col--3"><div class="tableOfContents_vrFS thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#key-information" class="table-of-contents__link toc-highlight">Key Information</a></li><li><a href="#annotations-tldr" class="table-of-contents__link toc-highlight">Annotations tl;dr</a></li><li><a href="#episodic-memory" class="table-of-contents__link toc-highlight">Episodic Memory</a><ul><li><a href="#natural-language-queries" class="table-of-contents__link toc-highlight">Natural Language Queries</a></li><li><a href="#moments" class="table-of-contents__link toc-highlight">Moments</a></li><li><a href="#visual-object-queries" class="table-of-contents__link toc-highlight">Visual Object Queries</a></li></ul></li><li><a href="#forecasting--hands--objects-fho" class="table-of-contents__link toc-highlight">Forecasting + Hands &amp; Objects (FHO)</a><ul><li><a href="#stage-1---critical-frames" class="table-of-contents__link toc-highlight">Stage 1 - Critical Frames</a></li><li><a href="#stage-2---pre-condition" class="table-of-contents__link toc-highlight">Stage 2 - Pre-condition</a></li><li><a href="#stage-3---post-condition" class="table-of-contents__link toc-highlight">Stage 3 - Post-condition</a></li></ul></li><li><a href="#audio-visual-diarization--social-avs" class="table-of-contents__link toc-highlight">Audio-Visual Diarization &amp; Social (AVS)</a><ul><li><a href="#av-step-0-automated-face--head-detection" class="table-of-contents__link toc-highlight">AV Step 0: Automated Face &amp; Head Detection</a></li><li><a href="#av-step-1-face--head-tracks-correction" class="table-of-contents__link toc-highlight">AV Step 1: Face &amp; Head Tracks Correction</a></li><li><a href="#av-step-2-speaker-labeling-and-av-anchor-extraction" class="table-of-contents__link toc-highlight">AV Step 2: Speaker Labeling and AV anchor extraction</a></li><li><a href="#av-step-3-speech-segmentation-per-speaker" class="table-of-contents__link toc-highlight">AV Step 3: Speech Segmentation (Per Speaker)</a></li><li><a href="#av-step-4-transcription" class="table-of-contents__link toc-highlight">AV Step 4: Transcription</a></li><li><a href="#av-step-5-correcting-speech-transcriptions-wip" class="table-of-contents__link toc-highlight">AV Step 5: Correcting Speech Transcriptions [WIP]</a></li><li><a href="#s-step-1-camera-wearer-attention" class="table-of-contents__link toc-highlight">S Step 1: Camera-Wearer Attention</a></li><li><a href="#s-step-2-speech-target-classification" class="table-of-contents__link toc-highlight">S Step 2: Speech Target Classification</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/intro/">Intro</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/ego4d" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/Ego4d" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2021 Ego4d</div></div></div></footer></div>
<script src="/assets/js/runtime~main.bbcd932f.js"></script>
<script src="/assets/js/main.3dd1840e.js"></script>
</body>
</html>